<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>AAAMLP前言</title>
      <link href="/%E6%9D%82%E5%AD%A6/AAAMLprob/index/"/>
      <url>/%E6%9D%82%E5%AD%A6/AAAMLprob/index/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p>Abhishek Thakur，很多 kaggler 对他都非常熟悉，2017 年，他在 Linkedin 发表了一篇名为<strong>Approaching (Almost) Any Machine Learning Problem</strong>的文章，介绍他建立的一个自动的机器学习框架，几乎可以解决任何机器学习问题，这篇文章曾火遍 Kaggle。</p><p>Abhishek 在 Kaggle 上的成就：</p><ul><li>Competitions Grandmaster（17 枚金牌，世界排名第 3）</li><li>Kernels Expert （Kagglers 排名前 1％）</li><li>Discussion Grandmaster（65 枚金牌，世界排名第 2）</li></ul><p>目前，Abhishek 在挪威 boost 公司担任首席数据科学家的职位，这是一家专门从事会话人工智能的软件公司。</p><p>本文对<strong>Approaching (Almost) Any Machine Learning Problem</strong>进行了<strong>中文翻译</strong>，由于本人水平有限，且未使用机器翻译，可能有部分言语不通顺或本土化程度不足，也请大家在阅读过程中多提供宝贵意见。另附上书籍原<a href="https://github.com/abhishekkrthakur/approachingalmost">项目地址</a></p><p>本项目<strong>支持在线阅读</strong>，方便您随时随地进行查阅。</p><p>因为有几章内容太过基础，所以未进行翻译，详细情况请参照书籍目录：</p><ul><li>准备环境（未翻译）</li><li>无监督和有监督学习（未翻译）</li><li><strong>交叉检验（已翻译）</strong></li><li><strong>评估指标（已翻译）</strong> -</li><li><strong>组织机器学习（已翻译）</strong></li><li><strong>处理分类变量（已翻译）</strong></li><li><strong>特征工程（已翻译）</strong></li><li><strong>特征选择（已翻译）</strong></li><li><strong>超参数优化（已翻译）</strong></li><li>图像分类和分割方法（未翻译）</li><li>文本分类或回归方法（未翻译）</li><li>组合和堆叠方法（未翻译）</li><li>可重复代码和模型方法（未翻译）</li></ul><p>我将会把完整的翻译版 <code>Markdown</code> 文件上传到 GitHub，以供大家免费下载和阅读。为了最佳的阅读体验，推荐使用 PDF 格式或是在线阅读进行查看</p><p>若您在阅读过程中发现任何错误或不准确之处，非常欢迎通过提交 Issue 或 Pull Request 来协助我进行修正。</p><p>随着时间推移，我可能会<strong>继续翻译尚未完成的章节</strong>。如果您觉得这个项目对您有帮助，请不吝给予 Star 或者进行关注。</p>]]></content>
      
      
      <categories>
          
          <category> 杂学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>处理分类变量</title>
      <link href="/%E6%9D%82%E5%AD%A6/AAAMLprob/%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F/"/>
      <url>/%E6%9D%82%E5%AD%A6/AAAMLprob/%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="处理分类变量"><a href="#处理分类变量" class="headerlink" title="处理分类变量"></a>处理分类变量</h1><p>很多人在处理分类变量时都会遇到很多困难，因此这值得用整整一章的篇幅来讨论。在本章中，我将讲述不同类型的分类数据，以及如何处理分类变量问题。</p><p><strong>什么是分类变量？</strong></p><p>分类变量/特征是指任何特征类型，可分为两大类：</p><ul><li>无序</li><li>有序</li></ul><p><strong>无序变量</strong>是指有两个或两个以上类别的变量，这些类别没有任何相关顺序。例如，如果将性别分为两组，即男性和女性，则可将其视为名义变量。</p><p><strong>有序变量</strong>则有 “等级 “或类别，并有特定的顺序。例如，一个顺序分类变量可以是一个具有低、中、高三个不同等级的特征。顺序很重要。</p><p>就定义而言，我们也可以将分类变量分为<strong>二元变量</strong>，即只有两个类别的分类变量。有些人甚至把分类变量称为 “<strong>循环</strong> “变量。周期变量以 “周期 “的形式存在，例如一周中的天数： 周日、周一、周二、周三、周四、周五和周六。周六过后，又是周日。这就是一个循环。另一个例子是一天中的小时数，如果我们将它们视为类别的话。</p><p>分类变量有很多不同的定义，很多人也谈到要根据分类变量的类型来处理不同的分类变量。不过，我认为没有必要这样做。所有涉及分类变量的问题都可以用同样的方法处理。</p><p>开始之前，我们需要一个数据集（一如既往）。要了解分类变量，最好的免费数据集之一是 Kaggle 分类特征编码挑战赛中的 <em>cat-in-the-dat</em>。共有两个挑战，我们将使用第二个挑战的数据，因为它比前一个版本有更多变量，难度也更大。</p><p>让我们来看看数据。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page85_image.png" alt=""></p><p align="center"><b>图 1：Cat-in-the-dat-ii challenge部分数据展示</b> </p><p>数据集由各种分类变量组成：</p><ul><li>无序</li><li>有序</li><li>循环</li><li>二元</li></ul><p>在图 1 中，我们只看到所有存在的变量和目标变量的子集。</p><p>这是一个二元分类问题。</p><p>目标变量对于我们学习分类变量来说并不十分重要，但最终我们将建立一个端到端模型，因此让我们看看图 2 中的目标变量分布。我们看到目标是<strong>偏斜</strong>的，因此对于这个二元分类问题来说，最好的指标是 ROC 曲线下面积（AUC）。我们也可以使用精确度和召回率，但 AUC 结合了这两个指标。因此，我们将使用 AUC 来评估我们在该数据集上建立的模型。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page86_image.png" alt=""></p><p align="center"><b>图 2：标签计数。X 轴表示标签，Y 轴表示标签计数</b> </p><p>总体而言，有：</p><ul><li>5 个二元变量</li><li>10 个无序变量</li><li>6 个有序变量</li><li>2 个循环变量</li><li>1 个目标变量</li></ul><p>让我们来看看数据集中的 <strong>ord_2</strong> 特征。它包括 6 个不同的类别：</p><ul><li>冰冻</li><li>温暖</li><li>寒冷</li><li>较热</li><li>热</li><li>非常热</li></ul><p>我们必须知道，计算机无法理解文本数据，因此我们需要将这些类别转换为数字。一个简单的方法是创建一个字典，将这些值映射为从 0 到 N-1 的数字，其中 N 是给定特征中类别的总数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 映射字典</span></span><br><span class="line">mapping = &#123;</span><br><span class="line"><span class="string">&quot;Freezing&quot;</span>: <span class="number">0</span>,</span><br><span class="line"><span class="string">&quot;Warm&quot;</span>: <span class="number">1</span>,</span><br><span class="line"><span class="string">&quot;Cold&quot;</span>: <span class="number">2</span>,</span><br><span class="line"><span class="string">&quot;Boiling Hot&quot;</span>: <span class="number">3</span>,</span><br><span class="line"><span class="string">&quot;Hot&quot;</span>: <span class="number">4</span>,</span><br><span class="line"><span class="string">&quot;Lava Hot&quot;</span>: <span class="number">5</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>现在，我们可以读取数据集，并轻松地将这些类别转换为数字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">df = pd.read_csv(<span class="string">&quot;../input/cat_train.csv&quot;</span>)</span><br><span class="line"><span class="comment"># 取*ord_2*列，并使用映射将类别转换为数字</span></span><br><span class="line">df.loc[:, <span class="string">&quot;*ord_2*&quot;</span>] = df.*ord_2*.<span class="built_in">map</span>(mapping)</span><br></pre></td></tr></table></figure><p>映射前的数值计数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df.*ord_2*.value_counts()</span><br><span class="line">Freezing <span class="number">142726</span></span><br><span class="line">Warm <span class="number">124239</span></span><br><span class="line">Cold           <span class="number">97822</span></span><br><span class="line">Boiling Hot    <span class="number">84790</span></span><br><span class="line">Hot            <span class="number">67508</span></span><br><span class="line">Lava Hot       <span class="number">64840</span></span><br><span class="line">Name: *ord_2*, dtype: int64</span><br></pre></td></tr></table></figure><p>映射后的数值计数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.0</span>   <span class="number">142726</span></span><br><span class="line"><span class="number">1.0</span>   <span class="number">124239</span></span><br><span class="line"><span class="number">2.0</span>    <span class="number">97822</span></span><br><span class="line"><span class="number">3.0</span>    <span class="number">84790</span></span><br><span class="line"><span class="number">4.0</span>    <span class="number">67508</span></span><br><span class="line"><span class="number">5.0</span>    <span class="number">64840</span></span><br><span class="line">Name: *ord_2*, dtype: int64</span><br></pre></td></tr></table></figure><p>这种分类变量的编码方式被称为标签编码（Label Encoding）我们将每个类别编码为一个数字标签。</p><p>我们也可以使用 scikit-learn 中的 LabelEncoder 进行编码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">df = pd.read_csv(<span class="string">&quot;../input/cat_train.csv&quot;</span>)</span><br><span class="line"><span class="comment"># 将缺失值填充为&quot;NONE&quot;</span></span><br><span class="line">df.loc[:, <span class="string">&quot;*ord_2*&quot;</span>] = df.*ord_2*.fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line"><span class="comment"># LabelEncoder编码</span></span><br><span class="line">lbl_enc = preprocessing.LabelEncoder()</span><br><span class="line"><span class="comment"># 转换数据</span></span><br><span class="line">df.loc[:, <span class="string">&quot;*ord_2*&quot;</span>] = lbl_enc.fit_transform(df.*ord_2*.values)</span><br></pre></td></tr></table></figure><p>你会看到我使用了 pandas 的 fillna。原因是 scikit-learn 的 LabelEncoder 无法处理 NaN 值，而 <em>ord_2</em> 列中有 NaN 值。</p><p>我们可以在许多基于树的模型中直接使用它：</p><ul><li>决策树</li><li>随机森林</li><li>提升树</li><li>或任何一种提升树模型</li><li>XGBoost</li><li>GBM</li><li>LightGBM</li></ul><p>这种编码方式不能用于线性模型、支持向量机或神经网络，因为它们希望数据是标准化的。</p><p>对于这些类型的模型，我们可以对数据进行二值化（binarize）处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Freezing    --&gt; <span class="number">0</span> --&gt; <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line">Warm        --&gt; <span class="number">1</span> --&gt; <span class="number">0</span> <span class="number">0</span> <span class="number">1</span></span><br><span class="line">Cold        --&gt; <span class="number">2</span> --&gt; <span class="number">0</span> <span class="number">1</span> <span class="number">0</span></span><br><span class="line">Boiling Hot --&gt; <span class="number">3</span> --&gt; <span class="number">0</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line">Hot         --&gt; <span class="number">4</span> --&gt; <span class="number">1</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line">Lava Hot    --&gt; <span class="number">5</span> --&gt; <span class="number">1</span> <span class="number">0</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>这只是将类别转换为数字，然后再转换为二值化表示。这样，我们就把一个特征分成了三个（在本例中）特征（或列）。如果我们有更多的类别，最终可能会分成更多的列。</p><p>如果我们用稀疏格式存储大量二值化变量，就可以轻松地存储这些变量。稀疏格式不过是一种在内存中存储数据的表示或方式，在这种格式中，你并不存储所有的值，而只存储重要的值。在上述二进制变量的情况中，最重要的就是有 1 的地方。</p><p>很难想象这样的格式，但举个例子就会明白。</p><p>假设上面的数据帧中只有一个特征：<em>ord_2</em>。</p><div class="table-container"><table><thead><tr><th style="text-align:center">Index</th><th style="text-align:center">Feature</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">Warm</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">Hot</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">Lava hot</td></tr></tbody></table></div><p>目前，我们只看到数据集中的三个样本。让我们将其转换为二值表示法，即每个样本有三个项目。</p><p>这三个项目就是三个特征。</p><div class="table-container"><table><thead><tr><th style="text-align:center">Index</th><th style="text-align:center">Feature_0</th><th style="text-align:center">Feature_1</th><th style="text-align:center">Feature_2</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr></tbody></table></div><p>因此，我们的特征存储在一个有 3 行 3 列（3x3）的矩阵中。矩阵的每个元素占用 8 个字节。因此，这个数组的总内存需求为 8x3x3 = 72 字节。</p><p>我们还可以使用一个简单的 python 代码段来检查这一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">example = np.array(</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(example.nbytes)</span><br></pre></td></tr></table></figure><p>这段代码将打印出 72，就像我们之前计算的那样。但我们需要存储这个矩阵的所有元素吗？如前所述，我们只对 1 感兴趣。0 并不重要，因为任何与 0 相乘的元素都是 0，而 0 与任何元素相加或相减也没有任何区别。只用 1 表示矩阵的一种方法是某种字典方法，其中键是行和列的索引，值是 1：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">0</span>, <span class="number">2</span>)      <span class="number">1</span></span><br><span class="line">(<span class="number">1</span>, <span class="number">0</span>)      <span class="number">1</span></span><br><span class="line">(<span class="number">2</span>, <span class="number">0</span>)      <span class="number">1</span></span><br><span class="line">(<span class="number">2</span>, <span class="number">2</span>)      <span class="number">1</span></span><br></pre></td></tr></table></figure><p>这样的符号占用的内存要少得多，因为它只需存储四个值（在本例中）。使用的总内存为 8x4 = 32 字节。任何 numpy 数组都可以通过简单的 python 代码转换为稀疏矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line"></span><br><span class="line">example = np.array(</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">sparse_example = sparse.csr_matrix(example)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sparse_example.data.nbytes)</span><br></pre></td></tr></table></figure><p>这将打印 32，比我们的密集数组少了这么多！稀疏 csr 矩阵的总大小是三个值的总和。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    sparse_example.data.nbytes +</span><br><span class="line">    sparse_example.indptr.nbytes +</span><br><span class="line">    sparse_example.indices.nbytes</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这将打印出 64 个元素，仍然少于我们的密集数组。遗憾的是，我不会详细介绍这些元素。你可以在 scipy 文档中了解更多。当我们拥有更大的数组时，比如说拥有数千个样本和数万个特征的数组，大小差异就会变得非常大。例如，我们使用基于计数特征的文本数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line">n_rows = <span class="number">10000</span></span><br><span class="line">n_cols = <span class="number">100000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成符合伯努利分布的随机数组，维度为[10000, 100000]</span></span><br><span class="line">example = np.random.binomial(<span class="number">1</span>, p=<span class="number">0.05</span>, size=(n_rows, n_cols))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of dense array: <span class="subst">&#123;example.nbytes&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 将随机矩阵转换为洗漱矩阵</span></span><br><span class="line">sparse_example = sparse.csr_matrix(example)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of sparse array: <span class="subst">&#123;sparse_example.data.nbytes&#125;</span>&quot;</span>)</span><br><span class="line">full_size = (</span><br><span class="line">    sparse_example.data.nbytes +</span><br><span class="line">    sparse_example.indptr.nbytes +</span><br><span class="line">    sparse_example.indices.nbytes</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Full size of sparse array: <span class="subst">&#123;full_size&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>这将打印：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Size of dense array: <span class="number">8000000000</span></span><br><span class="line">Size of sparse array: <span class="number">399932496</span></span><br><span class="line">Full size of sparse array: <span class="number">599938748</span></span><br></pre></td></tr></table></figure><p>因此，密集阵列需要 ~8000MB 或大约 8GB 内存。而稀疏阵列只占用 399MB 内存。</p><p>这就是为什么当我们的特征中有大量零时，我们更喜欢稀疏阵列而不是密集阵列的原因。</p><p>请注意，稀疏矩阵有多种不同的表示方法。这里我只展示了其中一种（可能也是最常用的）方法。深入探讨这些方法超出了本书的范围，因此留给读者一个练习。</p><p>尽管二值化特征的稀疏表示比其密集表示所占用的内存要少得多，但对于分类变量来说，还有一种转换所占用的内存更少。这就是所谓的 “<strong>独热编码</strong>“。</p><p>独热编码也是一种二值编码，因为只有 0 和 1 两个值。但必须注意的是，它并不是二值表示法。我们可以通过下面的例子来理解它的表示法。</p><p>假设我们用一个向量来表示 <em>ord_2</em> 变量的每个类别。这个向量的大小与 <em>ord_2</em> 变量的类别数相同。在这种特定情况下，每个向量的大小都是 6，并且除了一个位置外，其他位置都是 0。让我们来看看这个特殊的向量表。</p><div class="table-container"><table><thead><tr><th style="text-align:center">Freezing</th><th style="text-align:center">0</th><th style="text-align:center">0</th><th style="text-align:center">0</th><th style="text-align:center">0</th><th style="text-align:center">0</th><th style="text-align:center">1</th></tr></thead><tbody><tr><td style="text-align:center">Warm</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">Cold</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">Boiling Hot</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">Hot</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">Lava Hot</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr></tbody></table></div><p>我们看到向量的大小是 1x6，即向量中有 6 个元素。这个数字是怎么来的呢？如果你仔细观察，就会发现如前所述，有 6 个类别。在进行独热编码时，向量的大小必须与我们要查看的类别数相同。每个向量都有一个 1，其余所有值都是 0。现在，让我们用这些特征来代替之前的二值化特征，看看能节省多少内存。</p><p>如果你还记得以前的数据，它看起来如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center">Index</th><th style="text-align:center">Feature</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">Warm</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">Hot</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">Lava hot</td></tr></tbody></table></div><p>每个样本有 3 个特征。但在这种情况下，独热向量的大小为 6。因此，我们有 6 个特征，而不是 3 个。</p><div class="table-container"><table><thead><tr><th style="text-align:center">Index</th><th style="text-align:center">F_0</th><th style="text-align:center">F_1</th><th style="text-align:center">F_2</th><th style="text-align:center">F_3</th><th style="text-align:center">F_4</th><th style="text-align:center">F_5</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr></tbody></table></div><p>因此，我们有 6 个特征，而在这个 3x6 数组中，只有 3 个 1。使用 numpy 计算大小与二值化大小计算脚本非常相似。你需要改变的只是数组。让我们看看这段代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line">example = np.array(</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of dense array: <span class="subst">&#123;example.nbytes&#125;</span>&quot;</span>)</span><br><span class="line">sparse_example = sparse.csr_matrix(example)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of sparse array: <span class="subst">&#123;sparse_example.data.nbytes&#125;</span>&quot;</span>)</span><br><span class="line">full_size = (</span><br><span class="line">    sparse_example.data.nbytes +</span><br><span class="line">    sparse_example.indptr.nbytes +</span><br><span class="line">    sparse_example.indices.nbytes</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Full size of sparse array: <span class="subst">&#123;full_size&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>打印内存大小为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Size of dense array: <span class="number">144</span></span><br><span class="line">Size of sparse array: <span class="number">24</span></span><br><span class="line">Full size of sparse array: <span class="number">52</span></span><br></pre></td></tr></table></figure><p>我们可以看到，密集矩阵的大小远远大于二值化矩阵的大小。不过，稀疏数组的大小要更小。让我们用更大的数组来试试。在本例中，我们将使用 scikit-learn 中的 OneHotEncoder 将包含 1001 个类别的特征数组转换为密集矩阵和稀疏矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成符合均匀分布的随机整数，维度为[1000000, 10000000]</span></span><br><span class="line">example = np.random.randint(<span class="number">1000</span>, size=<span class="number">1000000</span>)</span><br><span class="line"><span class="comment"># 独热编码，非稀疏矩阵</span></span><br><span class="line">ohe = preprocessing.OneHotEncoder(sparse=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 将随机数组展平</span></span><br><span class="line">ohe_example = ohe.fit_transform(example.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of dense array: <span class="subst">&#123;ohe_example.nbytes&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 独热编码，稀疏矩阵</span></span><br><span class="line">ohe = preprocessing.OneHotEncoder(sparse=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 将随机数组展平</span></span><br><span class="line">ohe_example = ohe.fit_transform(example.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of sparse array: <span class="subst">&#123;ohe_example.data.nbytes&#125;</span>&quot;</span>)</span><br><span class="line">full_size = (</span><br><span class="line">    ohe_example.data.nbytes +</span><br><span class="line">    ohe_example.indptr.nbytes +</span><br><span class="line">    ohe_example.indices.nbytes</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Full size of sparse array: <span class="subst">&#123;full_size&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>上面代码打印的输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Size of dense array: <span class="number">8000000000</span></span><br><span class="line">Size of sparse array: <span class="number">8000000</span></span><br><span class="line">Full size of sparse array: <span class="number">16000004</span></span><br></pre></td></tr></table></figure><p>这里的密集阵列大小约为 8GB，稀疏阵列为 8MB。如果可以选择，你会选择哪个？在我看来，选择很简单，不是吗？</p><p>这三种方法（标签编码、稀疏矩阵、独热编码）是处理分类变量的最重要方法。不过，你还可以用很多其他不同的方法来处理分类变量。将分类变量转换为数值变量就是其中的一个例子。</p><p>假设我们回到之前的分类特征数据（原始数据中的 cat-in-the-dat-ii）。在数据中，<em>ord_2</em> 的值为“热“的 id 有多少？</p><p>我们可以通过计算数据的形状（shape）轻松计算出这个值，其中 <em>ord_2</em> 列的值为 <em>Boiling Hot</em>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df[df.ord_2 == <span class="string">&quot;Boiling Hot&quot;</span>].shape</span><br><span class="line">Out[X]: (<span class="number">84790</span>, <span class="number">25</span>)</span><br></pre></td></tr></table></figure><p>我们可以看到，有 84790 条记录具有此值。我们还可以使用 pandas 中的 <em>groupby</em> 计算所有类别的该值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.groupby([<span class="string">&quot;ord_2&quot;</span>])[<span class="string">&quot;id&quot;</span>].count()</span><br><span class="line">Out[X]:</span><br><span class="line">ord_2</span><br><span class="line">Boiling Hot    <span class="number">84790</span></span><br><span class="line">Cold <span class="number">97822</span></span><br><span class="line">Freezing <span class="number">142726</span></span><br><span class="line">Hot <span class="number">67508</span></span><br><span class="line">Lava Hot <span class="number">64840</span></span><br><span class="line">Warm <span class="number">124239</span></span><br><span class="line">Name: <span class="built_in">id</span>, dtype: int64</span><br></pre></td></tr></table></figure><p>如果我们只是将 <em>ord_2</em> 列替换为其计数值，那么我们就将其转换为一种数值特征了。我们可以使用 pandas 的<em>transform</em>函数和 <em>groupby</em> 来创建新列或替换这一列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.groupby([<span class="string">&quot;ord_2&quot;</span>])[<span class="string">&quot;id&quot;</span>].transform(<span class="string">&quot;count&quot;</span>)</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>         <span class="number">67508.0</span></span><br><span class="line"><span class="number">1</span>        <span class="number">124239.0</span></span><br><span class="line"><span class="number">2</span>        <span class="number">142726.0</span></span><br><span class="line"><span class="number">3</span>         <span class="number">64840.0</span></span><br><span class="line"><span class="number">4</span>         <span class="number">97822.0</span></span><br><span class="line">...</span><br><span class="line"><span class="number">599995</span>   <span class="number">142726.0</span></span><br><span class="line"><span class="number">599996</span>    <span class="number">84790.0</span></span><br><span class="line"><span class="number">599997</span>   <span class="number">142726.0</span></span><br><span class="line"><span class="number">599998</span>   <span class="number">124239.0</span></span><br><span class="line"><span class="number">599999</span>    <span class="number">84790.0</span></span><br><span class="line">Name: <span class="built_in">id</span>, Length: <span class="number">600000</span>, dtype: float64</span><br></pre></td></tr></table></figure><p>你可以添加所有特征的计数，也可以替换它们，或者根据多个列及其计数进行分组。例如，以下代码通过对 <em>ord_1</em> 和 <em>ord_2</em> 列分组进行计数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.groupby(</span><br><span class="line">   ...:     [</span><br><span class="line">   ...:        <span class="string">&quot;ord_1&quot;</span>,</span><br><span class="line">   ...:        <span class="string">&quot;ord_2&quot;</span></span><br><span class="line">   ...:     ]</span><br><span class="line">   ...: )[<span class="string">&quot;id&quot;</span>].count().reset_index(name=<span class="string">&quot;count&quot;</span>)</span><br><span class="line">Out[X]:</span><br><span class="line">ord_1        ord_2  count</span><br><span class="line"><span class="number">0</span>  Contributor  Boiling Hot <span class="number">15634</span></span><br><span class="line"><span class="number">1</span>  Contributor         Cold <span class="number">17734</span></span><br><span class="line"><span class="number">2</span>  Contributor     Freezing <span class="number">26082</span></span><br><span class="line"><span class="number">3</span>  Contributor          Hot <span class="number">12428</span></span><br><span class="line"><span class="number">4</span>  Contributor     Lava Hot <span class="number">11919</span></span><br><span class="line"><span class="number">5</span>  Contributor         Warm <span class="number">22774</span></span><br><span class="line"><span class="number">6</span>       Expert  Boiling Hot <span class="number">19477</span></span><br><span class="line"><span class="number">7</span>       Expert         Cold <span class="number">22956</span></span><br><span class="line"><span class="number">8</span>       Expert     Freezing <span class="number">33249</span></span><br><span class="line"><span class="number">9</span>       Expert          Hot <span class="number">15792</span></span><br><span class="line"><span class="number">10</span>      Expert     Lava Hot <span class="number">15078</span></span><br><span class="line"><span class="number">11</span>      Expert         Warm <span class="number">28900</span></span><br><span class="line"><span class="number">12</span> Grandmaster  Boiling Hot <span class="number">13623</span></span><br><span class="line"><span class="number">13</span> Grandmaster         Cold <span class="number">15464</span></span><br><span class="line"><span class="number">14</span> Grandmaster     Freezing <span class="number">22818</span></span><br><span class="line"><span class="number">15</span> Grandmaster          Hot <span class="number">10805</span></span><br><span class="line"><span class="number">16</span> Grandmaster     Lava Hot <span class="number">10363</span></span><br><span class="line"><span class="number">17</span> Grandmaster         Warm <span class="number">19899</span></span><br><span class="line"><span class="number">18</span>      Master  Boiling Hot <span class="number">10800</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>请注意，我已经从输出中删除了一些行，以便在一页中容纳这些行。这是另一种可以作为功能添加的计数。您现在一定已经注意到，我使用 id 列进行计数。不过，你也可以通过对列的组合进行分组，对其他列进行计数。</p><p>还有一个小窍门，就是从这些分类变量中创建新特征。你可以从现有的特征中创建新的分类特征，而且可以毫不费力地做到这一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df[<span class="string">&quot;new_feature&quot;</span>] = (</span><br><span class="line">   ...:     df.ord_1.astype(<span class="built_in">str</span>)</span><br><span class="line">   ...:     + <span class="string">&quot;_&quot;</span></span><br><span class="line">   ...:     + df.ord_2.astype(<span class="built_in">str</span>)</span><br><span class="line">   ...: )</span><br><span class="line">In [X]: df.new_feature</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>                Contributor_Hot</span><br><span class="line"><span class="number">1</span>               Grandmaster_Warm</span><br><span class="line"><span class="number">2</span>                   nan_Freezing</span><br><span class="line"><span class="number">3</span>                Novice_Lava Hot</span><br><span class="line"><span class="number">4</span>               Grandmaster_Cold</span><br><span class="line">               ...</span><br><span class="line"><span class="number">599999</span>   Contributor_Boiling Hot</span><br><span class="line">Name: new_feature, Length: <span class="number">600000</span>, dtype: <span class="built_in">object</span></span><br></pre></td></tr></table></figure><p>在这里，我们用下划线将 <em>ord_1</em> 和 <em>ord_2</em> 合并，然后将这些列转换为字符串类型。请注意，NaN 也会转换为字符串。不过没关系。我们也可以将 NaN 视为一个新的类别。这样，我们就有了一个由这两个特征组合而成的新特征。您还可以将三列以上或四列甚至更多列组合在一起。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df[<span class="string">&quot;new_feature&quot;</span>] = (</span><br><span class="line">   ...:     df.ord_1.astype(<span class="built_in">str</span>)</span><br><span class="line">   ...:     + <span class="string">&quot;_&quot;</span></span><br><span class="line">   ...:     + df.ord_2.astype(<span class="built_in">str</span>)</span><br><span class="line">   ...:     + <span class="string">&quot;_&quot;</span></span><br><span class="line">   ...:     + df.ord_3.astype(<span class="built_in">str</span>)</span><br><span class="line">   ...: )</span><br><span class="line">In [X]: df.new_feature</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>                Contributor_Hot_c</span><br><span class="line"><span class="number">1</span>               Grandmaster_Warm_e</span><br><span class="line"><span class="number">2</span>                   nan_Freezing_n</span><br><span class="line"><span class="number">3</span>                Novice_Lava Hot_a</span><br><span class="line"><span class="number">4</span>               Grandmaster_Cold_h</span><br><span class="line">               ...</span><br><span class="line"><span class="number">599999</span>   Contributor_Boiling Hot_b</span><br><span class="line">Name: new_feature, Length: <span class="number">600000</span>, dtype: <span class="built_in">object</span></span><br></pre></td></tr></table></figure><p>那么，我们应该把哪些类别结合起来呢？这并没有一个简单的答案。这取决于您的数据和特征类型。一些领域知识对于创建这样的特征可能很有用。但是，如果你不担心内存和 CPU 的使用，你可以采用一种贪婪的方法，即创建许多这样的组合，然后使用一个模型来决定哪些特征是有用的，并保留它们。我们将在本书稍后部分介绍这种方法。</p><p>无论何时获得分类变量，都要遵循以下简单步骤：</p><ul><li>填充 NaN 值（这一点非常重要！）。</li><li>使用 scikit-learn 的 LabelEncoder 或映射字典进行标签编码，将它们转换为整数。如果没有填充 NaN 值，可能需要在这一步中进行处理</li><li>创建独热编码。是的，你可以跳过二值化！</li><li>建模！我指的是机器学习。</li></ul><p>在分类特征中处理 NaN 数据非常重要，否则您可能会从 scikit-learn 的 LabelEncoder 中得到臭名昭著的错误信息：</p><p>ValueError: y 包含以前未见过的标签： [Nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan)</p><p>这仅仅意味着，在转换测试数据时，数据中出现了 NaN 值。这是因为你在训练时忘记了处理它们。<strong>处理 NaN 值</strong>的一个简单方法就是丢弃它们。虽然简单，但并不理想。NaN 值中可能包含很多信息，如果只是丢弃这些值，就会丢失这些信息。在很多情况下，大部分数据都是 NaN 值，因此不能丢弃 NaN 值的行/样本。处理 NaN 值的另一种方法是将其作为一个全新的类别。这是处理 NaN 值最常用的方法。如果使用 pandas，还可以通过非常简单的方式实现。</p><p>请看我们之前查看过的数据的 <em>ord_2</em> 列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.ord_2.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line">Freezing <span class="number">142726</span></span><br><span class="line">Warm <span class="number">124239</span></span><br><span class="line">Cold           <span class="number">97822</span></span><br><span class="line">Boiling Hot    <span class="number">84790</span></span><br><span class="line">Hot            <span class="number">67508</span></span><br><span class="line">Lava Hot       <span class="number">64840</span></span><br><span class="line">Name: ord_2, dtype: int64</span><br></pre></td></tr></table></figure><p>填入 NaN 值后，就变成了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.ord_2.fillna(<span class="string">&quot;NONE&quot;</span>).value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line">Freezing <span class="number">142726</span></span><br><span class="line">Warm <span class="number">124239</span></span><br><span class="line">Cold           <span class="number">97822</span></span><br><span class="line">Boiling Hot    <span class="number">84790</span></span><br><span class="line">Hot            <span class="number">67508</span></span><br><span class="line">Lava Hot       <span class="number">64840</span></span><br><span class="line">NONE           <span class="number">18075</span></span><br><span class="line">Name: ord_2, dtype: int64</span><br></pre></td></tr></table></figure><p>哇！这一列中有 18075 个 NaN 值，而我们之前甚至都没有考虑使用它们。增加了这个新类别后，类别总数从 6 个增加到了 7 个。这没关系，因为现在我们在建立模型时，也会考虑 NaN。相关信息越多，模型就越好。</p><p>假设 <em>ord_2</em> 没有任何 NaN 值。我们可以看到，这一列中的所有类别都有显著的计数。其中没有 “罕见 “类别，即只在样本总数中占很小比例的类别。现在，让我们假设您在生产中部署了使用这一列的模型，当模型或项目上线时，您在 <em>ord_2</em> 列中得到了一个在训练中不存在的类别。在这种情况下，模型管道会抛出一个错误，您对此无能为力。如果出现这种情况，那么可能是生产中的管道出了问题。如果这是预料之中的，那么您就必须修改您的模型管道，并在这六个类别中加入一个新类别。</p><p>这个新类别被称为 “罕见 “类别。罕见类别是一种不常见的类别，可以包括许多不同的类别。您也可以尝试使用近邻模型来 “预测 “未知类别。请记住，如果您预测了这个类别，它就会成为训练数据中的一个类别。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page101_image.png" alt=""></p><p align="center"><b>图 3：具有不同特征且无标签的数据集示意图，其中一个特征可能会在测试集或实时数据中出现新值</b> </p><p>当我们有一个如图 3 所示的数据集时，我们可以建立一个简单的模型，对除 “f3 “之外的所有特征进行训练。这样，你将创建一个模型，在不知道或训练中没有 “f3 “时预测它。我不敢说这样的模型是否能带来出色的性能，但也许能处理测试集或实时数据中的缺失值，就像机器学习中的其他事情一样，不尝试一下是说不准的。</p><p>如果你有一个固定的测试集，你可以将测试数据添加到训练中，以了解给定特征中的类别。这与半监督学习非常相似，即使用无法用于训练的数据来改进模型。这也会照顾到在训练数据中出现次数极少但在测试数据中大量存在的稀有值。你的模型将更加稳健。</p><p>很多人认为这种想法会过度拟合。可能过拟合，也可能不过拟合。有一个简单的解决方法。如果你在设计交叉验证时，能够在测试数据上运行模型时复制预测过程，那么它就永远不会过拟合。这意味着第一步应该是分离折叠，在每个折叠中，你应该应用与测试数据相同的预处理。假设您想合并训练数据和测试数据，那么在每个折叠中，您必须合并训练数据和验证数据，并确保验证数据集复制了测试集。在这种特定情况下，您必须以这样一种方式设计验证集，使其包含训练集中 “未见 “的类别。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page102_image.png" alt=""></p><p align="center"><b>图 4：对训练集和测试集进行简单合并，以了解测试集中存在但训练集中不存在的类别或训练集中罕见的类别</b> </p><p>只要看一下图 4 和下面的代码，就能很容易理解其工作原理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="comment"># 读取训练集</span></span><br><span class="line">train = pd.read_csv(<span class="string">&quot;../input/cat_train.csv&quot;</span>)</span><br><span class="line"><span class="comment"># 读取测试集</span></span><br><span class="line">test = pd.read_csv(<span class="string">&quot;../input/cat_test.csv&quot;</span>)</span><br><span class="line"><span class="comment"># 将测试集&quot;target&quot;列全部置为-1</span></span><br><span class="line">test.loc[:, <span class="string">&quot;target&quot;</span>] = -<span class="number">1</span></span><br><span class="line"><span class="comment"># 将训练集、测试集沿行拼接</span></span><br><span class="line">data = pd.concat([train, test]).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 将除&quot;id&quot;和&quot;target&quot;列的其他特征列名取出</span></span><br><span class="line">features = [x <span class="keyword">for</span> x <span class="keyword">in</span> train.columns <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;target&quot;</span>]]</span><br><span class="line"><span class="comment"># 遍历特征</span></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> features:</span><br><span class="line">    <span class="comment"># 标签编码</span></span><br><span class="line">    lbl_enc = preprocessing.LabelEncoder()</span><br><span class="line">    <span class="comment"># 将空值替换为&quot;NONE&quot;,并将该列格式变为str</span></span><br><span class="line">    temp_col = data[feat].fillna(<span class="string">&quot;NONE&quot;</span>).astype(<span class="built_in">str</span>).values</span><br><span class="line">    <span class="comment"># 转换数值</span></span><br><span class="line">    data.loc[:, feat] = lbl_enc.fit_transform(temp_col)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据&quot;target&quot;列将训练集与测试集分开</span></span><br><span class="line">train = data[data.target != -<span class="number">1</span>].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">test = data[data.target == -<span class="number">1</span>].reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>当您遇到已经有测试数据集的问题时，这个技巧就会起作用。必须注意的是，这一招在实时环境中不起作用。例如，假设您所在的公司提供实时竞价解决方案（RTB）。RTB 系统会对在线看到的每个用户进行竞价，以购买广告空间。这种模式可使用的功能可能包括网站中浏览的页面。我们假设这些特征是用户访问的最后五个类别/页面。在这种情况下，如果网站引入了新的类别，我们将无法再准确预测。在这种情况下，我们的模型就会失效。这种情况可以通过使用 <strong>“未知 “类别来避免</strong>。</p><p>在我们的 cat-in-the-dat 数据集中，<em>ord_2</em> 列中已经有了未知类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.ord_2.fillna(<span class="string">&quot;NONE&quot;</span>).value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line">Freezing       <span class="number">142726</span></span><br><span class="line">Warm           <span class="number">124239</span></span><br><span class="line">Cold           <span class="number">97822</span></span><br><span class="line">Boiling Hot    <span class="number">84790</span></span><br><span class="line">Hot            <span class="number">67508</span></span><br><span class="line">Lava Hot       <span class="number">64840</span></span><br><span class="line">NONE           <span class="number">18075</span></span><br><span class="line">Name: ord_2, dtype: int64</span><br></pre></td></tr></table></figure><p>我们可以将 “NONE “视为未知。因此，如果在实时测试过程中，我们获得了以前从未见过的新类别，我们就会将其标记为 “NONE”。</p><p>这与自然语言处理问题非常相似。我们总是基于固定的词汇建立模型。增加词汇量就会增加模型的大小。像 BERT 这样的转换器模型是在 ~30000 个单词（英语）的基础上训练的。因此，当有新词输入时，我们会将其标记为 UNK（未知）。</p><p>因此，您可以假设测试数据与训练数据具有相同的类别，也可以在训练数据中引入罕见或未知类别，以处理测试数据中的新类别。</p><p>让我们看看填入 NaN 值后 ord_4 列的值计数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.ord_4.fillna(<span class="string">&quot;NONE&quot;</span>).value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line">N       <span class="number">39978</span></span><br><span class="line">P       <span class="number">37890</span></span><br><span class="line">Y       <span class="number">36657</span></span><br><span class="line">A       <span class="number">36633</span></span><br><span class="line">R       <span class="number">33045</span></span><br><span class="line">U       <span class="number">32897</span></span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">K       <span class="number">21676</span></span><br><span class="line">I       <span class="number">19805</span></span><br><span class="line">NONE    <span class="number">17930</span></span><br><span class="line">D       <span class="number">17284</span></span><br><span class="line">F       <span class="number">16721</span></span><br><span class="line">W       <span class="number">8268</span></span><br><span class="line">Z       <span class="number">5790</span></span><br><span class="line">S       <span class="number">4595</span></span><br><span class="line">G       <span class="number">3404</span></span><br><span class="line">V       <span class="number">3107</span></span><br><span class="line">J       <span class="number">1950</span></span><br><span class="line">L       <span class="number">1657</span></span><br><span class="line">Name: ord_4, dtype: int64</span><br></pre></td></tr></table></figure><p>我们看到，有些数值只出现了几千次，有些则出现了近 40000 次。NaN 也经常出现。请注意，我已经从输出中删除了一些值。</p><p>现在，我们可以定义将一个值称为 “<strong>罕见（rare）</strong> “的标准了。比方说，在这一列中，稀有值的要求是计数小于 2000。这样看来，J 和 L 就可以被标记为稀有值了。使用 pandas，根据计数阈值替换类别非常简单。让我们来看看它是如何实现的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.ord_4 = df.ord_4.fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">In [X]: df.loc[</span><br><span class="line">   ...:     df[<span class="string">&quot;ord_4&quot;</span>].value_counts()[df[<span class="string">&quot;ord_4&quot;</span>]].values &lt; <span class="number">2000</span>,</span><br><span class="line">   ...:    <span class="string">&quot;ord_4&quot;</span></span><br><span class="line">   ...: ] = <span class="string">&quot;RARE&quot;</span></span><br><span class="line">In [X]: df.ord_4.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line">N      <span class="number">39978</span></span><br><span class="line">P      <span class="number">37890</span></span><br><span class="line">Y      <span class="number">36657</span></span><br><span class="line">A      <span class="number">36633</span></span><br><span class="line">R      <span class="number">33045</span></span><br><span class="line">U      <span class="number">32897</span></span><br><span class="line">M      <span class="number">32504</span></span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">B      <span class="number">25212</span></span><br><span class="line">E      <span class="number">21871</span></span><br><span class="line">K      <span class="number">21676</span></span><br><span class="line">I      <span class="number">19805</span></span><br><span class="line">NONE   <span class="number">17930</span></span><br><span class="line">D      <span class="number">17284</span></span><br><span class="line">F      <span class="number">16721</span></span><br><span class="line">W       <span class="number">8268</span></span><br><span class="line">Z       <span class="number">5790</span></span><br><span class="line">S       <span class="number">4595</span></span><br><span class="line">RARE    <span class="number">3607</span></span><br><span class="line">G       <span class="number">3404</span></span><br><span class="line">V       <span class="number">3107</span></span><br><span class="line">Name: ord_4, dtype: int64</span><br></pre></td></tr></table></figure><p>我们认为，只要某个类别的值小于 2000，就将其替换为罕见。因此，现在在测试数据时，所有未见过的新类别都将被映射为 “RARE”，而所有缺失值都将被映射为 “NONE”。</p><p>这种方法还能确保即使有新的类别，模型也能在实际环境中正常工作。</p><p>现在，我们已经具备了处理任何带有分类变量问题所需的一切条件。让我们尝试建立第一个模型，并逐步提高其性能。</p><p>在构建任何类型的模型之前，交叉检验至关重要。我们已经看到了标签/目标分布，知道这是一个目标偏斜的二元分类问题。因此，我们将使用 StratifiedKFold 来分割数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 读取数据文件</span></span><br><span class="line">df = pd.read_csv(<span class="string">&quot;../input/cat_train.csv&quot;</span>)</span><br><span class="line">    <span class="comment"># 添加&quot;kfold&quot;列，并置为-1</span></span><br><span class="line">df[<span class="string">&quot;kfold&quot;</span>] = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 打乱数据顺序，重置索引</span></span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 将目标列取出</span></span><br><span class="line">y = df.target.values</span><br><span class="line">    <span class="comment"># 分层k折交叉检验</span></span><br><span class="line">kf = model_selection.StratifiedKFold(n_splits=<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> f, (t_, v_) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kf.split(X=df, y=y)):</span><br><span class="line">        <span class="comment"># 区分折叠</span></span><br><span class="line">df.loc[v_, <span class="string">&#x27;kfold&#x27;</span>] = f</span><br><span class="line">    <span class="comment"># 保存文件</span></span><br><span class="line">df.to_csv(<span class="string">&quot;../input/cat_train_folds.csv&quot;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>现在我们可以检查新的折叠 csv，查看每个折叠的样本数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [X]: <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">In [X]: df = pd.read_csv(<span class="string">&quot;../input/cat_train_folds.csv&quot;</span>)</span><br><span class="line">In [X]: df.kfold.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">4</span>   <span class="number">120000</span></span><br><span class="line"><span class="number">3</span>   <span class="number">120000</span></span><br><span class="line"><span class="number">2</span>   <span class="number">120000</span></span><br><span class="line"><span class="number">1</span>   <span class="number">120000</span></span><br><span class="line"><span class="number">0</span>   <span class="number">120000</span></span><br><span class="line">Name: kfold, dtype: int64</span><br></pre></td></tr></table></figure><p>所有折叠都有 120000 个样本。这是意料之中的，因为训练数据有 600000 个样本，而我们做了 5 次折叠。到目前为止，一切顺利。</p><p>现在，我们还可以检查每个折叠的目标分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df[df.kfold==<span class="number">0</span>].target.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>   <span class="number">97536</span></span><br><span class="line"><span class="number">1</span>   <span class="number">22464</span></span><br><span class="line">Name: target, dtype: int64</span><br><span class="line">In [X]: df[df.kfold==<span class="number">1</span>].target.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>   <span class="number">97536</span></span><br><span class="line"><span class="number">1</span>   <span class="number">22464</span></span><br><span class="line">Name: target, dtype: int64</span><br><span class="line">In [X]: df[df.kfold==<span class="number">2</span>].target.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>   <span class="number">97535</span></span><br><span class="line"><span class="number">1</span>   <span class="number">22465</span></span><br><span class="line">Name: target, dtype: int64</span><br><span class="line">In [X]: df[df.kfold==<span class="number">3</span>].target.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>   <span class="number">97535</span></span><br><span class="line"><span class="number">1</span>   <span class="number">22465</span></span><br><span class="line">Name: target, dtype: int64</span><br><span class="line">In [X]: df[df.kfold==<span class="number">4</span>].target.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>   <span class="number">97535</span></span><br><span class="line"><span class="number">1</span>   <span class="number">22465</span></span><br><span class="line">Name: target, dtype: int64</span><br></pre></td></tr></table></figure><p>我们看到，在每个折叠中，目标的分布都是一样的。这正是我们所需要的。它也可以是相似的，并不一定要一直相同。现在，当我们建立模型时，每个折叠中的标签分布都将相同。</p><p>我们可以建立的最简单的模型之一是对所有数据进行独热编码并使用逻辑回归。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    <span class="comment"># 读取分层k折交叉检验数据</span></span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/cat_train_folds.csv&quot;</span>)</span><br><span class="line">    <span class="comment"># 取除&quot;id&quot;, &quot;target&quot;, &quot;kfold&quot;外的其他特征列</span></span><br><span class="line">    features = [</span><br><span class="line">        f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;id&quot;</span>, <span class="string">&quot;target&quot;</span>, <span class="string">&quot;kfold&quot;</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># 遍历特征列表</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="comment"># 将空值置为&quot;NONE&quot;</span></span><br><span class="line">        df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">    <span class="comment"># 取训练集（kfold列中不为fold的样本，重置索引）</span></span><br><span class="line">    df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 取验证集（kfold列中为fold的样本，重置索引）</span></span><br><span class="line">    df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 独热编码</span></span><br><span class="line">    ohe = preprocessing.OneHotEncoder()</span><br><span class="line">    <span class="comment"># 将训练集、验证集沿行合并</span></span><br><span class="line">    full_data = pd.concat([df_train[features], df_valid[features]], axis=<span class="number">0</span>)</span><br><span class="line">    ohe.fit(full_data[features])</span><br><span class="line">    <span class="comment"># 转换训练集</span></span><br><span class="line">    x_train = ohe.transform(df_train[features])</span><br><span class="line">    <span class="comment"># 转换测试集</span></span><br><span class="line">    x_valid = ohe.transform(df_valid[features])</span><br><span class="line">    <span class="comment"># 逻辑回归</span></span><br><span class="line">    model = linear_model.LogisticRegression()</span><br><span class="line">    <span class="comment"># 使用训练集训练模型</span></span><br><span class="line">    model.fit(x_train, df_train.target.values)</span><br><span class="line">    <span class="comment"># 使用验证集得到预测标签</span></span><br><span class="line">    valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 计算auc指标</span></span><br><span class="line">    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(auc)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 运行折叠0</span></span><br><span class="line">    run(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>那么，发生了什么呢？</p><p>我们创建了一个函数，将数据分为训练和验证两部分，给定折叠数，处理 NaN 值，对所有数据进行单次编码，并训练一个简单的逻辑回归模型。</p><p>当我们运行这部分代码时，会产生如下输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">❯ python ohe_logres.py</span><br><span class="line">/home/abhishek/miniconda3/envs/ml/lib/python3<span class="number">.7</span>/site-</span><br><span class="line">packages/sklearn/linear_model/_logistic.py:<span class="number">939</span>: ConvergenceWarning: lbfgs</span><br><span class="line">failed to converge (status=<span class="number">1</span>):</span><br><span class="line">STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.</span><br><span class="line">Increase the number of iterations (max_iter) <span class="keyword">or</span> scale the data <span class="keyword">as</span> shown</span><br><span class="line"><span class="keyword">in</span>:</span><br><span class="line">https://scikit-learn.org/stable/modules/preprocessing.html.</span><br><span class="line">Please also refer to the documentation <span class="keyword">for</span> alternative solver options:</span><br><span class="line">https://scikit-learn.org/stable/modules/linear_model.html<span class="comment">#logistic-</span></span><br><span class="line">regression</span><br><span class="line">extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)</span><br><span class="line"><span class="number">0.7847865042255127</span></span><br></pre></td></tr></table></figure><p>有一些警告。逻辑回归似乎没有收敛到最大迭代次数。我们没有调整参数，所以没有问题。我们看到 AUC 为 0.785。</p><p>现在让我们对代码进行简单修改，运行所有折叠。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">....</span><br><span class="line"></span><br><span class="line">model = linear_model.LogisticRegression()</span><br><span class="line">model.fit(x_train, df_train.target.values)</span><br><span class="line">valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 循环运行0~4折</span></span><br><span class="line"><span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">run(fold_)</span><br></pre></td></tr></table></figure><p>请注意，我们并没有做很大的改动，所以我只显示了部分代码行，其中一些代码行有改动。</p><p>这就打印出了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python -W ignore ohe_logres.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.7847865042255127</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.7853553605899214</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.7879321942914885</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.7870315929550808</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.7864668243125608</span></span><br></pre></td></tr></table></figure><p>请注意，我使用”-W ignore “忽略了所有警告。</p><p>我们看到，AUC 分数在所有褶皱中都相当稳定。平均 AUC 为 0.78631449527。对于我们的第一个模型来说相当不错！</p><p>很多人在遇到这种问题时会首先使用基于树的模型，比如随机森林。在这个数据集中应用随机森林时，我们可以使用标签编码（label encoding），将每一列中的每个特征都转换为整数，而不是之前讨论过的独热编码。</p><p>这种编码与独热编码并无太大区别。让我们来看看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> ensemble</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/cat_train_folds.csv&quot;</span>)</span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;id&quot;</span>, <span class="string">&quot;target&quot;</span>, <span class="string">&quot;kfold&quot;</span>) ]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="comment"># 标签编码</span></span><br><span class="line">        lbl = preprocessing.LabelEncoder()</span><br><span class="line">    lbl.fit(df[col])</span><br><span class="line">df.loc[:, col] = lbl.transform(df[col])</span><br><span class="line">df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">x_train = df_train[features].values</span><br><span class="line">x_valid = df_valid[features].values</span><br><span class="line">    <span class="comment"># 随机森林模型</span></span><br><span class="line">model = ensemble.RandomForestClassifier(n_jobs=-<span class="number">1</span>)</span><br><span class="line">    model.fit(x_train, df_train.target.values)</span><br><span class="line">valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"><span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">run(fold_)</span><br></pre></td></tr></table></figure><p>我们使用 scikit-learn 中的随机森林，并取消了独热编码。我们使用标签编码代替独热编码。得分如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python lbl_rf.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.7167390828113697</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.7165459672958506</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.7159709909587376</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.7161589664189556</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.7156020216155978</span></span><br></pre></td></tr></table></figure><p>哇 巨大的差异！ 随机森林模型在没有任何超参数调整的情况下，表现要比简单的逻辑回归差很多。</p><p>这就是为什么我们总是应该先从简单模型开始的原因。随机森林模型的粉丝会从这里开始，而忽略逻辑回归模型，认为这是一个非常简单的模型，不能带来比随机森林更好的价值。这种人将会犯下大错。在我们实现随机森林的过程中，与逻辑回归相比，折叠需要更长的时间才能完成。因此，我们不仅损失了 AUC，还需要更长的时间来完成训练。请注意，使用随机森林进行推理也很耗时，而且占用的空间也更大。</p><p>如果我们愿意，也可以尝试在稀疏的独热编码数据上运行随机森林，但这会耗费大量时间。我们还可以尝试使用奇异值分解来减少稀疏的独热编码矩阵。这是自然语言处理中提取主题的常用方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> decomposition</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> ensemble</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">df = pd.read_csv(<span class="string">&quot;../input/cat_train_folds.csv&quot;</span>)</span><br><span class="line">features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;id&quot;</span>, <span class="string">&quot;target&quot;</span>, <span class="string">&quot;kfold&quot;</span>)]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 独热编码</span></span><br><span class="line">ohe = preprocessing.OneHotEncoder()</span><br><span class="line">full_data = pd.concat([df_train[features], df_valid[features]], axis=<span class="number">0</span>)</span><br><span class="line">ohe.fit(full_data[features])</span><br><span class="line"></span><br><span class="line">x_train = ohe.transform(df_train[features])</span><br><span class="line">x_valid = ohe.transform(df_valid[features])</span><br><span class="line">    <span class="comment"># 奇异值分解</span></span><br><span class="line">svd = decomposition.TruncatedSVD(n_components=<span class="number">120</span>)</span><br><span class="line">    full_sparse = sparse.vstack((x_train, x_valid))</span><br><span class="line">    svd.fit(full_sparse)</span><br><span class="line">    x_train = svd.transform(x_train)</span><br><span class="line">    x_valid = svd.transform(x_valid)</span><br><span class="line">model = ensemble.RandomForestClassifier(n_jobs=-<span class="number">1</span>)</span><br><span class="line">    model.fit(x_train, df_train.target.values)</span><br><span class="line">    valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"><span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">run(fold_)</span><br></pre></td></tr></table></figure><p>我们对全部数据进行独热编码，然后用训练数据和验证数据在稀疏矩阵上拟合 scikit-learn 的 TruncatedSVD。这样，我们将高维稀疏矩阵减少到 120 个特征，然后拟合随机森林分类器。</p><p>以下是该模型的输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python ohe_svd_rf.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.7064863038754249</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.706050102937374</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.7086069243167242</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.7066819080085971</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.7058154015055585</span></span><br></pre></td></tr></table></figure><p>我们发现情况更糟。看来，解决这个问题的最佳方法是使用逻辑回归和独热编码。随机森林似乎耗时太多。也许我们可以试试 XGBoost。如果你不知道 XGBoost，它是最流行的梯度提升算法之一。由于它是一种基于树的算法，我们将使用标签编码数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/cat_train_folds.csv&quot;</span>)</span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;id&quot;</span>, <span class="string">&quot;target&quot;</span>, <span class="string">&quot;kfold&quot;</span>) ]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="comment"># 标签编码</span></span><br><span class="line">        lbl = preprocessing.LabelEncoder()</span><br><span class="line">        lbl.fit(df[col])</span><br><span class="line">        df.loc[:, col] = lbl.transform(df[col])</span><br><span class="line"></span><br><span class="line">df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">x_train = df_train[features].values</span><br><span class="line">x_valid = df_valid[features].values</span><br><span class="line">    <span class="comment"># XGBoost模型</span></span><br><span class="line">model = xgb.XGBClassifier(</span><br><span class="line">        n_jobs=-<span class="number">1</span>,</span><br><span class="line">        max_depth=<span class="number">7</span>,</span><br><span class="line">        n_estimators=<span class="number">200</span>)</span><br><span class="line">    model.fit(x_train, df_train.target.values)</span><br><span class="line">    valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">        <span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">            run(fold_)</span><br></pre></td></tr></table></figure><p>必须指出的是，在这段代码中，我对 xgboost 参数做了一些修改。xgboost 的默认最大深度（max_depth）是 3，我把它改成了 7，还把估计器数量（n_estimators）从 100 改成了 200。</p><p>该模型的 5 折交叉检验得分如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python lbl_xgb.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.7656768851999011</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.7633006564148015</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.7654277821434345</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.7663609758878182</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.764914671468069</span></span><br></pre></td></tr></table></figure><p>我们可以看到，在不做任何调整的情况下，我们的得分比普通随机森林要高得多。</p><p>您还可以尝试一些特征工程，放弃某些对模型没有任何价值的列等。但似乎我们能做的不多，无法证明模型的改进。让我们把数据集换成另一个有大量分类变量的数据集。另一个有名的数据集是<strong>美国成人人口普查数据（US adult census data）</strong>。这个数据集包含一些特征，而你的任务是预测工资等级。让我们来看看这个数据集。图 5 显示了该数据集中的一些列。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page119_image.png" alt=""></p><p align="center"><b>图 5：部分数据集展示</b> </p><p>该数据集有以下几列：</p><ul><li><p>年龄（age）</p></li><li><p>工作类别（workclass）</p></li><li>学历（fnlwgt）</li><li>教育程度（education）</li><li>教育程度（education.num）</li><li>婚姻状况（marital.status）</li><li>职业（occupation）</li><li>关系（relationship）</li><li>种族（race）</li><li>性别（sex）</li><li>资本收益（capital.gain）</li><li>资本损失（capital.loss）</li><li>每周小时数（hours.per.week）</li><li>原籍国（native.country）</li><li>收入（income）</li></ul><p>这些特征大多不言自明。那些不明白的，我们可以不考虑。让我们先尝试建立一个模型。</p><p>我们看到收入列是一个字符串。让我们对这一列进行数值统计。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [X]: <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">In [X]: df = pd.read_csv(<span class="string">&quot;../input/adult.csv&quot;</span>)</span><br><span class="line">In [X]: df.income.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line">&lt;=50K   <span class="number">24720</span></span><br><span class="line">&gt;50K     <span class="number">7841</span></span><br></pre></td></tr></table></figure><p>我们可以看到，有 7841 个实例的收入超过 5 万美元。这占样本总数的 24%。因此，我们将保持与猫数据集相同的评估方法，即 AUC。 在开始建模之前，为了简单起见，我们将去掉几列特征，即</p><ul><li>学历（fnlwgt）</li><li>年龄（age）</li><li>资本收益（capital.gain）</li><li>资本损失（capital.loss）</li><li>每周小时数（hours.per.week）</li></ul><p>让我们试着用逻辑回归和独热编码器，看看会发生什么。第一步总是要进行交叉验证。我不会在这里展示这部分代码。留待读者练习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/adult_folds.csv&quot;</span>)</span><br><span class="line">    <span class="comment"># 需要删除的列</span></span><br><span class="line">    num_cols = [</span><br><span class="line">        <span class="string">&quot;fnlwgt&quot;</span>,</span><br><span class="line">        <span class="string">&quot;age&quot;</span>,</span><br><span class="line">        <span class="string">&quot;capital.gain&quot;</span>,</span><br><span class="line">        <span class="string">&quot;capital.loss&quot;</span>,</span><br><span class="line">        <span class="string">&quot;hours.per.week&quot;</span></span><br><span class="line">    ]</span><br><span class="line">    df = df.drop(num_cols, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 映射</span></span><br><span class="line">    target_mapping = &#123;</span><br><span class="line">        <span class="string">&quot;&lt;=50K&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;&gt;50K&quot;</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 使用映射替换</span></span><br><span class="line">    df.loc[:, <span class="string">&quot;income&quot;</span>] = df.income.<span class="built_in">map</span>(target_mapping)</span><br><span class="line">    <span class="comment"># 取除&quot;kfold&quot;, &quot;income&quot;列的其他列名</span></span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;kfold&quot;</span>, <span class="string">&quot;income&quot;</span>) ]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="comment"># 将空值替换为&quot;NONE&quot;</span></span><br><span class="line">        df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">    <span class="comment"># 取训练集（kfold列中不为fold的样本，重置索引）</span></span><br><span class="line">df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 取验证集（kfold列中为fold的样本，重置索引）</span></span><br><span class="line">df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 独热编码</span></span><br><span class="line">    ohe = preprocessing.OneHotEncoder()</span><br><span class="line">    <span class="comment"># 将训练集、测试集沿行合并</span></span><br><span class="line">full_data = pd.concat([df_train[features], df_valid[features]], axis=<span class="number">0</span>)</span><br><span class="line">ohe.fit(full_data[features])</span><br><span class="line">    <span class="comment"># 转换训练集</span></span><br><span class="line">x_train = ohe.transform(df_train[features])</span><br><span class="line">    <span class="comment"># 转换验证集</span></span><br><span class="line">x_valid = ohe.transform(df_valid[features])</span><br><span class="line">    <span class="comment"># 构建逻辑回归模型</span></span><br><span class="line">model = linear_model.LogisticRegression()</span><br><span class="line">    <span class="comment"># 使用训练集训练模型</span></span><br><span class="line">model.fit(x_train, df_train.income.values)</span><br><span class="line">    <span class="comment"># 使用验证集得到预测标签</span></span><br><span class="line">valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 计算auc指标</span></span><br><span class="line">auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 运行0~4折</span></span><br><span class="line">    <span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        run(fold_)</span><br></pre></td></tr></table></figure><p>当我们运行这段代码时，我们会得到</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python -W ignore ohe_logres.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.8794809708119079</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.8875785068274882</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.8852609687685753</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.8681236223251438</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.8728581541840037</span></span><br></pre></td></tr></table></figure><p>对于一个如此简单的模型来说，这是一个非常不错的 AUC！<br>现在，让我们在不调整任何超参数的情况下尝试一下标签编码的 xgboost。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/adult_folds.csv&quot;</span>)</span><br><span class="line">    num_cols = [ <span class="string">&quot;fnlwgt&quot;</span>,</span><br><span class="line">                <span class="string">&quot;age&quot;</span>,</span><br><span class="line">                <span class="string">&quot;capital.gain&quot;</span>,</span><br><span class="line">                <span class="string">&quot;capital.loss&quot;</span>,</span><br><span class="line">                <span class="string">&quot;hours.per.week&quot;</span></span><br><span class="line">               ]</span><br><span class="line">    df = df.drop(num_cols, axis=<span class="number">1</span>)</span><br><span class="line">    target_mapping = &#123;</span><br><span class="line">        <span class="string">&quot;&lt;=50K&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;&gt;50K&quot;</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">df.loc[:, <span class="string">&quot;income&quot;</span>] = df.income.<span class="built_in">map</span>(target_mapping)</span><br><span class="line">features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;kfold&quot;</span>, <span class="string">&quot;income&quot;</span>) ]</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">    df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="comment"># 标签编码</span></span><br><span class="line">lbl = preprocessing.LabelEncoder()</span><br><span class="line">    lbl.fit(df[col])</span><br><span class="line">    df.loc[:, col] = lbl.transform(df[col])</span><br><span class="line"></span><br><span class="line">df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">x_train = df_train[features].values</span><br><span class="line">x_valid = df_valid[features].values</span><br><span class="line">    <span class="comment"># XGBoost模型</span></span><br><span class="line">model = xgb.XGBClassifier(n_jobs=-<span class="number">1</span>)</span><br><span class="line">model.fit(x_train, df_train.income.values)</span><br><span class="line">    valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 运行0~4折</span></span><br><span class="line"><span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">run(fold_)</span><br></pre></td></tr></table></figure><p>让我们运行上面代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python lbl_xgb.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.8800810634234078</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.886811884948154</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.8854421433318472</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.8676319549361007</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.8714450054900602</span></span><br></pre></td></tr></table></figure><p>这看起来已经相当不错了。让我们看看 <em>max_depth</em> 增加到 7 和 <em>n_estimators</em> 增加到 200 时的得分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python lbl_xgb.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.8764108944332032</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.8840708537662638</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.8816601162613102</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.8662335762581732</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.8698983461709926</span></span><br></pre></td></tr></table></figure><p>看起来并没有改善。</p><p>这表明，一个数据集的参数不能移植到另一个数据集。我们必须再次尝试调整参数，但我们将在接下来的章节中详细说明。</p><p>现在，让我们尝试在不调整参数的情况下将数值特征纳入 xgboost 模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/adult_folds.csv&quot;</span>)</span><br><span class="line">    <span class="comment"># 加入数值特征</span></span><br><span class="line">    num_cols = [</span><br><span class="line">        <span class="string">&quot;fnlwgt&quot;</span>,</span><br><span class="line">        <span class="string">&quot;age&quot;</span>,</span><br><span class="line">        <span class="string">&quot;capital.gain&quot;</span>,</span><br><span class="line">        <span class="string">&quot;capital.loss&quot;</span>,</span><br><span class="line">        <span class="string">&quot;hours.per.week&quot;</span></span><br><span class="line">    ]</span><br><span class="line">    target_mapping = &#123;</span><br><span class="line">        <span class="string">&quot;&lt;=50K&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;&gt;50K&quot;</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    df.loc[:, <span class="string">&quot;income&quot;</span>] = df.income.<span class="built_in">map</span>(target_mapping)</span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;kfold&quot;</span>, <span class="string">&quot;income&quot;</span>) ]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> num_cols:</span><br><span class="line">            <span class="comment"># 将空值置为&quot;NONE&quot;</span></span><br><span class="line">            df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> num_cols:</span><br><span class="line">            <span class="comment"># 标签编码</span></span><br><span class="line">            lbl = preprocessing.LabelEncoder()</span><br><span class="line">            lbl.fit(df[col])</span><br><span class="line">            df.loc[:, col] = lbl.transform(df[col])</span><br><span class="line"></span><br><span class="line">df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">x_train = df_train[features].values</span><br><span class="line">x_valid = df_valid[features].values</span><br><span class="line">    <span class="comment"># XGBoost模型</span></span><br><span class="line">model = xgb.XGBClassifier(n_jobs=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    model.fit(x_train, df_train.income.values)</span><br><span class="line">    valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        run(fold_)</span><br></pre></td></tr></table></figure><p>因此，我们保留数字列，只是不对其进行标签编码。这样，我们的最终特征矩阵就由数字列（原样）和编码分类列组成了。任何基于树的算法都能轻松处理这种混合。</p><p>请注意，在使用基于树的模型时，我们不需要对数据进行归一化处理。不过，这一点非常重要，在使用线性模型（如逻辑回归）时不容忽视。</p><p>现在让我们运行这个脚本！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python lbl_xgb_num.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.9209790185449889</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.9247157449144706</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.9269329887598243</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.9119349082169275</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.9166408030141667</span></span><br></pre></td></tr></table></figure><p>哇哦</p><p>这是一个很好的分数！</p><p>现在，我们可以尝试添加一些功能。我们将提取所有分类列，并创建所有二度组合。请看下面代码段中的 feature_engineering 函数，了解如何实现这一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">feature_engineering</span>(<span class="params">df, cat_cols</span>):</span><br><span class="line">    <span class="comment"># 生成两个特征的组合</span></span><br><span class="line">    combi = <span class="built_in">list</span>(itertools.combinations(cat_cols, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> c1, c2 <span class="keyword">in</span> combi:</span><br><span class="line">        df.loc[:, c1 + <span class="string">&quot;_&quot;</span> + c2] = df[c1].astype(<span class="built_in">str</span>) + <span class="string">&quot;_&quot;</span> + df[c2].astype(<span class="built_in">str</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/adult_folds.csv&quot;</span>)</span><br><span class="line">    num_cols = [ <span class="string">&quot;fnlwgt&quot;</span>,</span><br><span class="line">                <span class="string">&quot;age&quot;</span>,</span><br><span class="line">                <span class="string">&quot;capital.gain&quot;</span>,</span><br><span class="line">                <span class="string">&quot;capital.loss&quot;</span>,</span><br><span class="line">                <span class="string">&quot;hours.per.week&quot;</span></span><br><span class="line">               ]</span><br><span class="line">    target_mapping = &#123;</span><br><span class="line">        <span class="string">&quot;&lt;=50K&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;&gt;50K&quot;</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">df.loc[:, <span class="string">&quot;income&quot;</span>] = df.income.<span class="built_in">map</span>(target_mapping)</span><br><span class="line">cat_cols = [c <span class="keyword">for</span> c <span class="keyword">in</span> df.columns <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> num_cols <span class="keyword">and</span> c <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;kfold&quot;</span>, <span class="string">&quot;income&quot;</span>)]</span><br><span class="line">    <span class="comment"># 特征工程</span></span><br><span class="line">    df = feature_engineering(df, cat_cols)</span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;kfold&quot;</span>, <span class="string">&quot;income&quot;</span>)]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> num_cols:</span><br><span class="line">            df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> num_cols:</span><br><span class="line">            lbl = preprocessing.LabelEncoder()</span><br><span class="line">            lbl.fit(df[col])</span><br><span class="line">            df.loc[:, col] = lbl.transform(df[col])</span><br><span class="line"></span><br><span class="line">df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">x_train = df_train[features].values</span><br><span class="line">x_valid = df_valid[features].values</span><br><span class="line">model = xgb.XGBClassifier(n_jobs=-<span class="number">1</span>)</span><br><span class="line">model.fit(x_train, df_train.income.values)</span><br><span class="line">    valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        run(fold_)</span><br></pre></td></tr></table></figure><p>这是从分类列中创建特征的一种非常幼稚的方法。我们应该仔细研究数据，看看哪些组合最合理。如果使用这种方法，最终可能会创建大量特征，在这种情况下，就需要使用某种特征选择来选出最佳特征。稍后我们将详细介绍特征选择。现在让我们来看看分数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python lbl_xgb_num_feat.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.9211483465031423</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.9251499446866125</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.9262344766486692</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.9114264068794995</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.9177914453099201</span></span><br></pre></td></tr></table></figure><p>看来，即使不改变任何超参数，只增加一些特征，我们也能提高一些折叠得分。让我们看看将 <em>max_depth</em> 增加到 7 是否有帮助。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python lbl_xgb_num_feat.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.9286668430204137</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.9329340656165378</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.9319817543218744</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.919046187194538</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.9245692057162671</span></span><br></pre></td></tr></table></figure><p>我们再次改进了我们的模型。</p><p>请注意，我们还没有使用稀有值、二值化、独热编码和标签编码特征的组合以及其他几种方法。</p><p>从分类特征中进行特征工程的另一种方法是使用<strong>目标编码</strong>。但是，您必须非常小心，因为这可能会使您的模型过度拟合。目标编码是一种将给定特征中的每个类别映射到其平均目标值的技术，但必须始终以交叉验证的方式进行。这意味着首先要创建折叠，然后使用这些折叠为数据的不同列创建目标编码特征，方法与在折叠上拟合和预测模型的方法相同。因此，如果您创建了 5 个折叠，您就必须创建 5 次目标编码，这样最终，您就可以为每个折叠中的变量创建编码，而这些变量并非来自同一个折叠。然后在拟合模型时，必须再次使用相同的折叠。未见测试数据的目标编码可以来自全部训练数据，也可以是所有 5 个折叠的平均值。</p><p>让我们看看如何在同一个成人数据集上使用目标编码，以便进行比较。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mean_target_encoding</span>(<span class="params">data</span>):</span><br><span class="line">    df = copy.deepcopy(data)</span><br><span class="line">    num_cols = [ <span class="string">&quot;fnlwgt&quot;</span>,</span><br><span class="line">                <span class="string">&quot;age&quot;</span>,</span><br><span class="line">                <span class="string">&quot;capital.gain&quot;</span>,</span><br><span class="line">                <span class="string">&quot;capital.loss&quot;</span>,</span><br><span class="line">                <span class="string">&quot;hours.per.week&quot;</span>]</span><br><span class="line">    target_mapping = &#123;</span><br><span class="line">        <span class="string">&quot;&lt;=50K&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;&gt;50K&quot;</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    df.loc[:, <span class="string">&quot;income&quot;</span>] = df.income.<span class="built_in">map</span>(target_mapping)</span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;kfold&quot;</span>, <span class="string">&quot;income&quot;</span>) <span class="keyword">and</span> f <span class="keyword">not</span> <span class="keyword">in</span> num_cols]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> num_cols:</span><br><span class="line">            df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> num_cols:</span><br><span class="line">            <span class="comment"># 标签编码</span></span><br><span class="line">            lbl = preprocessing.LabelEncoder()</span><br><span class="line">            lbl.fit(df[col])</span><br><span class="line">            df.loc[:, col] = lbl.transform(df[col])</span><br><span class="line"></span><br><span class="line">encoded_dfs = []</span><br><span class="line">    <span class="keyword">for</span> fold <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">        df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">for</span> column <span class="keyword">in</span> features:</span><br><span class="line">            <span class="comment"># 目标编码</span></span><br><span class="line">            mapping_dict = <span class="built_in">dict</span>(df_train.groupby(column)[<span class="string">&quot;income&quot;</span>].mean() )</span><br><span class="line">            df_valid.loc[:, column + <span class="string">&quot;_enc&quot;</span>] = df_valid[column].<span class="built_in">map</span>(mapping_dict)</span><br><span class="line">        encoded_dfs.append(df_valid)</span><br><span class="line">encoded_df = pd.concat(encoded_dfs, axis=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">return</span> encoded_df</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">df, fold</span>):</span><br><span class="line">    df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;kfold&quot;</span>, <span class="string">&quot;income&quot;</span>) ]</span><br><span class="line">    x_train = df_train[features].values</span><br><span class="line">    x_valid = df_valid[features].values</span><br><span class="line">    model = xgb.XGBClassifier( n_jobs=-<span class="number">1</span>, max_depth=<span class="number">7</span>)</span><br><span class="line">    model.fit(x_train, df_train.income.values)</span><br><span class="line">    valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/adult_folds.csv&quot;</span>)</span><br><span class="line">    df = mean_target_encoding(df)</span><br><span class="line">    <span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        run(df, fold_)</span><br></pre></td></tr></table></figure><p>必须指出的是，在上述片段中，我在进行目标编码时并没有删除分类列。我保留了所有特征，并在此基础上添加了目标编码特征。此外，我还使用了平均值。您可以使用平均值、中位数、标准偏差或目标的任何其他函数。</p><p>让我们看看结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.9332240662017529</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.9363551625140347</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.9375013544556173</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.92237621307625</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.9292131180445478</span></span><br></pre></td></tr></table></figure><p>不错！看来我们又有进步了。不过，使用目标编码时必须非常小心，因为它太容易出现过度拟合。当我们使用目标编码时，最好使用某种平滑方法或在编码值中添加噪声。 Scikit-learn 的贡献库中有带平滑的目标编码，你也可以创建自己的平滑。平滑会引入某种正则化，有助于避免模型过度拟合。这并不难。</p><p>处理分类特征是一项复杂的任务。许多资源中都有大量信息。本章应该能帮助你开始解决分类变量的任何问题。不过，对于大多数问题来说，除了独热编码和标签编码之外，你不需要更多的东西。 要进一步改进模型，你可能需要更多！</p><p>在本章的最后，我们不能不在这些数据上使用神经网络。因此，让我们来看看一种称为<strong>实体嵌入</strong>的技术。在实体嵌入中，类别用向量表示。在二值化和独热编码方法中，我们都是用向量来表示类别的。 但是，如果我们有数以万计的类别怎么办？这将会产生巨大的矩阵，我们将需要很长时间来训练复杂的模型。因此，我们可以用带有浮点值的向量来表示它们。</p><p>这个想法非常简单。每个分类特征都有一个嵌入层。因此，一列中的每个类别现在都可以映射到一个嵌入层（就像在自然语言处理中将单词映射到嵌入层一样）。然后，根据其维度重塑这些嵌入层，使其扁平化，然后将所有扁平化的输入嵌入层连接起来。然后添加一堆密集层和一个输出层，就大功告成了。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page136_image.png" alt=""></p><p align="center"><b>图 6：类别转换为浮点或嵌入向量</b> </p><p>出于某种原因，我发现使用 TF/Keras 可以非常容易地做到这一点。因此，让我们来看看如何使用 TF/Keras 实现它。此外，这是本书中唯一一个使用 TF/Keras 的示例，将其转换为 PyTorch（使用 cat-in-the-dat-ii 数据集）也非常容易</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics, preprocessing</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model, load_model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> callbacks</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> utils</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_model</span>(<span class="params">data, catcols</span>):</span><br><span class="line">    <span class="comment"># 创建空的输入列表和输出列表，用于存储模型的输入和输出</span></span><br><span class="line">    inputs = []</span><br><span class="line">    outputs = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历分类特征列表中的每个特征</span></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> catcols:</span><br><span class="line">        <span class="comment"># 计算特征中唯一值的数量</span></span><br><span class="line">        num_unique_values = <span class="built_in">int</span>(data[c].nunique())</span><br><span class="line">        <span class="comment"># 计算嵌入维度，最大不超过50</span></span><br><span class="line">        embed_dim = <span class="built_in">int</span>(<span class="built_in">min</span>(np.ceil((num_unique_values) / <span class="number">2</span>), <span class="number">50</span>))</span><br><span class="line">        <span class="comment"># 创建模型的输入层，每个特征对应一个输入</span></span><br><span class="line">        inp = layers.Input(shape=(<span class="number">1</span>,))</span><br><span class="line">        <span class="comment"># 创建嵌入层，将分类特征映射到低维度的连续向量</span></span><br><span class="line">        out = layers.Embedding(num_unique_values + <span class="number">1</span>, embed_dim, name=c)(inp)</span><br><span class="line">        <span class="comment"># 对嵌入层进行空间丢弃（Dropout）</span></span><br><span class="line">        out = layers.SpatialDropout1D(<span class="number">0.3</span>)(out)</span><br><span class="line">        <span class="comment"># 将嵌入层的形状重新调整为一维</span></span><br><span class="line">        out = layers.Reshape(target_shape=(embed_dim,))(out)</span><br><span class="line">        <span class="comment"># 将输入和输出添加到对应的列表中</span></span><br><span class="line">        inputs.append(inp)</span><br><span class="line">        outputs.append(out)</span><br><span class="line">    <span class="comment"># 使用Concatenate层将所有的嵌入层输出连接在一起</span></span><br><span class="line">    x = layers.Concatenate()(outputs)</span><br><span class="line">    <span class="comment"># 对连接后的数据进行批量归一化</span></span><br><span class="line">    x = layers.BatchNormalization()(x)</span><br><span class="line">    <span class="comment"># 添加一个具有300个神经元的密集层，并使用ReLU激活函数</span></span><br><span class="line">    x = layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># 对该层的输出进行Dropout</span></span><br><span class="line">    x = layers.Dropout(<span class="number">0.3</span>)(x)</span><br><span class="line">    <span class="comment"># 再次进行批量归一化</span></span><br><span class="line">    x = layers.BatchNormalization()(x)</span><br><span class="line">    <span class="comment"># 添加另一个具有300个神经元的密集层，并使用ReLU激活函数</span></span><br><span class="line">    x = layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># 对该层的输出进行Dropout</span></span><br><span class="line">    x = layers.Dropout(<span class="number">0.3</span>)(x)</span><br><span class="line">    <span class="comment"># 再次进行批量归一化</span></span><br><span class="line">    x = layers.BatchNormalization()(x)</span><br><span class="line">    <span class="comment"># 输出层，具有2个神经元（用于二进制分类），并使用softmax激活函数</span></span><br><span class="line">    y = layers.Dense(<span class="number">2</span>, activation=<span class="string">&quot;softmax&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># 创建模型，将输入和输出传递给Model构造函数</span></span><br><span class="line">    model = Model(inputs=inputs, outputs=y)</span><br><span class="line">    <span class="comment"># 编译模型，指定损失函数和优化器</span></span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>)</span><br><span class="line">    <span class="comment"># 返回创建的模型</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/cat_train_folds.csv&quot;</span>)</span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;id&quot;</span>, <span class="string">&quot;target&quot;</span>, <span class="string">&quot;kfold&quot;</span>) ]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> features:</span><br><span class="line">        lbl_enc = preprocessing.LabelEncoder()</span><br><span class="line">        df.loc[:, feat] = lbl_enc.fit_transform(df[feat].values)</span><br><span class="line"></span><br><span class="line">    df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    model = create_model(df, features)</span><br><span class="line">    xtrain = [df_train[features].values[:, k] <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(features))]</span><br><span class="line">    xvalid = [df_valid[features].values[:, k] <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(features)) ]</span><br><span class="line">    ytrain = df_train.target.values</span><br><span class="line">    yvalid = df_valid.target.values</span><br><span class="line">    ytrain_cat = utils.to_categorical(ytrain)</span><br><span class="line">    yvalid_cat = utils.to_categorical(yvalid)</span><br><span class="line">    model.fit(xtrain,</span><br><span class="line">              ytrain_cat,</span><br><span class="line">              validation_data=(xvalid, yvalid_cat),</span><br><span class="line">              verbose=<span class="number">1</span>,</span><br><span class="line">              batch_size=<span class="number">1024</span>,</span><br><span class="line">              epochs=<span class="number">3</span></span><br><span class="line">             )</span><br><span class="line">    valid_preds = model.predict(xvalid)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="built_in">print</span>(metrics.roc_auc_score(yvalid, valid_preds))</span><br><span class="line">    K.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">run(<span class="number">0</span>)</span><br><span class="line">run(<span class="number">1</span>)</span><br><span class="line">run(<span class="number">2</span>)</span><br><span class="line">run(<span class="number">3</span>)</span><br><span class="line">run(<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>你会发现这种方法效果最好，而且如果你有 GPU，速度也超快！这种方法还可以进一步改进，而且你无需担心特征工程，因为神经网络会自行处理。在处理大量分类特征数据集时，这绝对值得一试。当嵌入大小与唯一类别的数量相同时，我们就可以使用独热编码（one-hot-encoding）。</p><p>本章基本上都是关于特征工程的。让我们在下一章中看看如何在数字特征和不同类型特征的组合方面进行更多的特征工程。</p>]]></content>
      
      
      <categories>
          
          <category> 杂学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>特征工程</title>
      <link href="/%E6%9D%82%E5%AD%A6/AAAMLprob/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
      <url>/%E6%9D%82%E5%AD%A6/AAAMLprob/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h1><p>特征工程是构建良好机器学习模型的最关键部分之一。如果我们拥有有用的特征，模型就会表现得更好。在许多情况下，您可以避免使用大型复杂模型，而使用具有关键工程特征的简单模型。我们必须牢记，只有当你对问题的领域有一定的了解，并且在很大程度上取决于相关数据时，才能以最佳方式完成特征工程。不过，您可以尝试使用一些通用技术，从几乎所有类型的数值变量和分类变量中创建特征。特征工程不仅仅是从数据中创建新特征，还包括不同类型的归一化和转换。</p><p>在有关分类特征的章节中，我们已经了解了结合不同分类变量的方法、如何将分类变量转换为计数、标签编码和使用嵌入。这些几乎都是利用分类变量设计特征的方法。因此，在本章中，我们的重点将仅限于数值变量以及数值变量和分类变量的组合。</p><p>让我们从最简单但应用最广泛的特征工程技术开始。假设你正在处理日期和时间数据。因此，我们有一个带有日期类型列的 pandas 数据帧。利用这一列，我们可以创建以下特征：</p><ul><li>年</li><li>年中的周</li><li>月</li><li>星期</li><li>周末</li><li>小时</li><li>还有更多</li></ul><p>而使用 pandas 就可以非常容易地做到这一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加&#x27;year&#x27;列，将 &#x27;datetime_column&#x27; 中的年份提取出来</span></span><br><span class="line">df.loc[:, <span class="string">&#x27;year&#x27;</span>] = df[<span class="string">&#x27;datetime_column&#x27;</span>].dt.year</span><br><span class="line"><span class="comment"># 添加&#x27;weekofyear&#x27;列，将 &#x27;datetime_column&#x27; 中的周数提取出来</span></span><br><span class="line">df.loc[:, <span class="string">&#x27;weekofyear&#x27;</span>] = df[<span class="string">&#x27;datetime_column&#x27;</span>].dt.weekofyear</span><br><span class="line"><span class="comment"># 添加&#x27;month&#x27;列，将 &#x27;datetime_column&#x27; 中的月份提取出来</span></span><br><span class="line">df.loc[:, <span class="string">&#x27;month&#x27;</span>] = df[<span class="string">&#x27;datetime_column&#x27;</span>].dt.month</span><br><span class="line"><span class="comment"># 添加&#x27;dayofweek&#x27;列，将 &#x27;datetime_column&#x27; 中的星期几提取出来</span></span><br><span class="line">df.loc[:, <span class="string">&#x27;dayofweek&#x27;</span>] = df[<span class="string">&#x27;datetime_column&#x27;</span>].dt.dayofweek</span><br><span class="line"><span class="comment"># 添加&#x27;weekend&#x27;列，判断当天是否为周末</span></span><br><span class="line">df.loc[:, <span class="string">&#x27;weekend&#x27;</span>] = (df.datetime_column.dt.weekday &gt;=<span class="number">5</span>).astype(<span class="built_in">int</span>)</span><br><span class="line"><span class="comment"># 添加 &#x27;hour&#x27; 列，将 &#x27;datetime_column&#x27; 中的小时提取出来</span></span><br><span class="line">df.loc[:, <span class="string">&#x27;hour&#x27;</span>] = df[<span class="string">&#x27;datetime_column&#x27;</span>].dt.hour</span><br></pre></td></tr></table></figure><p>因此，我们将使用日期时间列创建一系列新列。让我们来看看可以创建的一些示例功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 创建日期时间序列，包含了从 &#x27;2020-01-06&#x27; 到 &#x27;2020-01-10&#x27; 的日期时间点，时间间隔为10小时</span></span><br><span class="line">s = pd.date_range(<span class="string">&#x27;2020-01-06&#x27;</span>, <span class="string">&#x27;2020-01-10&#x27;</span>, freq=<span class="string">&#x27;10H&#x27;</span>).to_series()</span><br><span class="line"><span class="comment"># 提取对应时间特征</span></span><br><span class="line">features = &#123;</span><br><span class="line">    <span class="string">&quot;dayofweek&quot;</span>: s.dt.dayofweek.values,</span><br><span class="line">    <span class="string">&quot;dayofyear&quot;</span>: s.dt.dayofyear.values,</span><br><span class="line">    <span class="string">&quot;hour&quot;</span>: s.dt.hour.values,</span><br><span class="line">    <span class="string">&quot;is_leap_year&quot;</span>: s.dt.is_leap_year.values,</span><br><span class="line">    <span class="string">&quot;quarter&quot;</span>: s.dt.quarter.values,</span><br><span class="line">    <span class="string">&quot;weekofyear&quot;</span>: s.dt.weekofyear.values</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这将从给定系列中生成一个特征字典。您可以将此应用于 pandas 数据中的任何日期时间列。这些是 pandas 提供的众多日期时间特征中的一部分。在处理时间序列数据时，日期时间特征非常重要，例如，在预测一家商店的销售额时，如果想在聚合特征上使用 xgboost 等模型，日期时间特征就非常重要。</p><p>假设我们有一个如下所示的数据：</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page142_image.png" alt=""></p><p align="center"><b>图 1：包含分类和日期特征的样本数据</b> </p><p>在图 1 中，我们可以看到有一个日期列，从中可以轻松提取年、月、季度等特征。然后，我们有一个 customer_id 列，该列有多个条目，因此一个客户会被看到很多次（截图中看不到）。每个日期和客户 ID 都有三个分类特征和一个数字特征。我们可以从中创建大量特征：</p><ul><li>客户最活跃的月份是几月</li><li>某个客户的 cat1、cat2、cat3 的计数是多少</li><li>某年某月某周某客户的 cat1、cat2、cat3 数量是多少？</li><li>某个客户的 num1 平均值是多少？</li><li>等等。</li></ul><p>使用 pandas 中的聚合，可以很容易地创建类似的功能。让我们来看看如何实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_features</span>(<span class="params">df</span>):</span><br><span class="line">    df.loc[:, <span class="string">&#x27;year&#x27;</span>] = df[<span class="string">&#x27;date&#x27;</span>].dt.year</span><br><span class="line">    df.loc[:, <span class="string">&#x27;weekofyear&#x27;</span>] = df[<span class="string">&#x27;date&#x27;</span>].dt.weekofyear</span><br><span class="line">    df.loc[:, <span class="string">&#x27;month&#x27;</span>] = df[<span class="string">&#x27;date&#x27;</span>].dt.month</span><br><span class="line">    df.loc[:, <span class="string">&#x27;dayofweek&#x27;</span>] = df[<span class="string">&#x27;date&#x27;</span>].dt.dayofweek</span><br><span class="line">    df.loc[:, <span class="string">&#x27;weekend&#x27;</span>] = (df[<span class="string">&#x27;date&#x27;</span>].dt.weekday &gt;=<span class="number">5</span>).astype(<span class="built_in">int</span>)</span><br><span class="line">    aggs = &#123;&#125;</span><br><span class="line">    <span class="comment"># 对 &#x27;month&#x27; 列进行 nunique 和 mean 聚合</span></span><br><span class="line">    aggs[<span class="string">&#x27;month&#x27;</span>] = [<span class="string">&#x27;nunique&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>]</span><br><span class="line">    <span class="comment"># 对 &#x27;weekofyear&#x27; 列进行 nunique 和 mean 聚合</span></span><br><span class="line">    aggs[<span class="string">&#x27;weekofyear&#x27;</span>] = [<span class="string">&#x27;nunique&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>]</span><br><span class="line">    <span class="comment"># 对 &#x27;num1&#x27; 列进行 sum、max、min、mean 聚合</span></span><br><span class="line">    aggs[<span class="string">&#x27;num1&#x27;</span>] = [<span class="string">&#x27;sum&#x27;</span>,<span class="string">&#x27;max&#x27;</span>,<span class="string">&#x27;min&#x27;</span>,<span class="string">&#x27;mean&#x27;</span>]</span><br><span class="line">    <span class="comment"># 对 &#x27;customer_id&#x27; 列进行 size 聚合</span></span><br><span class="line">    aggs[<span class="string">&#x27;customer_id&#x27;</span>] = [<span class="string">&#x27;size&#x27;</span>]</span><br><span class="line">    <span class="comment"># 对 &#x27;customer_id&#x27; 列进行 nunique 聚合</span></span><br><span class="line">    aggs[<span class="string">&#x27;customer_id&#x27;</span>] = [<span class="string">&#x27;nunique&#x27;</span>]</span><br><span class="line">    <span class="comment"># 对数据应用不同的聚合函数</span></span><br><span class="line">    agg_df = df.groupby(<span class="string">&#x27;customer_id&#x27;</span>).agg(aggs)</span><br><span class="line">    <span class="comment"># 重置索引</span></span><br><span class="line">    agg_df = agg_df.reset_index()</span><br><span class="line"><span class="keyword">return</span> agg_df</span><br></pre></td></tr></table></figure><p>请注意，在上述函数中，我们跳过了分类变量，但您可以像使用其他聚合变量一样使用它们。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page144_image.png" alt=""></p><p align="center"><b>图 2：总体特征和其他特征</b> </p><p>现在，我们可以将图 2 中的数据与带有 customer_id 列的原始数据帧连接起来，开始训练模型。在这里，我们并不是要预测什么；我们只是在创建通用特征。不过，如果我们试图在这里预测什么，创建特征会更容易。</p><p>例如，有时在处理时间序列问题时，您可能需要的特征不是单个值，而是一系列值。 例如，客户在特定时间段内的交易。在这种情况下，我们会创建不同类型的特征，例如：使用数值特征时，在对分类列进行分组时，会得到类似于时间分布值列表的特征。在这种情况下，您可以创建一系列统计特征，例如</p><ul><li>平均值</li><li>最大值</li><li>最小值</li><li>独特性</li><li>偏斜</li><li>峰度</li><li>Kstat</li><li>百分位数</li><li>定量</li><li>峰值到峰值</li><li>以及更多</li></ul><p>这些可以使用简单的 numpy 函数创建，如下面的 python 代码段所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 创建字典，用于存储不同的统计特征</span></span><br><span class="line">feature_dict = &#123;&#125;</span><br><span class="line"><span class="comment"># 计算 x 中元素的平均值，并将结果存储在 feature_dict 中的 &#x27;mean&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;mean&#x27;</span>] = np.mean(x)</span><br><span class="line"><span class="comment"># 计算 x 中元素的最大值，并将结果存储在 feature_dict 中的 &#x27;max&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;max&#x27;</span>] = np.<span class="built_in">max</span>(x)</span><br><span class="line"><span class="comment"># 计算 x 中元素的最小值，并将结果存储在 feature_dict 中的 &#x27;min&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;min&#x27;</span>] = np.<span class="built_in">min</span>(x)</span><br><span class="line"><span class="comment"># 计算 x 中元素的标准差，并将结果存储在 feature_dict 中的 &#x27;std&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;std&#x27;</span>] = np.std(x)</span><br><span class="line"><span class="comment"># 计算 x 中元素的方差，并将结果存储在 feature_dict 中的 &#x27;var&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;var&#x27;</span>] = np.var(x)</span><br><span class="line"><span class="comment"># 计算 x 中元素的差值，并将结果存储在 feature_dict 中的 &#x27;ptp&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;ptp&#x27;</span>] = np.ptp(x)</span><br><span class="line"><span class="comment"># 计算 x 中元素的第10百分位数（即百分之10分位数），并将结果存储在 feature_dict 中的 &#x27;percentile_10&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;percentile_10&#x27;</span>] = np.percentile(x, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># 计算 x 中元素的第60百分位数，将结果存储在 feature_dict 中的 &#x27;percentile_60&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;percentile_60&#x27;</span>] = np.percentile(x, <span class="number">60</span>)</span><br><span class="line"><span class="comment"># 计算 x 中元素的第90百分位数，将结果存储在 feature_dict 中的 &#x27;percentile_90&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;percentile_90&#x27;</span>] = np.percentile(x, <span class="number">90</span>)</span><br><span class="line"><span class="comment"># 计算 x 中元素的5%分位数（即0.05分位数），将结果存储在 feature_dict 中的 &#x27;quantile_5&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;quantile_5&#x27;</span>] = np.quantile(x, <span class="number">0.05</span>)</span><br><span class="line"><span class="comment"># 计算 x 中元素的95%分位数（即0.95分位数），将结果存储在 feature_dict 中的 &#x27;quantile_95&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;quantile_95&#x27;</span>] = np.quantile(x, <span class="number">0.95</span>)</span><br><span class="line"><span class="comment"># 计算 x 中元素的99%分位数（即0.99分位数），将结果存储在 feature_dict 中的 &#x27;quantile_99&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;quantile_99&#x27;</span>] = np.quantile(x, <span class="number">0.99</span>)</span><br></pre></td></tr></table></figure><p>时间序列数据（数值列表）可以转换成许多特征。在这种情况下，一个名为 tsfresh 的 python 库非常有用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tsfresh.feature_extraction <span class="keyword">import</span> feature_calculators <span class="keyword">as</span> fc</span><br><span class="line"><span class="comment"># 计算 x 数列的绝对能量（abs_energy），并将结果存储在 feature_dict 字典中的 &#x27;abs_energy&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;abs_energy&#x27;</span>] = fc.abs_energy(x)</span><br><span class="line"><span class="comment"># 计算 x 数列中高于均值的数据点数量，将结果存储在 feature_dict 字典中的 &#x27;count_above_mean&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;count_above_mean&#x27;</span>] = fc.count_above_mean(x)</span><br><span class="line"><span class="comment"># 计算 x 数列中低于均值的数据点数量，将结果存储在 feature_dict 字典中的 &#x27;count_below_mean&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;count_below_mean&#x27;</span>] = fc.count_below_mean(x)</span><br><span class="line"><span class="comment"># 计算 x 数列的均值绝对变化（mean_abs_change），并将结果存储在 feature_dict 字典中的 &#x27;mean_abs_change&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;mean_abs_change&#x27;</span>] = fc.mean_abs_change(x)</span><br><span class="line"><span class="comment"># 计算 x 数列的均值变化率（mean_change），并将结果存储在 feature_dict 字典中的 &#x27;mean_change&#x27; 键下</span></span><br><span class="line">feature_dict[<span class="string">&#x27;mean_change&#x27;</span>] = fc.mean_change(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这还不是全部；tsfresh 提供了数百种特征和数十种不同特征的变体，你可以将它们用于基于时间序列（值列表）的特征。在上面的例子中，x 是一个值列表。但这还不是全部。您还可以为包含或不包含分类数据的数值数据创建许多其他特征。生成许多特征的一个简单方法就是创建一堆多项式特征。例如，从两个特征 “a “和 “b “生成的二级多项式特征包括 “a”、”b”、”ab”、”a^2 “和 “b^2”。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">df = pd.DataFrame(</span><br><span class="line">np.random.rand(<span class="number">100</span>, <span class="number">2</span>),</span><br><span class="line">columns=[<span class="string">f&quot;f_<span class="subst">&#123;i&#125;</span>&quot;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">3</span>)])</span><br></pre></td></tr></table></figure><p>如图 3 所示，它给出了一个数据表。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page146_image.png" alt=""></p><p align="center"><b>图 3：包含两个数字特征的随机数据表</b> </p><p>我们可以使用 scikit-learn 的 PolynomialFeatures 创建两次多项式特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="comment"># 指定多项式的次数为 2，不仅考虑交互项，不包括偏差（include_bias=False）</span></span><br><span class="line">pf = preprocessing.PolynomialFeatures(degree=<span class="number">2</span>,</span><br><span class="line">                                      interaction_only=<span class="literal">False</span>,</span><br><span class="line">                                      include_bias=<span class="literal">False</span></span><br><span class="line">                                     )</span><br><span class="line"><span class="comment"># 拟合，创建多项式特征</span></span><br><span class="line">pf.fit(df)</span><br><span class="line"><span class="comment"># 转换数据</span></span><br><span class="line">poly_feats = pf.transform(df)</span><br><span class="line"><span class="comment"># 获取生成的多项式特征的数量</span></span><br><span class="line">num_feats = poly_feats.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 为新生成的特征命名</span></span><br><span class="line">df_transformed = pd.DataFrame(poly_feats,columns=[<span class="string">f&quot;f_<span class="subst">&#123;i&#125;</span>&quot;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_feats + <span class="number">1</span>)] )</span><br></pre></td></tr></table></figure><p>这样就得到了一个数据表，如图 4 所示。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page147_image.png" alt=""></p><p align="center"><b>图 4：带有多项式特征的样本数据表</b> </p><p>现在，我们创建了一些多项式特征。如果创建的是三次多项式特征，最终总共会有九个特征。特征的数量越多，多项式特征的数量也就越多，而且你还必须记住，如果数据集中有很多样本，那么创建这类特征就需要花费一些时间。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page148_image.png" alt=""></p><p align="center"><b>图 5：数字特征列的直方图</b> </p><p>另一个有趣的功能是将数字转换为类别。这就是所谓的<strong>分箱</strong>。让我们看一下图 5，它显示了一个随机数字特征的样本直方图。我们在该图中使用了 10 个分箱，可以看到我们可以将数据分为 10 个部分。这可以使用 pandas 的 cat 函数来实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建10个分箱</span></span><br><span class="line">df[<span class="string">&quot;f_bin_10&quot;</span>] = pd.cut(df[<span class="string">&quot;f_1&quot;</span>], bins=<span class="number">10</span>, labels=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 创建100个分箱</span></span><br><span class="line">df[<span class="string">&quot;f_bin_100&quot;</span>] = pd.cut(df[<span class="string">&quot;f_1&quot;</span>], bins=<span class="number">100</span>, labels=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>如图 6 所示，这将在数据帧中生成两个新特征。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page148_image_1.png" alt=""></p><p align="center"><b>图 6：数值特征分箱</b> </p><p>当你进行分类时，可以同时使用分箱和原始特征。我们将在本章后半部分学习更多关于选择特征的知识。分箱还可以将数字特征视为分类特征。</p><p>另一种可以从数值特征中创建的有趣特征类型是对数变换。请看图 7 中的特征 f_3。</p><p>与其他方差较小的特征相比（假设如此），f_3 是一种方差非常大的特殊特征。因此，我们希望降低这一列的方差，这可以通过对数变换来实现。</p><p>f_3 列的值范围为 0 到 10000，直方图如图 8 所示。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page149_image_2.png" alt=""></p><p align="center"><b>图 8：特征 f_3 的直方图</b> </p><p>我们可以对这一列应用 log(1 + x) 来减少其方差。图 9 显示了应用对数变换后直方图的变化。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page150_image.png" alt=""></p><p align="center"><b>图 9：应用对数变换后的 f_3 直方图</b> </p><p>让我们来看看不使用对数变换和使用对数变换的方差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.f_3.var()</span><br><span class="line">Out[X]: <span class="number">8077265.875858586</span></span><br><span class="line">In [X]: df.f_3.apply(<span class="keyword">lambda</span> x: np.log(<span class="number">1</span> + x)).var()</span><br><span class="line">Out[X]: <span class="number">0.6058771732119975</span></span><br></pre></td></tr></table></figure><p>有时，也可以用指数来代替对数。一种非常有趣的情况是，您使用基于对数的评估指标，例如 RMSLE。在这种情况下，您可以在对数变换的目标上进行训练，然后在预测时使用指数值转换回原始值。这将有助于针对指标优化模型。</p><p>大多数情况下，这类数字特征都是基于直觉创建的。没有公式可循。如果您从事的是某一行业，您将创建特定行业的特征。</p><p>在处理分类变量和数值变量时，可能会遇到缺失值。在上一章中，我们介绍了一些处理分类特征中缺失值的方法，但还有更多方法可以处理缺失值/NaN 值。这也被视为特征工程。</p><p>如果在分类特征中遇到缺失值，就将其视为一个新的类别！这样做虽然简单，但（几乎）总是有效的！</p><p>在数值数据中填补缺失值的一种方法是选择一个在特定特征中没有出现的值，然后用它来填补。例如，假设特征中没有 0。这是其中一种方法，但可能不是最有效的。对于数值数据来说，比填充 0 更有效的方法之一是使用平均值进行填充。您也可以尝试使用该特征所有值的中位数来填充，或者使用最常见的值来填充缺失值。这样做的方法有很多。</p><p>填补缺失值的一种高级方法是使用 <strong>K 近邻法</strong>。 您可以选择一个有缺失值的样本，然后利用某种距离度量（例如欧氏距离）找到最近的邻居。然后取所有近邻的平均值来填补缺失值。您可以使用 KNN 来填补这样的缺失值。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page151_image.png" alt=""></p><p align="center"><b>图 10：有缺失值的二维数组</b> </p><p>让我们看看 KNN 是如何处理图 10 所示的缺失值矩阵的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> impute</span><br><span class="line"><span class="comment"># 生成维度为 (10, 6) 的随机整数矩阵 X，数值范围在 1 到 14 之间</span></span><br><span class="line">X = np.random.randint(<span class="number">1</span>, <span class="number">15</span>, (<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"><span class="comment"># 数据类型转换为 float</span></span><br><span class="line">X = X.astype(<span class="built_in">float</span>)</span><br><span class="line"><span class="comment"># 在矩阵 X 中随机选择 10 个位置，将这些位置的元素设置为 NaN（缺失值）</span></span><br><span class="line">X.ravel()[np.random.choice(X.size, <span class="number">10</span>, replace=<span class="literal">False</span>)] = np.nan</span><br><span class="line"><span class="comment"># 创建一个 KNNImputer 对象 knn_imputer，指定邻居数量为 2</span></span><br><span class="line">knn_imputer = impute.KNNImputer(n_neighbors=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># # 使用 knn_imputer 对矩阵 X 进行拟合和转换，用 K-最近邻方法填补缺失值</span></span><br><span class="line">knn_imputer.fit_transform(X)</span><br></pre></td></tr></table></figure><p>如图 11 所示，它填充了上述矩阵。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page152_image.png" alt=""></p><p align="center"><b>图 11：KNN估算的数值</b> </p><p>另一种弥补列缺失值的方法是训练回归模型，试图根据其他列预测某列的缺失值。因此，您可以从有缺失值的一列开始，将这一列作为无缺失值回归模型的目标列。现在，您可以使用所有其他列，对相关列中没有缺失值的样本进行模型训练，然后尝试预测之前删除的样本的目标列（同一列）。这样，基于模型的估算就会更加稳健。</p><p>请务必记住，对于基于树的模型，没有必要进行数值归一化，因为它们可以自行处理。</p><p>到目前为止，我所展示的只是创建一般特征的一些方法。现在，假设您正在处理一个预测不同商品（每周或每月）商店销售额的问题。您有商品，也有商店 ID。因此，您可以创建每个商店的商品等特征。现在，这是上文没有讨论的特征之一。这类特征不能一概而论，完全来自于领域、数据和业务知识。查看数据，找出适合的特征，然后创建相应的特征。如果您使用的是逻辑回归等线性模型或 SVM 等模型，请务必记住对特征进行缩放或归一化处理。基于树的模型无需对特征进行任何归一化处理即可正常工作。</p>]]></content>
      
      
      <categories>
          
          <category> 杂学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>特征选择</title>
      <link href="/%E6%9D%82%E5%AD%A6/AAAMLprob/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
      <url>/%E6%9D%82%E5%AD%A6/AAAMLprob/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</url>
      
        <content type="html"><![CDATA[<h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>当你创建了成千上万个特征后，就该从中挑选出几个了。但是，我们绝不应该创建成百上千个无用的特征。特征过多会带来一个众所周知的问题，即 “维度诅咒”。如果你有很多特征，你也必须有很多训练样本来捕捉所有特征。什么是 “大量 “并没有正确的定义，这需要您通过正确验证您的模型和检查训练模型所需的时间来确定。</p><p>选择特征的最简单方法是<strong>删除方差非常小的特征</strong>。如果特征的方差非常小（即非常接近于 0），它们就接近于常量，因此根本不会给任何模型增加任何价值。最好的办法就是去掉它们，从而降低复杂度。请注意，方差也取决于数据的缩放。 Scikit-learn 的 VarianceThreshold 实现了这一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line">data = ...</span><br><span class="line"><span class="comment"># 创建 VarianceThreshold 对象 var_thresh，指定方差阈值为 0.1</span></span><br><span class="line">var_thresh = VarianceThreshold(threshold=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># 使用 var_thresh 对数据 data 进行拟合和变换，将方差低于阈值的特征移除</span></span><br><span class="line">transformed_data = var_thresh.fit_transform(data)</span><br></pre></td></tr></table></figure><p>我们还可以删除相关性较高的特征。要计算不同数字特征之间的相关性，可以使用皮尔逊相关性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing</span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">data = fetch_california_housing()</span><br><span class="line"><span class="comment"># 从数据集中提取特征矩阵 X</span></span><br><span class="line">X = data[<span class="string">&quot;data&quot;</span>]</span><br><span class="line"><span class="comment"># 从数据集中提取特征的列名</span></span><br><span class="line">col_names = data[<span class="string">&quot;feature_names&quot;</span>]</span><br><span class="line"><span class="comment"># 从数据集中提取目标变量 y</span></span><br><span class="line">y = data[<span class="string">&quot;target&quot;</span>]</span><br><span class="line">df = pd.DataFrame(X, columns=col_names)</span><br><span class="line"><span class="comment"># 添加 MedInc_Sqrt 列，是 MedInc 列中每个元素进行平方根运算的结果</span></span><br><span class="line">df.loc[:, <span class="string">&quot;MedInc_Sqrt&quot;</span>] = df.MedInc.apply(np.sqrt)</span><br><span class="line"><span class="comment"># 计算皮尔逊相关性矩阵</span></span><br><span class="line">df.corr()</span><br></pre></td></tr></table></figure><p>得出相关矩阵，如图 1 所示。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page155_image.png" alt=""></p><p align="center"><b>图 1：皮尔逊相关矩阵样本</b> </p><p>我们看到，MedInc_Sqrt 与 MedInc 的相关性非常高。因此，我们可以删除其中一个特征。</p><p>现在我们可以转向一些<strong>单变量特征选择方法</strong>。单变量特征选择只不过是针对给定目标对每个特征进行评分。<strong>互信息</strong>、<strong>方差分析 F 检验和 chi2</strong> 是一些最常用的单变量特征选择方法。在 scikit- learn 中，有两种方法可以使用这些方法。</p><ul><li>SelectKBest：保留得分最高的 k 个特征</li><li>SelectPercentile：保留用户指定百分比内的顶级特征。</li></ul><p>必须注意的是，只有非负数据才能使用 chi2。在自然语言处理中，当我们有一些单词或基于 tf-idf 的特征时，这是一种特别有用的特征选择技术。最好为单变量特征选择创建一个包装器，几乎可以用于任何新问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_classif</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_regression</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> mutual_info_classif</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> mutual_info_regression</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectPercentile</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UnivariateFeatureSelction</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_features, problem_type, scoring</span>):</span><br><span class="line">        <span class="comment"># 若问题类型是分类问题</span></span><br><span class="line">        <span class="keyword">if</span> problem_type == <span class="string">&quot;classification&quot;</span>:</span><br><span class="line">            <span class="comment"># 创建字典 valid_scoring ，包含各种特征重要性衡量方式</span></span><br><span class="line">            valid_scoring = &#123;</span><br><span class="line">                <span class="string">&quot;f_classif&quot;</span>: f_classif,</span><br><span class="line">                <span class="string">&quot;chi2&quot;</span>: chi2,</span><br><span class="line">                <span class="string">&quot;mutual_info_classif&quot;</span>: mutual_info_classif</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="comment"># 若问题类型是回归问题</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 创建字典 valid_scoring，包含各种特征重要性衡量方式</span></span><br><span class="line">            valid_scoring = &#123;</span><br><span class="line">                <span class="string">&quot;f_regression&quot;</span>: f_regression,</span><br><span class="line">                <span class="string">&quot;mutual_info_regression&quot;</span>: mutual_info_regression</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="comment"># 检查特征重要性方式是否在字典中</span></span><br><span class="line"><span class="keyword">if</span> scoring <span class="keyword">not</span> <span class="keyword">in</span> valid_scoring:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">&quot;Invalid scoring function&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检查 n_features 的类型，如果是整数，则使用 SelectKBest 进行特征选择</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(n_features, <span class="built_in">int</span>):</span><br><span class="line">            self.selection = SelectKBest(</span><br><span class="line">                valid_scoring[scoring],</span><br><span class="line">                k=n_features</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 如果 n_features 是浮点数，则使用 SelectPercentile 进行特征选择</span></span><br><span class="line"><span class="keyword">elif</span> <span class="built_in">isinstance</span>(n_features, <span class="built_in">float</span>):</span><br><span class="line">self.selection = SelectPercentile(</span><br><span class="line">                valid_scoring[scoring],</span><br><span class="line">                percentile=<span class="built_in">int</span>(n_features * <span class="number">100</span>)</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 如果 n_features 类型无效，引发异常</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">raise</span> Exception(<span class="string">&quot;Invalid type of feature&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义 fit 方法，用于拟合特征选择器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y</span>):</span><br><span class="line"><span class="keyword">return</span> self.selection.fit(X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义 transform 方法，用于对数据进行特征选择转换</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transform</span>(<span class="params">self, X</span>):</span><br><span class="line"><span class="keyword">return</span> self.selection.transform(X)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 定义 fit_transform 方法，用于拟合特征选择器并同时进行特征选择转换</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit_transform</span>(<span class="params">self, X, y</span>):</span><br><span class="line"><span class="keyword">return</span> self.selection.fit_transform(X, y)</span><br></pre></td></tr></table></figure><p>使用该类非常简单。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化特征选择器，保留前10%的特征，回归问题，使用f_regression衡量特征重要性</span></span><br><span class="line">ufs = UnivariateFeatureSelction(n_features=<span class="number">0.1</span>,</span><br><span class="line">                                problem_type=<span class="string">&quot;regression&quot;</span>,</span><br><span class="line">                                scoring=<span class="string">&quot;f_regression&quot;</span></span><br><span class="line">                               )</span><br><span class="line"><span class="comment"># 拟合特征选择器</span></span><br><span class="line">ufs.fit(X, y)</span><br><span class="line"><span class="comment"># 特征转换</span></span><br><span class="line">X_transformed = ufs.transform(X)</span><br></pre></td></tr></table></figure><p>这样就能满足大部分单变量特征选择的需求。请注意，创建较少而重要的特征通常比创建数以百计的特征要好。单变量特征选择不一定总是表现良好。大多数情况下，人们更喜欢使用机器学习模型进行特征选择。让我们来看看如何做到这一点。</p><p>使用模型进行特征选择的最简单形式被称为贪婪特征选择。在贪婪特征选择中，第一步是选择一个模型。第二步是选择损失/评分函数。第三步也是最后一步是反复评估每个特征，如果能提高损失/评分，就将其添加到 “好 “特征列表中。没有比这更简单的了。但你必须记住，这被称为贪婪特征选择是有原因的。这种特征选择过程在每次评估特征时都会适合给定的模型。这种方法的计算成本非常高。完成这种特征选择也需要大量时间。如果不正确使用这种特征选择，甚至会导致模型过度拟合。</p><p>让我们来看看它是如何实现的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GreedyFeatureSelection</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义评估分数的方法，用于评估模型性能</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate_score</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        <span class="comment"># 逻辑回归模型</span></span><br><span class="line">        model = linear_model.LogisticRegression()</span><br><span class="line">        <span class="comment"># 训练模型</span></span><br><span class="line">        model.fit(X, y)</span><br><span class="line">        <span class="comment"># 预测概率值</span></span><br><span class="line">        predictions = model.predict_proba(X)[:, <span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 计算 AUC 分数</span></span><br><span class="line">        auc = metrics.roc_auc_score(y, predictions)</span><br><span class="line">        <span class="keyword">return</span> auc</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 特征选择函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_feature_selection</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        <span class="comment"># 初始化空列表，用于存储最佳特征和最佳分数</span></span><br><span class="line">        good_features = []</span><br><span class="line">        best_scores = []</span><br><span class="line">        <span class="comment"># 获取特征数量</span></span><br><span class="line">        num_features = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 开始特征选择的循环</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            this_feature = <span class="literal">None</span></span><br><span class="line">            best_score = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 遍历每个特征</span></span><br><span class="line">            <span class="keyword">for</span> feature <span class="keyword">in</span> <span class="built_in">range</span>(num_features):</span><br><span class="line">                <span class="keyword">if</span> feature <span class="keyword">in</span> good_features:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                selected_features = good_features + [feature]</span><br><span class="line">                xtrain = X[:, selected_features]</span><br><span class="line">                score = self.evaluate_score(xtrain, y)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 如果当前特征的得分优于之前的最佳得分，则更新</span></span><br><span class="line">                <span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">                    this_feature = feature</span><br><span class="line">                    best_score = score</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 若找到了新的最佳特征</span></span><br><span class="line">            <span class="keyword">if</span> this_feature != <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 特征添加到 good_features 列表</span></span><br><span class="line">                good_features.append(this_feature)</span><br><span class="line">                <span class="comment"># 得分添加到 best_scores 列表</span></span><br><span class="line">                best_scores.append(best_score)</span><br><span class="line">            <span class="comment"># 如果 best_scores 列表长度大于2，并且最后两个得分相比较差，则结束循环</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(best_scores) &gt; <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">if</span> best_scores[-<span class="number">1</span>] &lt; best_scores[-<span class="number">2</span>]:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 返回最佳特征的得分列表和最佳特征列表</span></span><br><span class="line"><span class="keyword">return</span> best_scores[:-<span class="number">1</span>], good_features[:-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义类的调用方法，用于执行特征选择</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        scores, features = self._feature_selection(X, y)</span><br><span class="line">        <span class="keyword">return</span> X[:, features], scores</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 生成一个示例的分类数据集 X 和标签 y</span></span><br><span class="line">    X, y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">100</span>)</span><br><span class="line">    <span class="comment"># 实例化 GreedyFeatureSelection 类，并使用 __call__ 方法进行特征选择</span></span><br><span class="line">    X_transformed, scores = GreedyFeatureSelection()(X, y)</span><br></pre></td></tr></table></figure><p>这种贪婪特征选择方法会返回分数和特征索引列表。图 2 显示了在每次迭代中增加一个新特征后，分数是如何提高的。我们可以看到，在某一点之后，我们就无法提高分数了，这就是我们停止的地方。</p><p>另一种贪婪的方法被称为递归特征消除法（RFE）。在前一种方法中，我们从一个特征开始，然后不断添加新的特征，但在 RFE 中，我们从所有特征开始，在每次迭代中不断去除一个对给定模型提供最小值的特征。但我们如何知道哪个特征的价值最小呢？如果我们使用线性支持向量机（SVM）或逻辑回归等模型，我们会为每个特征得到一个系数，该系数决定了特征的重要性。而对于任何基于树的模型，我们得到的是特征重要性，而不是系数。在每次迭代中，我们都可以剔除最不重要的特征，直到达到所需的特征数量为止。因此，我们可以决定要保留多少特征。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page161_image.png" alt=""></p><p align="center"><b>图 2：增加新特征后，贪婪特征选择的 AUC 分数如何变化</b> </p><p>当我们进行递归特征剔除时，在每次迭代中，我们都会剔除特征重要性较高的特征或系数接近 0 的特征。请记住，当你使用逻辑回归这样的模型进行二元分类时，如果特征对正分类很重要，其系数就会更正，而如果特征对负分类很重要，其系数就会更负。修改我们的贪婪特征选择类，创建一个新的递归特征消除类非常容易，但 scikit-learn 也提供了 RFE。下面的示例展示了一个简单的用法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing</span><br><span class="line">data = fetch_california_housing()</span><br><span class="line">X = data[<span class="string">&quot;data&quot;</span>]</span><br><span class="line">col_names = data[<span class="string">&quot;feature_names&quot;</span>]</span><br><span class="line">y = data[<span class="string">&quot;target&quot;</span>]</span><br><span class="line">model = LinearRegression()</span><br><span class="line"><span class="comment"># 创建 RFE（递归特征消除），指定模型为线性回归模型，要选择的特征数量为 3</span></span><br><span class="line">rfe = RFE(</span><br><span class="line">    estimator=model,</span><br><span class="line">    n_features_to_select=<span class="number">3</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">rfe.fit(X, y)</span><br><span class="line"><span class="comment"># 使用 RFE 选择的特征进行数据转换</span></span><br><span class="line">X_transformed = rfe.transform(X)</span><br></pre></td></tr></table></figure><p>我们看到了从模型中选择特征的两种不同的贪婪方法。但也可以根据数据拟合模型，然后通过特征系数或特征的重要性从模型中选择特征。如果使用系数，则可以选择一个阈值，如果系数高于该阈值，则可以保留该特征，否则将其剔除。</p><p>让我们看看如何从随机森林这样的模型中获取特征重要性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_diabetes</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line">data = load_diabetes()</span><br><span class="line">X = data[<span class="string">&quot;data&quot;</span>]</span><br><span class="line">col_names = data[<span class="string">&quot;feature_names&quot;</span>]</span><br><span class="line">y = data[<span class="string">&quot;target&quot;</span>]</span><br><span class="line"><span class="comment"># 实例化随机森林模型</span></span><br><span class="line">model = RandomForestRegressor()</span><br><span class="line"><span class="comment"># 拟合模型</span></span><br><span class="line">model.fit(X, y)</span><br></pre></td></tr></table></figure><p>随机森林（或任何模型）的特征重要性可按如下方式绘制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取特征重要性</span></span><br><span class="line">importances = model.feature_importances_</span><br><span class="line"><span class="comment"># 降序排列</span></span><br><span class="line">idxs = np.argsort(importances)</span><br><span class="line"><span class="comment"># 设定标题</span></span><br><span class="line">plt.title(<span class="string">&#x27;Feature Importances&#x27;</span>)</span><br><span class="line"><span class="comment"># 创建直方图</span></span><br><span class="line">plt.barh(<span class="built_in">range</span>(<span class="built_in">len</span>(idxs)), importances[idxs], align=<span class="string">&#x27;center&#x27;</span>)</span><br><span class="line"><span class="comment"># y轴标签</span></span><br><span class="line">plt.yticks(<span class="built_in">range</span>(<span class="built_in">len</span>(idxs)), [col_names[i] <span class="keyword">for</span> i <span class="keyword">in</span> idxs])</span><br><span class="line"><span class="comment"># x轴标签</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Random Forest Feature Importance&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果如图 3 所示。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page163_image.png" alt=""></p><p align="center"><b>图 3：特征重要性图</b> </p><p>从模型中选择最佳特征并不是什么新鲜事。您可以从一个模型中选择特征，然后使用另一个模型进行训练。例如，你可以使用逻辑回归系数来选择特征，然后使用随机森林（Random Forest）对所选特征进行模型训练。Scikit-learn 还提供了 SelectFromModel 类，可以帮助你直接从给定的模型中选择特征。您还可以根据需要指定系数或特征重要性的阈值，以及要选择的特征的最大数量。</p><p>请看下面的代码段，我们使用 SelectFromModel 中的默认参数来选择特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_diabetes</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line">data = load_diabetes()</span><br><span class="line">X = data[<span class="string">&quot;data&quot;</span>]</span><br><span class="line">col_names = data[<span class="string">&quot;feature_names&quot;</span>]</span><br><span class="line">y = data[<span class="string">&quot;target&quot;</span>]</span><br><span class="line"><span class="comment"># 创建随机森林模型回归模型</span></span><br><span class="line">model = RandomForestRegressor()</span><br><span class="line"><span class="comment"># 创建 SelectFromModel 对象 sfm，使用随机森林模型作为估算器</span></span><br><span class="line">sfm = SelectFromModel(estimator=model)</span><br><span class="line"><span class="comment"># 使用 sfm 对特征矩阵 X 和目标变量 y 进行特征选择</span></span><br><span class="line">X_transformed = sfm.fit_transform(X, y)</span><br><span class="line"><span class="comment"># 获取经过特征选择后的特征掩码（True 表示特征被选择，False 表示特征未被选择）</span></span><br><span class="line">support = sfm.get_support()</span><br><span class="line"><span class="comment"># 打印被选择的特征列名</span></span><br><span class="line"><span class="built_in">print</span>([x <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(col_names, support) <span class="keyword">if</span> y == <span class="literal">True</span> ])</span><br></pre></td></tr></table></figure><p>上面程序打印结果： [‘bmi’，’s5’]。我们再看图 3，就会发现这是最重要的两个特征。因此，我们也可以直接从随机森林提供的特征重要性中进行选择。我们还缺少一件事，那就是使用 <strong>L1（Lasso）惩罚模型</strong>进行特征选择。当我们使用 L1 惩罚进行正则化时，大部分系数都将为 0（或接近 0），因此我们要选择系数不为 0 的特征。只需将模型选择片段中的随机森林替换为支持 L1 惩罚的模型（如 lasso 回归）即可。所有基于树的模型都提供特征重要性，因此本章中展示的所有基于模型的片段都可用于 XGBoost、LightGBM 或 CatBoost。特征重要性函数的名称可能不同，产生结果的格式也可能不同，但用法是一样的。最后，在进行特征选择时必须小心谨慎。在训练数据上选择特征，并在验证数据上验证模型，以便在不过度拟合模型的情况下正确选择特征。</p>]]></content>
      
      
      <categories>
          
          <category> 杂学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>组织机器学习项目</title>
      <link href="/%E6%9D%82%E5%AD%A6/AAAMLprob/%E7%BB%84%E7%BB%87%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/"/>
      <url>/%E6%9D%82%E5%AD%A6/AAAMLprob/%E7%BB%84%E7%BB%87%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="组织机器学习项目"><a href="#组织机器学习项目" class="headerlink" title="组织机器学习项目"></a>组织机器学习项目</h1><p>终于，我们可以开始构建第一个机器学习模型了。</p><p>是这样吗？</p><p>在开始之前，我们必须注意几件事。请记住，我们将在集成开发环境/文本编辑器中工作，而不是在 jupyter notebook 中。你也可以在 jupyter notebook 中工作，这完全取决于你。不过，我将只使用 jupyter notebook 来探索数据、绘制图表和图形。我们将以这样一种方式构建分类框架，即插即用。您无需对代码做太多改动就能训练模型，而且当您改进模型时，还能使用 git 对其进行跟踪。</p><p>我们首先来看看文件的结构。对于你正在做的任何项目，都要创建一个新文件夹。在本例中，我将项目命名为 “project”。</p><p>项目文件夹内部应该如下所示。</p><ul><li>input<ul><li>train.csv</li><li>test.csv</li></ul></li><li>src<ul><li>create_folds.py</li><li>train.py</li><li>inference.py</li><li>models.py</li><li>config.py</li><li>model_dispatcher.py</li></ul></li><li>models<ul><li>model_rf.bin</li><li>model_et.bin</li></ul></li><li>notebooks<ul><li>exploration.ipynb</li><li>check_data.ipynb</li></ul></li><li>README.md</li><li>LICENSE</li></ul><p>让我们来看看这些文件夹和文件的内容。</p><p><em>input/</em>：该文件夹包含机器学习项目的所有输入文件和数据。如果您正在开发 NLP 项目，您可以将 embeddings 放在这里。如果是图像项目，所有图像都放在该文件夹下的子文件夹中。</p><p><em>src/</em>：我们将在这里保存与项目相关的所有 python 脚本。如果我说的是一个 python 脚本，即任何 *.py 文件，它都存储在 src 文件夹中。</p><p><em>models/</em>：该文件夹保存所有训练过的模型。</p><p><em>notebook/</em>：所有 jupyter notebook（即任何 *.ipynb 文件）都存储在笔记本 文件夹中。</p><p><em>README.md</em>：这是一个标记符文件，您可以在其中描述您的项目，并写明如何训练模型或在生产环境中使用。</p><p><em>LICENSE</em>：这是一个简单的文本文件，包含项目的许可证，如 MIT、Apache 等。关于许可证的详细介绍超出了本书的范围。</p><p>假设你正在建立一个模型来对 MNIST 数据集（几乎每本机器学习书籍都会用到的数据集）进行分类。如果你还记得，我们在交叉检验一章中也提到过 MNIST 数据集。所以，我就不解释这个数据集是什么样子了。网上有许多不同格式的 MNIST 数据集，但我们将使用 CSV 格式的数据集。</p><p>在这种格式的数据集中，CSV 的每一行都包含图像的标签和 784 个像素值，像素值范围从 0 到 255。数据集包含 60000 张这种格式的图像。</p><p>我们可以使用 pandas 轻松读取这种数据格式。</p><p>请注意，尽管图 1 显示所有像素值均为零，但事实并非如此。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page74_image.png" alt=""></p><p align="center"><b>图 1：CSV格式的 MNIST 数据集</b> </p><p>让我们来看看这个数据集中标签列的计数。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page74_image_1.png" alt=""></p><p align="center"><b>图 2：MNIST 数据集中的标签计数</b> </p><p>我们不需要对这个数据集进行更多的探索。我们已经知道了我们所拥有的数据，没有必要再对不同的像素值进行绘图。从图 2 中可以清楚地看出，标签的分布相当均匀。因此，我们可以使用准确率/F1 作为衡量标准。这就是处理机器学习问题的第一步：确定衡量标准！</p><p>现在，我们可以编写一些代码了。我们需要创建 <em>src/</em> 文件夹和一些 python 脚本。</p><p>请注意，训练 CSV 文件位于 <em>input/</em> 文件夹中，名为 <em>mnist_train.csv</em>。</p><p>对于这样一个项目，这些文件应该是什么样的呢？</p><p>首先要创建的脚本是 <strong>create_folds.py</strong>。</p><p>这将在 <em>input/</em> 文件夹中创建一个名为 <em>mnist_train_folds.csv</em> 的新文件，与 <em>mnist_train.csv</em> 相同。唯一不同的是，这个 CSV 文件经过了随机排序，并新增了一列名为 <em>kfold</em> 的内容。</p><p>一旦我们决定了要使用哪种评估指标并创建了折叠，就可以开始创建基本模型了。这可以在 train.py 中完成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    <span class="comment"># 读取数据文件</span></span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/mnist_train_folds.csv&quot;</span>)</span><br><span class="line">    <span class="comment"># 选取df中kfold列不等于fold</span></span><br><span class="line">    df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 选取df中kfold列等于fold</span></span><br><span class="line">    df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 训练集输入，删除label列</span></span><br><span class="line">    x_train = df_train.drop(<span class="string">&quot;label&quot;</span>, axis=<span class="number">1</span>).values</span><br><span class="line">    <span class="comment"># 训练集输出，取label列</span></span><br><span class="line">    y_train = df_train.label.values</span><br><span class="line">    <span class="comment"># 验证集输入，删除label列</span></span><br><span class="line">    x_valid = df_valid.drop(<span class="string">&quot;label&quot;</span>, axis=<span class="number">1</span>).values</span><br><span class="line">    <span class="comment"># 验证集输出，取label列</span></span><br><span class="line">    y_valid = df_valid.label.values</span><br><span class="line">    <span class="comment"># 实例化决策树模型</span></span><br><span class="line">    clf = tree.DecisionTreeClassifier()</span><br><span class="line">    <span class="comment"># 使用训练集训练模型</span></span><br><span class="line">    clf.fit(x_train, y_train)</span><br><span class="line">    <span class="comment"># 使用验证集输入得到预测结果</span></span><br><span class="line">    preds = clf.predict(x_valid)</span><br><span class="line">    <span class="comment"># 计算验证集准确率</span></span><br><span class="line">    accuracy = metrics.accuracy_score(y_valid, preds)</span><br><span class="line">    <span class="comment"># 打印fold信息和准确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold=<span class="subst">&#123;fold&#125;</span>, Accuracy=<span class="subst">&#123;accuracy&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    joblib.dump(clf, <span class="string">f&quot;../models/dt_<span class="subst">&#123;fold&#125;</span>.bin&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 运行每个折叠</span></span><br><span class="line">run(fold=<span class="number">0</span>)</span><br><span class="line">run(fold=<span class="number">1</span>)</span><br><span class="line">run(fold=<span class="number">2</span>)</span><br><span class="line">run(fold=<span class="number">3</span>)</span><br><span class="line">run(fold=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>您可以在控制台调用 python train.py 运行该脚本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python train.py</span><br><span class="line">Fold=<span class="number">0</span>, Accuracy=<span class="number">0.8680833333333333</span></span><br><span class="line">Fold=<span class="number">1</span>, Accuracy=<span class="number">0.8685</span></span><br><span class="line">Fold=<span class="number">2</span>, Accuracy=<span class="number">0.8674166666666666</span></span><br><span class="line">Fold=<span class="number">3</span>, Accuracy=<span class="number">0.8703333333333333</span></span><br><span class="line">Fold=<span class="number">4</span>, Accuracy=<span class="number">0.8699166666666667</span></span><br></pre></td></tr></table></figure><p>查看训练脚本时，您会发现还有一些内容是硬编码的，例如折叠数、训练文件和输出文件夹。</p><p>因此，我们可以创建一个包含所有这些信息的配置文件：<strong>config.py</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TRAINING_FILE = <span class="string">&quot;../input/mnist_train_folds.csv&quot;</span></span><br><span class="line">MODEL_OUTPUT = <span class="string">&quot;../models/&quot;</span></span><br></pre></td></tr></table></figure><p>我们还对训练脚本进行了一些修改。训练文件现在使用配置文件。这样，更改数据或模型输出就更容易了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    <span class="comment"># 使用config中的路径读取数据</span></span><br><span class="line">    df = pd.read_csv(config.TRAINING_FILE)</span><br><span class="line">    df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    x_train = df_train.drop(<span class="string">&quot;label&quot;</span>, axis=<span class="number">1</span>).values</span><br><span class="line">    y_train = df_train.label.values</span><br><span class="line">    x_valid = df_valid.drop(<span class="string">&quot;label&quot;</span>, axis=<span class="number">1</span>).values</span><br><span class="line">    y_valid = df_valid.label.values</span><br><span class="line">    clf = tree.DecisionTreeClassifier()</span><br><span class="line">    clf.fit(x_train, y_train)</span><br><span class="line">    preds = clf.predict(x_valid)</span><br><span class="line">    accuracy = metrics.accuracy_score(y_valid, preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold=<span class="subst">&#123;fold&#125;</span>, Accuracy=<span class="subst">&#123;accuracy&#125;</span>&quot;</span>)</span><br><span class="line">    joblib.dump(clf,os.path.join(config.MODEL_OUTPUT, <span class="string">f&quot;dt_<span class="subst">&#123;fold&#125;</span>.bin&quot;</span>) )</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 运行每个折叠</span></span><br><span class="line">run(fold=<span class="number">0</span>)</span><br><span class="line">run(fold=<span class="number">1</span>)</span><br><span class="line">run(fold=<span class="number">2</span>)</span><br><span class="line">run(fold=<span class="number">3</span>)</span><br><span class="line">run(fold=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>请注意，我并没有展示这个培训脚本与之前脚本的区别。请仔细阅读这两个脚本，自己找出不同之处。区别并不多。</p><p>与训练脚本相关的还有一点可以改进。正如你所看到的，我们为每个折叠多次调用运行函数。有时，在同一个脚本中运行多个折叠并不可取，因为内存消耗可能会不断增加，程序可能会崩溃。为了解决这个问题，我们可以向训练脚本传递参数。我喜欢使用 argparse。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 实例化参数环境</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    <span class="comment"># fold参数</span></span><br><span class="line">    parser.add_argument( <span class="string">&quot;--fold&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">    <span class="comment"># 读取参数</span></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    run(fold=args.fold)</span><br></pre></td></tr></table></figure><p>现在，我们可以再次运行 python 脚本，但仅限于给定的折叠。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">❯ python train.py --fold <span class="number">0</span></span><br><span class="line">Fold=<span class="number">0</span>, Accuracy=<span class="number">0.8656666666666667</span></span><br></pre></td></tr></table></figure><p>仔细观察，我们的第 0 折得分与之前有些不同。这是因为模型中存在随机性。我们将在后面的章节中讨论如何处理随机性。</p><p>现在，如果你愿意，可以创建一个 <strong>shell 脚本</strong>，针对不同的折叠使用不同的命令，然后一起运行，如下图所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python train.py --fold <span class="number">0</span></span><br><span class="line">python train.py --fold <span class="number">1</span></span><br><span class="line">python train.py --fold <span class="number">2</span></span><br><span class="line">python train.py --fold <span class="number">3</span></span><br><span class="line">python train.py --fold <span class="number">4</span></span><br></pre></td></tr></table></figure><p>您可以通过以下命令运行它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ sh run.sh</span><br><span class="line">Fold=<span class="number">0</span>, Accuracy=<span class="number">0.8675</span></span><br><span class="line">Fold=<span class="number">1</span>, Accuracy=<span class="number">0.8693333333333333</span></span><br><span class="line">Fold=<span class="number">2</span>, Accuracy=<span class="number">0.8683333333333333</span></span><br><span class="line">Fold=<span class="number">3</span>, Accuracy=<span class="number">0.8704166666666666</span></span><br><span class="line">Fold=<span class="number">4</span>, Accuracy=<span class="number">0.8685</span></span><br></pre></td></tr></table></figure><p>我们现在已经取得了一些进展，但如果我们看一下我们的训练脚本，我们仍然受到一些东西的限制，例如模型。模型是硬编码在训练脚本中的，只有修改脚本才能改变它。因此，我们将创建一个新的 python 脚本，名为 <strong>model_dispatcher.py</strong>。model_dispatcher.py，顾名思义，将调度我们的模型到训练脚本中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line">models = &#123;</span><br><span class="line">    <span class="comment"># 以gini系数度量的决策树</span></span><br><span class="line">    <span class="string">&quot;decision_tree_gini&quot;</span>: tree.DecisionTreeClassifier(</span><br><span class="line">        criterion=<span class="string">&quot;gini&quot;</span></span><br><span class="line">    ),</span><br><span class="line">    <span class="comment"># 以entropy系数度量的决策树</span></span><br><span class="line">    <span class="string">&quot;decision_tree_entropy&quot;</span>: tree.DecisionTreeClassifier(</span><br><span class="line">        criterion=<span class="string">&quot;entropy&quot;</span></span><br><span class="line">    ),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>model_dispatcher.py 从 scikit-learn 中导入了 tree，并定义了一个字典，其中键是模型的名称，值是模型本身。在这里，我们定义了两种不同的决策树，一种使用基尼标准，另一种使用熵标准。要使用 py，我们需要对训练脚本做一些修改。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"><span class="keyword">import</span> model_dispatcher</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold, model</span>):</span><br><span class="line">    df = pd.read_csv(config.TRAINING_FILE)</span><br><span class="line">    df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">x_train = df_train.drop(<span class="string">&quot;label&quot;</span>, axis=<span class="number">1</span>).values</span><br><span class="line">y_train = df_train.label.values</span><br><span class="line">    x_valid = df_valid.drop(<span class="string">&quot;label&quot;</span>, axis=<span class="number">1</span>).values</span><br><span class="line">y_valid = df_valid.label.values</span><br><span class="line">    <span class="comment"># 根据model参数选择模型</span></span><br><span class="line">clf = model_dispatcher.models[model]</span><br><span class="line">    clf.fit(x_train, y_train)</span><br><span class="line">preds = clf.predict(x_valid)</span><br><span class="line">    accuracy = metrics.accuracy_score(y_valid, preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold=<span class="subst">&#123;fold&#125;</span>, Accuracy=<span class="subst">&#123;accuracy&#125;</span>&quot;</span>)</span><br><span class="line">    joblib.dump( clf,os.path.join(config.MODEL_OUTPUT, <span class="string">f&quot;dt_<span class="subst">&#123;fold&#125;</span>.bin&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">    <span class="comment"># fold参数</span></span><br><span class="line">parser.add_argument(<span class="string">&quot;--fold&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">    <span class="comment"># model参数</span></span><br><span class="line">parser.add_argument(<span class="string">&quot;--model&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    run(fold=args.fold, model=args.model)</span><br></pre></td></tr></table></figure><p>train.py 有几处重大改动：</p><ul><li>导入<em>model_dispatcher</em></li><li>为 ArgumentParser 添加 —model 参数</li><li>为 run() 函数添加 model 参数</li><li>使用调度程序获取指定名称的模型</li></ul><p>现在，我们可以使用以下命令运行脚本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">❯ python train.py --fold <span class="number">0</span> --model decision_tree_gini</span><br><span class="line">Fold=<span class="number">0</span>, Accuracy=<span class="number">0.8665833333333334</span></span><br></pre></td></tr></table></figure><p>或执行以下命令</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">❯ python train.py --fold <span class="number">0</span> --model decision_tree_entropy</span><br><span class="line">Fold=<span class="number">0</span>, Accuracy=<span class="number">0.8705833333333334</span></span><br></pre></td></tr></table></figure><p>现在，如果要添加新模型，只需修改 <em>model_dispatcher.py</em>。让我们尝试添加随机森林，看看准确率会有什么变化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> ensemble</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line">models = &#123;</span><br><span class="line">    <span class="string">&quot;decision_tree_gini&quot;</span>: tree.DecisionTreeClassifier(</span><br><span class="line">        criterion=<span class="string">&quot;gini&quot;</span></span><br><span class="line">    ),</span><br><span class="line">    <span class="string">&quot;decision_tree_entropy&quot;</span>: tree.DecisionTreeClassifier(</span><br><span class="line">        criterion=<span class="string">&quot;entropy&quot;</span></span><br><span class="line">    ),</span><br><span class="line">    <span class="comment"># 随机森林模型</span></span><br><span class="line">    <span class="string">&quot;rf&quot;</span>: ensemble.RandomForestClassifier(),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>让我们运行这段代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">❯ python train.py --fold <span class="number">0</span> --model rf</span><br><span class="line">Fold=<span class="number">0</span>, Accuracy=<span class="number">0.9670833333333333</span></span><br></pre></td></tr></table></figure><p>哇，一个简单的改动就能让分数有如此大的提升！现在，让我们使用 <em>run.sh</em> 脚本运行 5 个折叠！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python train.py --fold <span class="number">0</span> --model rf</span><br><span class="line">python train.py --fold <span class="number">1</span> --model rf</span><br><span class="line">python train.py --fold <span class="number">2</span> --model rf</span><br><span class="line">python train.py --fold <span class="number">3</span> --model rf</span><br><span class="line">python train.py --fold <span class="number">4</span> --model rf</span><br></pre></td></tr></table></figure><p>得分情况如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ sh run.sh</span><br><span class="line">Fold=<span class="number">0</span>, Accuracy=<span class="number">0.9674166666666667</span></span><br><span class="line">Fold=<span class="number">1</span>, Accuracy=<span class="number">0.9698333333333333</span></span><br><span class="line">Fold=<span class="number">2</span>, Accuracy=<span class="number">0.96575</span></span><br><span class="line">Fold=<span class="number">3</span>, Accuracy=<span class="number">0.9684166666666667</span></span><br><span class="line">Fold=<span class="number">4</span>, Accuracy=<span class="number">0.9666666666666667</span></span><br></pre></td></tr></table></figure><p>MNIST 几乎是每本书和每篇博客都会讨论的问题。但我试图将这个问题转换得更有趣，并向你展示如何为你正在做的或计划在不久的将来做的几乎所有机器学习项目编写一个基本框架。有许多不同的方法可以改进这个 MNIST 模型和这个框架，我们将在以后的章节中看到。</p><p>我使用了一些脚本，如 <em>model_dispatcher.py</em> 和 <em>config.py</em>，并将它们导入到我的训练脚本中。请注意，我没有导入 <em>，你也不应该导入。如果我导入了 </em>，你就永远不会知道模型字典是从哪里来的。编写优秀、易懂的代码是一个人必须具备的基本素质，但许多数据科学家却忽视了这一点。如果你所做的项目能让其他人理解并使用，而无需咨询你的意见，那么你就节省了他们的时间和自己的时间，可以将这些时间投入到改进你的项目或开发新项目中去。</p>]]></content>
      
      
      <categories>
          
          <category> 杂学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>评估指标</title>
      <link href="/%E6%9D%82%E5%AD%A6/AAAMLprob/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/"/>
      <url>/%E6%9D%82%E5%AD%A6/AAAMLprob/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/</url>
      
        <content type="html"><![CDATA[<h1 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h1><p>说到机器学习问题，你会在现实世界中遇到很多不同类型的指标。有时，人们甚至会根据业务问题创建度量标准。逐一介绍和解释每一种度量类型超出了本书的范围。相反，我们将介绍一些最常见的度量标准，供你在最初的几个项目中使用。</p><p>在本书的开头，我们介绍了监督学习和非监督学习。虽然无监督学习可以使用一些指标，但我们将只关注有监督学习。这是因为有监督问题比无监督问题多，而且对无监督方法的评估相当主观。</p><p>如果我们谈论分类问题，最常用的指标是：</p><ul><li>准确率（Accuracy）</li><li>精确率（P）</li><li>召回率（R）</li><li>F1 分数（F1）</li><li>AUC（AUC）</li><li>对数损失（Log loss）</li><li>k 精确率（P@k）</li><li>k 平均精率（AP@k）</li><li>k 均值平均精确率（MAP@k）</li></ul><p>说到回归，最常用的评价指标是</p><ul><li>平均绝对误差 （MAE）</li><li>均方误差 （MSE）</li><li>均方根误差 （RMSE）</li><li>均方根对数误差 （RMSLE）</li><li>平均百分比误差 （MPE）</li><li>平均绝对百分比误差 （MAPE）</li><li>R2</li></ul><p>了解上述指标的工作原理并不是我们必须了解的唯一事情。我们还必须知道何时使用哪些指标，而这取决于你有什么样的数据和目标。我认为这与目标有关，而与数据无关。</p><p>要进一步了解这些指标，让我们从一个简单的问题开始。假设我们有一个<strong>二元分类</strong>问题，即只有两个目标的问题，假设这是一个胸部 X 光图像分类问题。有的胸部 X 光图像没有问题，而有的胸部 X 光图像有肺塌陷，也就是所谓的气胸。因此，我们的任务是建立一个分类器，在给定胸部 X 光图像的情况下，它能检测出图像是否有气胸。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page30_image.png" alt=""></p><p align="center"><b>图 1：气胸肺部图像</b> </p><p>我们还假设有相同数量的气胸和非气胸胸部 X 光图像，比如各 100 张。因此，我们有 100 张阳性样本和 100 张阴性样本，共计 200 张图像。</p><p>第一步是将上述数据分为两组，每组 100 张图像，即训练集和验证集。在这两个集合中，我们都有 50 个正样本和 50 个负样本。</p><p>在二元分类指标中，当正负样本数量相等时，我们通常使用准确率、精确率、召回率和 F1。</p><p><strong>准确率</strong>：这是机器学习中最直接的指标之一。它定义了模型的准确度。对于上述问题，如果你建立的模型能准确分类 90 张图片，那么你的准确率就是 90% 或 0.90。如果只有 83 幅图像被正确分类，那么模型的准确率就是 83% 或 0.83。</p><p>计算准确率的 Python 代码也非常简单。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 为正确预测数初始化一个简单计数器</span></span><br><span class="line">    correct_counter = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历y_true，y_pred中所有元素</span></span><br><span class="line">    <span class="keyword">for</span> yt, yp <span class="keyword">in</span> <span class="built_in">zip</span>(y_true, y_pred):</span><br><span class="line">        <span class="keyword">if</span> yt == yp:</span><br><span class="line">            <span class="comment"># 如果预测标签与真实标签相同，则增加计数器</span></span><br><span class="line">            correct_counter += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 返回正确率，正确标签数/总标签数</span></span><br><span class="line">    <span class="keyword">return</span> correct_counter / <span class="built_in">len</span>(y_true)</span><br></pre></td></tr></table></figure><p>我们还可以使用 scikit-learn 计算准确率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [X]: <span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">   ...: l1 = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">   ...: l2 = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">   ...: metrics.accuracy_score(l1, l2)</span><br><span class="line">Out[X]: <span class="number">0.625</span></span><br></pre></td></tr></table></figure><p>现在，假设我们把数据集稍微改动一下，有 180 张没有气胸的胸部 X 光图像，只有 20 张有气胸。即使在这种情况下，我们也要创建正负（气胸与非气胸）目标比例相同的训练集和验证集。在每一组中，我们有 90 张非气胸图像和 10 张气胸图像。如果说验证集中的所有图像都是非气胸图像，那么您的准确率会是多少呢？让我们来看看；您对 90% 的图像进行了正确分类。因此，您的准确率是 90%。</p><p>但请再看一遍。</p><p>你甚至没有建立一个模型，就得到了 90% 的准确率。这似乎有点没用。如果我们仔细观察，就会发现数据集是偏斜的，也就是说，一个类别中的样本数量比另一个类别中的样本数量多很多。在这种情况下，使用准确率作为评估指标是不可取的，因为它不能代表数据。因此，您可能会获得很高的准确率，但您的模型在实际样本中的表现可能并不理想，而且您也无法向经理解释原因。</p><p>在这种情况下，最好还是看看<strong>精确率</strong>等其他指标。</p><p>在学习精确率之前，我们需要了解一些术语。在这里，我们假设有气胸的胸部 X 光图像为正类 (1)，没有气胸的为负类 (0)。</p><p><strong>真阳性 （TP）</strong>： 给定一幅图像，如果您的模型预测该图像有气胸，而该图像的实际目标有气胸，则视为真阳性。</p><p><strong>真阴性 （TN）</strong>： 给定一幅图像，如果您的模型预测该图像没有气胸，而实际目标显示该图像没有气胸，则视为真阴性。</p><p>简单地说，如果您的模型正确预测了阳性类别，它就是真阳性；如果您的模型准确预测了阴性类别，它就是真阴性。</p><p><strong>假阳性 （FP）</strong>：给定一张图像，如果您的模型预测为气胸，而该图像的实际目标是非气胸，则为假阳性。</p><p><strong>假阴性 （FN）</strong>： 给定一幅图像，如果您的模型预测为非气胸，而该图像的实际目标是气胸，则为假阴性。</p><p>简单地说，如果您的模型错误地（或虚假地）预测了阳性类，那么它就是假阳性。如果模型错误地（或虚假地）预测了阴性类别，则是假阴性。</p><p>让我们逐一看看这些实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">true_positive</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 初始化真阳性样本计数器</span></span><br><span class="line">    tp = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历y_true，y_pred中所有元素</span></span><br><span class="line">    <span class="keyword">for</span> yt, yp <span class="keyword">in</span> <span class="built_in">zip</span>(y_true, y_pred):</span><br><span class="line">        <span class="comment"># 若真实标签为正类且预测标签也为正类，计数器增加</span></span><br><span class="line">        <span class="keyword">if</span> yt == <span class="number">1</span> <span class="keyword">and</span> yp == <span class="number">1</span>:</span><br><span class="line">            tp += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 返回真阳性样本数</span></span><br><span class="line">    <span class="keyword">return</span> tp</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">true_negative</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 初始化真阴性样本计数器</span></span><br><span class="line">    tn = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历y_true，y_pred中所有元素</span></span><br><span class="line">    <span class="keyword">for</span> yt, yp <span class="keyword">in</span> <span class="built_in">zip</span>(y_true, y_pred):</span><br><span class="line">         <span class="comment"># 若真实标签为负类且预测标签也为负类，计数器增加</span></span><br><span class="line">        <span class="keyword">if</span> yt == <span class="number">0</span> <span class="keyword">and</span> yp == <span class="number">0</span>:</span><br><span class="line">            tn += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 返回真阴性样本数</span></span><br><span class="line">    <span class="keyword">return</span> tn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">false_positive</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 初始化假阳性计数器</span></span><br><span class="line">    fp = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历y_true，y_pred中所有元素</span></span><br><span class="line">    <span class="keyword">for</span> yt, yp <span class="keyword">in</span> <span class="built_in">zip</span>(y_true, y_pred):</span><br><span class="line">        <span class="comment"># 若真实标签为负类而预测标签为正类，计数器增加</span></span><br><span class="line">        <span class="keyword">if</span> yt == <span class="number">0</span> <span class="keyword">and</span> yp == <span class="number">1</span>:</span><br><span class="line">            fp += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 返回假阳性样本数</span></span><br><span class="line">    <span class="keyword">return</span> fp</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">false_negative</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 初始化假阴性计数器</span></span><br><span class="line">    fn = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历y_true，y_pred中所有元素</span></span><br><span class="line">    <span class="keyword">for</span> yt, yp <span class="keyword">in</span> <span class="built_in">zip</span>(y_true, y_pred):</span><br><span class="line">        <span class="comment"># 若真实标签为正类而预测标签为负类，计数器增加</span></span><br><span class="line">        <span class="keyword">if</span> yt == <span class="number">1</span> <span class="keyword">and</span> yp == <span class="number">0</span>:</span><br><span class="line">            fn += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 返回假阴性数</span></span><br><span class="line"><span class="keyword">return</span> fn</span><br></pre></td></tr></table></figure><p>我在这里实现这些功能的方法非常简单，而且只适用于二元分类。让我们检查一下这些函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [X]: l1 = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">   ...: l2 = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">In [X]: true_positive(l1, l2)</span><br><span class="line">Out[X]: <span class="number">2</span></span><br><span class="line">In [X]: false_positive(l1, l2)</span><br><span class="line">Out[X]: <span class="number">1</span></span><br><span class="line">In [X]: false_negative(l1, l2)</span><br><span class="line">Out[X]: <span class="number">2</span></span><br><span class="line">In [X]: true_negative(l1, l2)</span><br><span class="line">Out[X]: <span class="number">3</span></span><br></pre></td></tr></table></figure><p>如果我们必须用上述术语来定义精确率，我们可以写为：</p><script type="math/tex; mode=display">Accuracy Score = (TP + TN)/(TP + TN + FP +FN)</script><p>现在，我们可以在 python 中使用 TP、TN、FP 和 FN 快速实现准确度得分。我们将其称为 accuracy_v2。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy_v2</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 真阳性样本数</span></span><br><span class="line">    tp = true_positive(y_true, y_pred)</span><br><span class="line">    <span class="comment"># 假阳性样本数</span></span><br><span class="line">    fp = false_positive(y_true, y_pred)</span><br><span class="line">    <span class="comment"># 假阴性样本数</span></span><br><span class="line">    fn = false_negative(y_true, y_pred)</span><br><span class="line">    <span class="comment"># 真阴性样本数</span></span><br><span class="line">    tn = true_negative(y_true, y_pred)</span><br><span class="line">    <span class="comment"># 准确率</span></span><br><span class="line">    accuracy_score = (tp + tn) / (tp + tn + fp + fn)</span><br><span class="line"><span class="keyword">return</span> accuracy_score</span><br></pre></td></tr></table></figure><p>我们可以通过与之前的实现和 scikit-learn 版本进行比较，快速检查该函数的正确性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [X]: l1 = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">   ...: l2 = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">In [X]: accuracy(l1, l2)</span><br><span class="line">Out[X]: <span class="number">0.625</span></span><br><span class="line">In [X]: accuracy_v2(l1, l2)</span><br><span class="line">Out[X]: <span class="number">0.625</span></span><br><span class="line">In [X]: metrics.accuracy_score(l1, l2)</span><br><span class="line">Out[X]: <span class="number">0.625</span></span><br></pre></td></tr></table></figure><p>请注意，在这段代码中，metrics.accuracy_score 来自 scikit-learn。</p><p>很好。所有值都匹配。这说明我们在实现过程中没有犯任何错误。</p><p>现在，我们可以转向其他重要指标。</p><p>首先是精确率。精确率的定义是</p><script type="math/tex; mode=display">Precision = TP/(TP + FP)</script><p>假设我们在新的偏斜数据集上建立了一个新模型，我们的模型正确识别了 90 张图像中的 80 张非气胸图像和 10 张图像中的 8 张气胸图像。因此，我们成功识别了 100 张图像中的 88 张。因此，准确率为 0.88 或 88%。</p><p>但是，在这 100 张样本中，有 10 张非气胸图像被误判为气胸，2 张气胸图像被误判为非气胸。</p><p>因此，我们有</p><ul><li>TP : 8</li><li>TN: 80</li><li>FP: 10</li><li>FN: 2</li></ul><p>精确率为 8 / (8 + 10) = 0.444。这意味着我们的模型在识别阳性样本（气胸）时有 44.4% 的正确率。</p><p>现在，既然我们已经实现了 TP、TN、FP 和 FN，我们就可以很容易地在 python 中实现精确率了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">precision</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 真阳性样本数</span></span><br><span class="line">    tp = true_positive(y_true, y_pred)</span><br><span class="line">    <span class="comment"># 假阳性样本数</span></span><br><span class="line">    fp = false_positive(y_true, y_pred)</span><br><span class="line">    <span class="comment"># 精确率</span></span><br><span class="line">    precision = tp / (tp + fp)</span><br><span class="line">    <span class="keyword">return</span> precision</span><br></pre></td></tr></table></figure><p>让我们试试这种精确率的实现方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [X]: l1 = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">   ...: l2 = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">In [X]: precision(l1, l2)</span><br><span class="line">Out[X]: <span class="number">0.6666666666666666</span></span><br></pre></td></tr></table></figure><p>这似乎没有问题。 接下来，我们来看<strong>召回率</strong>。召回率的定义是：</p><script type="math/tex; mode=display">Recall = TP/(TP + FN)</script><p>在上述情况下，召回率为 8 / (8 + 2) = 0.80。这意味着我们的模型正确识别了 80% 的阳性样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">recall</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 真阳性样本数</span></span><br><span class="line">    tp = true_positive(y_true, y_pred)</span><br><span class="line">    <span class="comment"># 假阴性样本数</span></span><br><span class="line">    fn = false_negative(y_true, y_pred)</span><br><span class="line">    <span class="comment"># 召回率</span></span><br><span class="line">    recall = tp / (tp + fn)</span><br><span class="line">    <span class="keyword">return</span> recall</span><br></pre></td></tr></table></figure><p>就我们的两个小列表而言，召回率应该是 0.5。让我们检查一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [X]: l1 = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">   ...: l2 = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">In [X]: recall(l1, l2)</span><br><span class="line">Out[X]: <span class="number">0.5</span></span><br></pre></td></tr></table></figure><p>这与我们的计算值相符！</p><p>对于一个 “好 “模型来说，精确率和召回值都应该很高。我们看到，在上面的例子中，召回值相当高。但是，精确率却很低！我们的模型产生了大量的误报，但误报较少。在这类问题中，假阴性较少是好事，因为你不想在病人有气胸的情况下却说他们没有气胸。这样做会造成更大的伤害。但我们也有很多假阳性结果，这也不是好事。</p><p>大多数模型都会预测一个概率，当我们预测时，通常会将这个阈值选为 0.5。这个阈值并不总是理想的，根据这个阈值，精确率和召回率的值可能会发生很大的变化。如果我们选择的每个阈值都能计算出精确率和召回率，那么我们就可以在这些值之间绘制出曲线图。这幅图或曲线被称为 “精确率-召回率曲线”。</p><p>在研究精确率-调用曲线之前，我们先假设有两个列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [X]: y_true = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,</span><br><span class="line">   ...:          <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">In [X]: y_pred = [<span class="number">0.02638412</span>, <span class="number">0.11114267</span>, <span class="number">0.31620708</span>,</span><br><span class="line">   ...:          <span class="number">0.0490937</span>, <span class="number">0.0191491</span>, <span class="number">0.17554844</span>,</span><br><span class="line">   ...:          <span class="number">0.15952202</span>, <span class="number">0.03819563</span>, <span class="number">0.11639273</span>,</span><br><span class="line">   ...:          <span class="number">0.079377</span>,  <span class="number">0.08584789</span>, <span class="number">0.39095342</span>,</span><br><span class="line">   ...:          <span class="number">0.27259048</span>, <span class="number">0.03447096</span>, <span class="number">0.04644807</span>,</span><br><span class="line">   ...:          <span class="number">0.03543574</span>, <span class="number">0.18521942</span>, <span class="number">0.05934905</span>,</span><br><span class="line">   ...:          <span class="number">0.61977213</span>, <span class="number">0.33056815</span>]</span><br></pre></td></tr></table></figure><p>因此，y_true 是我们的目标值，而 y_pred 是样本被赋值为 1 的概率值。因此，现在我们要看的是预测中的概率，而不是预测值（大多数情况下，预测值的计算阈值为 0.5）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">precisions = []</span><br><span class="line">recalls = []</span><br><span class="line">thresholds = [<span class="number">0.0490937</span> , <span class="number">0.05934905</span>, <span class="number">0.079377</span>,</span><br><span class="line">              <span class="number">0.08584789</span>, <span class="number">0.11114267</span>, <span class="number">0.11639273</span>,</span><br><span class="line">              <span class="number">0.15952202</span>, <span class="number">0.17554844</span>, <span class="number">0.18521942</span>,</span><br><span class="line">              <span class="number">0.27259048</span>, <span class="number">0.31620708</span>, <span class="number">0.33056815</span>,</span><br><span class="line">              <span class="number">0.39095342</span>, <span class="number">0.61977213</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历预测阈值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> thresholds:</span><br><span class="line">    <span class="comment"># 若样本为正类（1）的概率大于阈值，为1，否则为0</span></span><br><span class="line">    temp_prediction = [<span class="number">1</span> <span class="keyword">if</span> x &gt;= i <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">    <span class="comment"># 计算精确率</span></span><br><span class="line">    p = precision(y_true, temp_prediction)</span><br><span class="line">    <span class="comment"># 计算召回率</span></span><br><span class="line">    r = recall(y_true, temp_prediction)</span><br><span class="line">    <span class="comment"># 加入精确率列表</span></span><br><span class="line">    precisions.append(p)</span><br><span class="line">    <span class="comment"># 加入召回率列表</span></span><br><span class="line">    recalls.append(r)</span><br></pre></td></tr></table></figure><p>现在，我们可以绘制精确率-召回率曲线。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建画布</span></span><br><span class="line">plt.figure(figsize=(<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line"><span class="comment"># x轴为召回率，y轴为精确率</span></span><br><span class="line">plt.plot(recalls, precisions)</span><br><span class="line"><span class="comment"># 添加x轴标签，字体大小为15</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Recall&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line"><span class="comment"># 添加y轴标签，字条大小为15</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Precision&#x27;</span>, fontsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><p>图 2 显示了我们通过这种方法得到的精确率-召回率曲线。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page39_image.png" alt=""></p><p align="center"><b>图 2：精确率-召回率曲线</b> </p><p>这条<strong>精确率-召回率曲线</strong>与您在互联网上看到的曲线截然不同。这是因为我们只有 20 个样本，其中只有 3 个是阳性样本。但这没什么好担心的。这还是那条精确率-召回曲线。</p><p>你会发现，选择一个既能提供良好精确率又能提供召回值的阈值是很有挑战性的。如果阈值过高，真阳性的数量就会减少，而假阴性的数量就会增加。这会降低召回率，但精确率得分会很高。如果将阈值降得太低，则误报会大量增加，精确率也会降低。</p><p>精确率和召回率的范围都是从 0 到 1，越接近 1 越好。</p><p>F1 分数是精确率和召回率的综合指标。它被定义为精确率和召回率的简单加权平均值（调和平均值）。如果我们用 P 表示精确率，用 R 表示召回率，那么 F1 分数可以表示为：</p><script type="math/tex; mode=display">F1 = 2PR/(P + R)</script><p>根据 TP、FP 和 FN，稍加数学计算就能得出以下 F1 等式：</p><script type="math/tex; mode=display">F1 = 2TP/(2TP + FP + FN)</script><p>Python 实现很简单，因为我们已经实现了这些</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f1</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 计算精确率</span></span><br><span class="line">    p = precision(y_true, y_pred)</span><br><span class="line">    <span class="comment"># 计算召回率</span></span><br><span class="line">    r = recall(y_true, y_pred)</span><br><span class="line">    <span class="comment"># 计算f1值</span></span><br><span class="line">    score = <span class="number">2</span> * p * r / (p + r)</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure><p>让我们看看其结果，并与 scikit-learn 进行比较。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [X]: y_true = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,</span><br><span class="line">   ...:          <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">In [X]: y_pred = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,</span><br><span class="line">   ...:          <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">In [X]: f1(y_true, y_pred)</span><br><span class="line">Out[X]: <span class="number">0.5714285714285715</span></span><br></pre></td></tr></table></figure><p>通过 scikit learn，我们可以得到相同的列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In [X]: <span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">In [X]: metrics.f1_score(y_true, y_pred)</span><br><span class="line">Out[X]: <span class="number">0.5714285714285715</span></span><br></pre></td></tr></table></figure><p>与其单独看精确率和召回率，您还可以只看 F1 分数。与精确率、召回率和准确度一样，F1 分数的范围也是从 0 到 1，完美预测模型的 F1 分数为 1。</p><p>此外，我们还应该了解其他一些关键术语。</p><p>第一个术语是 TPR 或真阳性率（True Positive Rate），它与召回率相同。</p><script type="math/tex; mode=display">TPR = TP/(TP + FN)</script><p>尽管它与召回率相同，但我们将为它创建一个 python 函数，以便今后使用这个名称。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tpr</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 真阳性率（TPR），与召回率计算公式一致</span></span><br><span class="line"><span class="keyword">return</span> recall(y_true, y_pred)</span><br></pre></td></tr></table></figure><p>TPR 或召回率也被称为灵敏度。</p><p>而 FPR 或假阳性率（False Positive Rate）的定义是：</p><script type="math/tex; mode=display">FPR = FP / (TN + FP)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fpr</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 假阳性样本数</span></span><br><span class="line">    fp = false_positive(y_true, y_pred)</span><br><span class="line">    <span class="comment"># 真阴性样本数</span></span><br><span class="line">    tn = true_negative(y_true, y_pred)</span><br><span class="line"><span class="comment"># 返回假阳性率（FPR）</span></span><br><span class="line"><span class="keyword">return</span> fp / (tn + fp)</span><br></pre></td></tr></table></figure><p>1 - FPR 被称为特异性或真阴性率或 TNR。这些术语很多，但其中最重要的只有 TPR 和 FPR。假设我们只有 15 个样本，其目标值为二元：</p><p>Actual targets : [0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1]</p><p>我们训练一个类似随机森林的模型，就能得到样本呈阳性的概率。</p><p>Predicted probabilities for 1: [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 0.85, 0.15, 0.99]</p><p>对于 &gt;= 0.5 的典型阈值，我们可以评估上述所有精确率、召回率/TPR、F1 和 FPR 值。但是，如果我们将阈值选为 0.4 或 0.6，也可以做到这一点。事实上，我们可以选择 0 到 1 之间的任何值，并计算上述所有指标。</p><p>不过，我们只计算两个值： TPR 和 FPR。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化真阳性率列表</span></span><br><span class="line">tpr_list = []</span><br><span class="line"><span class="comment"># 初始化假阳性率列表</span></span><br><span class="line">fpr_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实样本标签</span></span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>,</span><br><span class="line">          <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测样本为正类（1）的概率</span></span><br><span class="line">y_pred = [<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">0.05</span>,</span><br><span class="line">          <span class="number">0.9</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.66</span>, <span class="number">0.3</span>, <span class="number">0.2</span>,</span><br><span class="line">          <span class="number">0.85</span>, <span class="number">0.15</span>, <span class="number">0.99</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测阈值</span></span><br><span class="line">thresholds = [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>,</span><br><span class="line">              <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.85</span>, <span class="number">0.9</span>, <span class="number">0.99</span>, <span class="number">1.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历预测阈值</span></span><br><span class="line"><span class="keyword">for</span> thresh <span class="keyword">in</span> thresholds:</span><br><span class="line">    <span class="comment"># 若样本为正类（1）的概率大于阈值，为1，否则为0</span></span><br><span class="line">    temp_pred = [<span class="number">1</span> <span class="keyword">if</span> x &gt;= thresh <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">    <span class="comment"># 真阳性率</span></span><br><span class="line">    temp_tpr = tpr(y_true, temp_pred)</span><br><span class="line">    <span class="comment"># 假阳性率</span></span><br><span class="line">    temp_fpr = fpr(y_true, temp_pred)</span><br><span class="line">    <span class="comment"># 将真阳性率加入列表</span></span><br><span class="line">    tpr_list.append(temp_tpr)</span><br><span class="line">    <span class="comment"># 将假阳性率加入列表</span></span><br><span class="line">    fpr_list.append(temp_fpr)</span><br></pre></td></tr></table></figure><p>因此，我们可以得到每个阈值的 TPR 值和 FPR 值。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page43_image.png" alt=""></p><p align="center"><b>图 3：阈值、TPR 和 FPR 值表</b> </p><p>如果我们绘制如图 3 所示的表格，即以 TPR 为 Y 轴，FPR 为 X 轴，就会得到如图 4 所示的曲线。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page44_image.png" alt=""></p><p align="center"><b>图 4：ROC曲线</b> </p><p>这条曲线也被称为 ROC 曲线。如果我们计算这条 ROC 曲线下的面积，就是在计算另一个指标，当数据集的二元目标偏斜时，这个指标就会非常常用。</p><p>这个指标被称为 ROC 曲线下面积或曲线下面积，简称 AUC。计算 ROC 曲线下面积的方法有很多。在此，我们将采用 scikit- learn 的奇妙实现方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [X]: <span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">In [X]: y_true = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>,</span><br><span class="line">   ...:          <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">In [X]: y_pred = [<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">0.05</span>,</span><br><span class="line">   ...:          <span class="number">0.9</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.66</span>, <span class="number">0.3</span>, <span class="number">0.2</span>,</span><br><span class="line">   ...:          <span class="number">0.85</span>, <span class="number">0.15</span>, <span class="number">0.99</span>]</span><br><span class="line">In [X]: metrics.roc_auc_score(y_true, y_pred)</span><br><span class="line">Out[X]: <span class="number">0.8300000000000001</span></span><br></pre></td></tr></table></figure><p>AUC 值从 0 到 1 不等。</p><ul><li>AUC = 1 意味着您拥有一个完美的模型。大多数情况下，这意味着你在验证时犯了一些错误，应该重新审视数据处理和验证流程。如果你没有犯任何错误，那么恭喜你，你已经拥有了针对数据集建立的最佳模型。</li><li>AUC = 0 意味着您的模型非常糟糕（或非常好！）。试着反转预测的概率，例如，如果您预测正类的概率是 p，试着用 1-p 代替它。这种 AUC 也可能意味着您的验证或数据处理存在问题。</li><li>AUC = 0.5 意味着你的预测是随机的。因此，对于任何二元分类问题，如果我将所有目标都预测为 0.5，我将得到 0.5 的 AUC。</li></ul><p>AUC 值介于 0 和 0.5 之间，意味着你的模型比随机模型更差。大多数情况下，这是因为你颠倒了类别。 如果您尝试反转预测，您的 AUC 值可能会超过 0.5。接近 1 的 AUC 值被认为是好值。</p><p>但 AUC 对我们的模型有什么影响呢？</p><p>假设您建立了一个从胸部 X 光图像中检测气胸的模型，其 AUC 值为 0.85。这意味着，如果您从数据集中随机选择一张有气胸的图像（阳性样本）和另一张没有气胸的图像（阴性样本），那么气胸图像的排名将高于非气胸图像，概率为 0.85。</p><p>计算概率和 AUC 后，您需要对测试集进行预测。根据问题和使用情况，您可能需要概率或实际类别。如果你想要概率，这并不难。如果您想要类别，则需要选择一个阈值。在二元分类的情况下，您可以采用类似下面的方法。</p><script type="math/tex; mode=display">Prediction = Probability >= Threshold</script><p>也就是说，预测是一个只包含二元变量的新列表。如果概率大于或等于给定的阈值，则预测中的一项为 1，否则为 0。</p><p>你猜怎么着，你可以使用 ROC 曲线来选择这个阈值！ROC 曲线会告诉您阈值对假阳性率和真阳性率的影响，进而影响假阳性和真阳性。您应该选择最适合您的问题和数据集的阈值。</p><p>例如，如果您不希望有太多的误报，那么阈值就应该高一些。不过，这也会带来更多的误报。注意权衡利弊，选择最佳阈值。让我们看看这些阈值如何影响真阳性和假阳性值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 真阳性样本数列表</span></span><br><span class="line">tp_list = []</span><br><span class="line"><span class="comment"># 假阳性样本数列表</span></span><br><span class="line">fp_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实标签</span></span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>,</span><br><span class="line">          <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测样本为正类（1）的概率</span></span><br><span class="line">y_pred = [<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">0.05</span>,</span><br><span class="line">          <span class="number">0.9</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.66</span>, <span class="number">0.3</span>, <span class="number">0.2</span>,</span><br><span class="line">          <span class="number">0.85</span>, <span class="number">0.15</span>, <span class="number">0.99</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测阈值</span></span><br><span class="line">thresholds = [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>,</span><br><span class="line">              <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.85</span>, <span class="number">0.9</span>, <span class="number">0.99</span>, <span class="number">1.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历预测阈值</span></span><br><span class="line"><span class="keyword">for</span> thresh <span class="keyword">in</span> thresholds:</span><br><span class="line">    <span class="comment"># 若样本为正类（1）的概率大于阈值，为1，否则为0</span></span><br><span class="line">    temp_pred = [<span class="number">1</span> <span class="keyword">if</span> x &gt;= thresh <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">    <span class="comment"># 真阳性样本数</span></span><br><span class="line">    temp_tp = true_positive(y_true, temp_pred)</span><br><span class="line">    <span class="comment"># 假阳性样本数</span></span><br><span class="line">    temp_fp = false_positive(y_true, temp_pred)</span><br><span class="line">    <span class="comment"># 加入真阳性样本数列表</span></span><br><span class="line">    tp_list.append(temp_tp)</span><br><span class="line">    <span class="comment"># 加入假阳性样本数列表</span></span><br><span class="line">    fp_list.append(temp_fp)</span><br></pre></td></tr></table></figure><p>利用这一点，我们可以创建一个表格，如图 5 所示。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page47_image.png" alt=""></p><p align="center"><b>图 5：不同阈值的 TP 值和 FP 值</b> </p><p>如图 6 所示，大多数情况下，ROC 曲线左上角的值应该是一个相当不错的阈值。</p><p>对比表格和 ROC 曲线，我们可以发现，0.6 左右的阈值相当不错，既不会丢失大量的真阳性结果，也不会出现大量的假阳性结果。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page48_image.png" alt=""></p><p align="center"><b>图 6：从 ROC 曲线最左侧的顶点选择最佳阈值</b> </p><p>AUC 是业内广泛应用于偏斜二元分类任务的指标，也是每个人都应该了解的指标。一旦理解了 AUC 背后的理念（如上文所述），也就很容易向业界可能会评估您的模型的非技术人员解释它了。</p><p>学习 AUC 后，你应该学习的另一个重要指标是对数损失。对于二元分类问题，我们将对数损失定义为：</p><script type="math/tex; mode=display">LogLoss = -1.0 \times (target \times log(prediction) + (1-target) \times log(1-prediction))</script><p>其中，目标值为 0 或 1，预测值为样本属于类别 1 的概率。</p><p>对于数据集中的多个样本，所有样本的对数损失只是所有单个对数损失的平均值。需要记住的一点是，对数损失会对不正确或偏差较大的预测进行相当高的惩罚，也就是说，对数损失会对非常确定和非常错误的预测进行惩罚。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log_loss</span>(<span class="params">y_true, y_proba</span>):</span><br><span class="line">    <span class="comment"># 极小值，防止0做分母</span></span><br><span class="line">    epsilon = <span class="number">1e-15</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对数损失列表</span></span><br><span class="line">    loss = []</span><br><span class="line">    <span class="comment"># 遍历y_true，y_pred中所有元素</span></span><br><span class="line">    <span class="keyword">for</span> yt, yp <span class="keyword">in</span> <span class="built_in">zip</span>(y_true, y_proba):</span><br><span class="line">        <span class="comment"># 限制yp范围，最小为epsilon，最大为1-epsilon</span></span><br><span class="line">        yp = np.clip(yp, epsilon, <span class="number">1</span> - epsilon)</span><br><span class="line">        <span class="comment"># 计算对数损失</span></span><br><span class="line">        temp_loss = - <span class="number">1.0</span> * (yt * np.log(yp)+ (<span class="number">1</span> - yt) * np.log(<span class="number">1</span> - yp))</span><br><span class="line">        <span class="comment"># 加入对数损失列表</span></span><br><span class="line">        loss.append(temp_loss)</span><br><span class="line"><span class="keyword">return</span> np.mean(loss)</span><br></pre></td></tr></table></figure><p>让我们测试一下函数执行情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [X]: y_true = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>,</span><br><span class="line">   ...:          <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">In [X]: y_proba = [<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">0.05</span>,</span><br><span class="line">   ...:          <span class="number">0.9</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.66</span>, <span class="number">0.3</span>, <span class="number">0.2</span>,</span><br><span class="line">   ...:          <span class="number">0.85</span>, <span class="number">0.15</span>, <span class="number">0.99</span>]</span><br><span class="line">In [X]: log_loss(y_true, y_proba)</span><br><span class="line">Out[X]: <span class="number">0.49882711861432294</span></span><br></pre></td></tr></table></figure><p>我们可以将其与 scikit-learn 进行比较：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In [X]: <span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">In [X]: metrics.log_loss(y_true, y_proba)</span><br><span class="line">Out[X]: <span class="number">0.49882711861432294</span></span><br></pre></td></tr></table></figure><p>因此，我们的实现是正确的。 对数损失的实现很容易。解释起来似乎有点困难。你必须记住，对数损失的惩罚要比其他指标大得多。</p><p>例如，如果您有 51% 的把握认为样本属于第 1 类，那么对数损失就是：</p><script type="math/tex; mode=display">-1.0 \times (1 \times log(0.51) + (1 - 1) \times log(1 - 0.51))=0.67</script><p>如果你对属于 0 类的样本有 49% 的把握，对数损失就是：</p><script type="math/tex; mode=display">-1.0 \times (1 \times log(0.49) + (1 - 1) \times log(1 - 0.49))=0.67</script><p>因此，即使我们可以选择 0.5 的截断值并得到完美的预测结果，我们仍然会有非常高的对数损失。因此，在处理对数损失时，你需要非常小心；任何不确定的预测都会产生非常高的对数损失。</p><p>我们之前讨论过的大多数指标都可以转换成多类版本。这个想法很简单。以精确率和召回率为例。我们可以计算多类分类问题中每一类的精确率和召回率。</p><p>有三种不同的计算方法，有时可能会令人困惑。假设我们首先对精确率感兴趣。我们知道，精确率取决于真阳性和假阳性。</p><ul><li><strong>宏观平均精确率</strong>（Macro averaged precision）：分别计算所有类别的精确率然后求平均值</li><li><strong>微观平均精确率</strong>（Micro averaged precision）：按类计算真阳性和假阳性，然后用其计算总体精确率。然后以此计算总体精确率</li><li><strong>加权精确率</strong>（Weighted precision）：与宏观精确率相同，但这里是加权平均精确率 取决于每个类别中的项目数</li></ul><p>这看似复杂，但在 python 实现中很容易理解。让我们看看宏观平均精确率是如何实现的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">macro_precision</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 种类数</span></span><br><span class="line">    num_classes = <span class="built_in">len</span>(np.unique(y_true))</span><br><span class="line">    <span class="comment"># 初始化精确率</span></span><br><span class="line">    precision = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历0~（种类数-1）</span></span><br><span class="line">    <span class="keyword">for</span> class_ <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">        <span class="comment"># 若真实标签为class_为1，否则为0</span></span><br><span class="line">        temp_true = [<span class="number">1</span> <span class="keyword">if</span> p == class_ <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> p <span class="keyword">in</span> y_true]</span><br><span class="line">        <span class="comment"># 如预测标签为class_为1，否则为0</span></span><br><span class="line">        temp_pred = [<span class="number">1</span> <span class="keyword">if</span> p == class_ <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> p <span class="keyword">in</span> y_pred]</span><br><span class="line">        <span class="comment"># 真阳性样本数</span></span><br><span class="line">        tp = true_positive(temp_true, temp_pred)</span><br><span class="line">        <span class="comment"># 假阳性样本数</span></span><br><span class="line">        fp = false_positive(temp_true, temp_pred)</span><br><span class="line">        <span class="comment"># 计算精确度</span></span><br><span class="line">        temp_precision = tp / (tp + fp)</span><br><span class="line">        <span class="comment"># 各类精确率相加</span></span><br><span class="line">        precision += temp_precision</span><br><span class="line">    <span class="comment"># 计算平均值</span></span><br><span class="line">    precision /= num_classes</span><br><span class="line"><span class="keyword">return</span> precision</span><br></pre></td></tr></table></figure><p>你会发现这并不难。同样，我们还有微平均精确率分数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">micro_precision</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 种类数</span></span><br><span class="line">    num_classes = <span class="built_in">len</span>(np.unique(y_true))</span><br><span class="line">    <span class="comment"># 初始化真阳性样本数</span></span><br><span class="line">    tp = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 初始化假阳性样本数</span></span><br><span class="line">    fp = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历0~（种类数-1）</span></span><br><span class="line">    <span class="keyword">for</span> class_ <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">        <span class="comment"># 若真实标签为class_为1，否则为0</span></span><br><span class="line">        temp_true = [<span class="number">1</span> <span class="keyword">if</span> p == class_ <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> p <span class="keyword">in</span> y_true]</span><br><span class="line">        <span class="comment"># 若预测标签为class_为1，否则为0</span></span><br><span class="line">        temp_pred = [<span class="number">1</span> <span class="keyword">if</span> p == class_ <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> p <span class="keyword">in</span> y_pred]</span><br><span class="line">        <span class="comment"># 真阳性样本数相加</span></span><br><span class="line">        tp += true_positive(temp_true, temp_pred)</span><br><span class="line">        <span class="comment"># 假阳性样本数相加</span></span><br><span class="line">        fp += false_positive(temp_true, temp_pred)</span><br><span class="line">    <span class="comment"># 精确率</span></span><br><span class="line">precision = tp / (tp + fp)</span><br><span class="line"><span class="keyword">return</span> precision</span><br></pre></td></tr></table></figure><p>这也不难。那什么难？什么都不难。机器学习很简单。现在，让我们来看看加权精确率的实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">weighted_precision</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 种类数</span></span><br><span class="line">    num_classes = <span class="built_in">len</span>(np.unique(y_true))</span><br><span class="line">    <span class="comment"># 统计各种类样本数</span></span><br><span class="line">    class_counts = Counter(y_true)</span><br><span class="line">    <span class="comment"># 初始化精确率</span></span><br><span class="line">    precision = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历0~（种类数-1）</span></span><br><span class="line">    <span class="keyword">for</span> class_ <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">        <span class="comment"># 若真实标签为class_为1，否则为0</span></span><br><span class="line">        temp_true = [<span class="number">1</span> <span class="keyword">if</span> p == class_ <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> p <span class="keyword">in</span> y_true]</span><br><span class="line">        <span class="comment"># 若预测标签为class_为1，否则为0</span></span><br><span class="line">        temp_pred = [<span class="number">1</span> <span class="keyword">if</span> p == class_ <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> p <span class="keyword">in</span> y_pred]</span><br><span class="line">        <span class="comment"># 真阳性样本数</span></span><br><span class="line">        tp = true_positive(temp_true, temp_pred)</span><br><span class="line">        <span class="comment"># 假阳性样本数</span></span><br><span class="line">        fp = false_positive(temp_true, temp_pred)</span><br><span class="line">        <span class="comment"># 精确率</span></span><br><span class="line">        temp_precision = tp / (tp + fp)</span><br><span class="line">        <span class="comment"># 根据该种类样本数分配权重</span></span><br><span class="line">        weighted_precision = class_counts[class_] * temp_precision</span><br><span class="line">        <span class="comment"># 加权精确率求和</span></span><br><span class="line">        precision += weighted_precision</span><br><span class="line">    <span class="comment"># 计算平均精确率</span></span><br><span class="line">overall_precision = precision / <span class="built_in">len</span>(y_true)</span><br><span class="line"><span class="keyword">return</span> overall_precision</span><br></pre></td></tr></table></figure><p>将我们的实现与 scikit-learn 进行比较，以了解实现是否正确。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In [X]: <span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">In [X]: y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">In [X]: y_pred = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>]</span><br><span class="line">In [X]: macro_precision(y_true, y_pred)</span><br><span class="line">Out[X]: <span class="number">0.3611111111111111</span></span><br><span class="line">In [X]: metrics.precision_score(y_true, y_pred, average=<span class="string">&quot;macro&quot;</span>)</span><br><span class="line">Out[X]: <span class="number">0.3611111111111111</span></span><br><span class="line">In [X]: micro_precision(y_true, y_pred)</span><br><span class="line">Out[X]: <span class="number">0.4444444444444444</span></span><br><span class="line">In [X]: metrics.precision_score(y_true, y_pred, average=<span class="string">&quot;micro&quot;</span>)</span><br><span class="line">Out[X]: <span class="number">0.4444444444444444</span></span><br><span class="line">In [X]: weighted_precision(y_true, y_pred)</span><br><span class="line">Out[X]: <span class="number">0.39814814814814814</span></span><br><span class="line">In [X]: metrics.precision_score(y_true, y_pred, average=<span class="string">&quot;weighted&quot;</span>)</span><br><span class="line">Out[X]: <span class="number">0.39814814814814814</span></span><br></pre></td></tr></table></figure><p>看来我们已经正确地实现了一切。 请注意，这里展示的实现可能不是最有效的，但却是最容易理解的。</p><p>同样，我们也可以实现<strong>多类别的召回率指标</strong>。精确率和召回率取决于真阳性、假阳性和假阴性，而 F1 则取决于精确率和召回率。</p><p>召回率的实现方法留待读者练习，这里实现的是多类 F1 的一个版本，即加权平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">weighted_f1</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 种类数</span></span><br><span class="line">    num_classes = <span class="built_in">len</span>(np.unique(y_true))</span><br><span class="line">    <span class="comment"># 统计各种类样本数</span></span><br><span class="line">    class_counts = Counter(y_true)</span><br><span class="line">    <span class="comment"># 初始化F1值</span></span><br><span class="line">    f1 = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历0~（种类数-1）</span></span><br><span class="line">    <span class="keyword">for</span> class_ <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">        <span class="comment"># 若真实标签为class_为1，否则为0</span></span><br><span class="line">        temp_true = [<span class="number">1</span> <span class="keyword">if</span> p == class_ <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> p <span class="keyword">in</span> y_true]</span><br><span class="line">        <span class="comment"># 若预测标签为class_为1，否则为0</span></span><br><span class="line">        temp_pred = [<span class="number">1</span> <span class="keyword">if</span> p == class_ <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> p <span class="keyword">in</span> y_pred]</span><br><span class="line">        <span class="comment"># 计算精确率</span></span><br><span class="line">        p = precision(temp_true, temp_pred)</span><br><span class="line">        <span class="comment"># 计算召回率</span></span><br><span class="line">        r = recall(temp_true, temp_pred)</span><br><span class="line">        <span class="comment"># 若精确率+召回率不为0，则使用公式计算F1值</span></span><br><span class="line">        <span class="keyword">if</span> p + r != <span class="number">0</span>:</span><br><span class="line">            temp_f1 = <span class="number">2</span> * p * r / (p + r)</span><br><span class="line">        <span class="comment"># 否则直接为0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            temp_f1 = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 根据样本数分配权重</span></span><br><span class="line">weighted_f1 = class_counts[class_] * temp_f1</span><br><span class="line">        <span class="comment"># 加权F1值相加</span></span><br><span class="line">        f1 += weighted_f1</span><br><span class="line">    <span class="comment"># 计算加权平均F1值</span></span><br><span class="line">    overall_f1 = f1 / <span class="built_in">len</span>(y_true)</span><br><span class="line"><span class="keyword">return</span> overall_f1</span><br></pre></td></tr></table></figure><p>请注意，上面有几行代码是新写的。因此，你应该仔细阅读这些代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [X]: <span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">In [X]: y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">In [X]: y_pred = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>]</span><br><span class="line">In [X]: weighted_f1(y_true, y_pred)</span><br><span class="line">Out[X]: <span class="number">0.41269841269841273</span></span><br><span class="line">In [X]: metrics.f1_score(y_true, y_pred, average=<span class="string">&quot;weighted&quot;</span>)</span><br><span class="line">Out[X]: <span class="number">0.41269841269841273</span></span><br></pre></td></tr></table></figure><p>因此，我们已经为多类问题实现了精确率、召回率和 F1。同样，您也可以将 AUC 和对数损失转换为多类格式。这种转换格式被称为 <strong>one-vs-all</strong>。这里我不打算实现它们，因为实现方法与我们已经讨论过的很相似。</p><p>在二元或多类分类中，看一下<strong>混淆矩阵</strong>也很流行。不要困惑，这很简单。混淆矩阵只不过是一个包含 TP、FP、TN 和 FN 的表格。使用混淆矩阵，您可以快速查看有多少样本被错误分类，有多少样本被正确分类。也许有人会说，混淆矩阵应该在本章很早就讲到，但我没有这么做。如果了解了 TP、FP、TN、FN、精确率、召回率和 AUC，就很容易理解和解释混淆矩阵了。让我们看看图 7 中二元分类问题的混淆矩阵。</p><p>我们可以看到，混淆矩阵由 TP、FP、FN 和 TN 组成。我们只需要这些值来计算精确率、召回率、F1 分数和 AUC。有时，人们也喜欢把 FP 称为<strong>第一类错误</strong>，把 FN 称为<strong>第二类错误</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page57_image.png" alt=""></p><p align="center"><b>图 7：二元分类任务的混淆矩阵</b> </p><p>我们还可以将二元混淆矩阵扩展为多类混淆矩阵。它会是什么样子呢？如果我们有 N 个类别，它将是一个大小为 NxN 的矩阵。对于每个类别，我们都要计算相关类别和其他类别的样本总数。举个例子可以让我们更好地理解这一点。</p><p>假设我们有以下真实标签：</p><script type="math/tex; mode=display">[0, 1, 2, 0, 1, 2, 0, 2, 2]</script><p>我们的预测标签是：</p><script type="math/tex; mode=display">[0, 2, 1, 0, 2, 1, 0, 0, 2]</script><p>那么，我们的混淆矩阵将如图 8 所示。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page58_image.png" alt=""></p><p align="center"><b>图 8：多分类问题的混淆矩阵</b> </p><p>图 8 说明了什么？</p><p>让我们来看看 0 类。我们看到，在真实标签中，有 3 个样本属于 0 类。然而，在预测中，我们有 3 个样本属于第 0 类，1 个样本属于第 1 类。理想情况下，对于真实标签中的类别 0，预测标签 1 和 2 应该没有任何样本。让我们看看类别 2。在真实标签中，这个数字加起来是 4，而在预测标签中，这个数字加起来是 3。</p><p>一个完美的混淆矩阵只能从左到右斜向填充。</p><p><strong>混淆矩阵</strong>提供了一种简单的方法来计算我们之前讨论过的不同指标。Scikit-learn 提供了一种简单直接的方法来生成混淆矩阵。请注意，我在图 8 中显示的混淆矩阵是 scikit-learn 混淆矩阵的转置，原始版本可以通过以下代码绘制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实样本标签</span></span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line"><span class="comment"># 预测样本标签</span></span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算混淆矩阵</span></span><br><span class="line">cm = metrics.confusion_matrix(y_true, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建画布</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line"><span class="comment"># 创建方格</span></span><br><span class="line">cmap = sns.cubehelix_palette(<span class="number">50</span>, hue=<span class="number">0.05</span>, rot=<span class="number">0</span>, light=<span class="number">0.9</span>, dark=<span class="number">0</span>,</span><br><span class="line">as_cmap=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 规定字体大小</span></span><br><span class="line">sns.<span class="built_in">set</span>(font_scale=<span class="number">2.5</span>)</span><br><span class="line"><span class="comment"># 绘制热图</span></span><br><span class="line">sns.heatmap(cm, annot=<span class="literal">True</span>, cmap=cmap, cbar=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># y轴标签，字体大小为20</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Actual Labels&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line"><span class="comment"># x轴标签，字体大小为20</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Predicted Labels&#x27;</span>, fontsize=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><p>因此，到目前为止，我们已经解决了二元分类和多类分类的度量问题。接下来，我们将讨论另一种类型的分类问题，即多标签分类。在多标签分类中，每个样本都可能与一个或多个类别相关联。这类问题的一个简单例子就是要求你预测给定图像中的不同物体。</p><p>图 9 显示了一个著名数据集的图像示例。请注意，该数据集的目标有所不同，但我们暂且不去讨论它。我们假设其目的只是预测图像中是否存在某个物体。在图 9 中，我们有椅子、花盆、窗户，但没有其他物体，如电脑、床、电视等。因此，一幅图像可能有多个相关目标。这类问题就是多标签分类问题。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page59_image.png" alt=""></p><p align="center"><b>图 9：图像中的不同物体</b> </p><p>这类分类问题的衡量标准有些不同。一些合适的 最常见的指标有：</p><ul><li>k 精确率（P@k）</li><li>k 平均精确率（AP@k）</li><li>k 均值平均精确率（MAP@k）</li><li>对数损失（Log loss）</li></ul><p>让我们从<strong>k 精确率或者 P@k</strong>我们不能将这一精确率与前面讨论的精确率混淆。如果您有一个给定样本的原始类别列表和同一个样本的预测类别列表，那么精确率的定义就是预测列表中仅考虑前 k 个预测结果的命中数除以 k。</p><p>如果您对此感到困惑，使用 python 代码后就会明白。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pk</span>(<span class="params">y_true, y_pred, k</span>):</span><br><span class="line">    <span class="comment"># 如果k为0</span></span><br><span class="line">    <span class="keyword">if</span> k == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 返回0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 取预测标签前k个</span></span><br><span class="line">    y_pred = y_pred[:k]</span><br><span class="line">    <span class="comment"># 将预测标签转换为集合</span></span><br><span class="line">    pred_set = <span class="built_in">set</span>(y_pred)</span><br><span class="line">    <span class="comment"># 将真实标签转换为集合</span></span><br><span class="line">    true_set = <span class="built_in">set</span>(y_true)</span><br><span class="line">    <span class="comment"># 预测标签集合与真实标签集合交集</span></span><br><span class="line">    common_values = pred_set.intersection(true_set)</span><br><span class="line">    <span class="comment"># 计算精确率</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(common_values) / <span class="built_in">len</span>(y_pred[:k])</span><br></pre></td></tr></table></figure><p>有了代码，一切都变得更容易理解了。</p><p>现在，我们有了<strong>k 平均精确率或 AP@k</strong>。AP@k 是通过 P@k 计算得出的。例如，如果要计算 AP@3，我们要先计算 P@1、P@2 和 P@3，然后将总和除以 3。</p><p>让我们来看看它的实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">apk</span>(<span class="params">y_true, y_pred, k</span>):</span><br><span class="line">    <span class="comment"># 初始化P@k列表</span></span><br><span class="line">    pk_values = []</span><br><span class="line">    <span class="comment"># 遍历1~k</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, k + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 将P@k加入列表</span></span><br><span class="line">        pk_values.append(pk(y_true, y_pred, i))</span><br><span class="line">    <span class="comment"># 若长度为0</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(pk_values) == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 返回0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 否则计算AP@K</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(pk_values) / <span class="built_in">len</span>(pk_values)</span><br></pre></td></tr></table></figure><p>这两个函数可以用来计算两个给定列表的 k 平均精确率 (AP@k)；让我们看看如何计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">In [X]: y_true = [</span><br><span class="line">   ...:     [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">   ...:     [<span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">   ...:     [<span class="number">1</span>],</span><br><span class="line">   ...:     [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">   ...:     [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">   ...:     []</span><br><span class="line">   ...: ]</span><br><span class="line">In [X]: y_pred = [</span><br><span class="line">   ...:     [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">   ...:     [<span class="number">1</span>],</span><br><span class="line">   ...:     [<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">   ...:     [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">   ...:     [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">   ...:     [<span class="number">0</span>]</span><br><span class="line">   ...: ]</span><br><span class="line">In [X]: <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y_true)):</span><br><span class="line">   ...:    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">4</span>):</span><br><span class="line">   ...:         <span class="built_in">print</span>(</span><br><span class="line">   ...:            <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   ...:             y_true=<span class="subst">&#123;y_true[i]&#125;</span>,</span></span><br><span class="line"><span class="string">   ...:             y_pred=<span class="subst">&#123;y_pred[i]&#125;</span>,</span></span><br><span class="line"><span class="string">   ...:             AP@<span class="subst">&#123;j&#125;</span>=<span class="subst">&#123;apk(y_true[i], y_pred[i], k=j)&#125;</span></span></span><br><span class="line"><span class="string">   ...:             &quot;&quot;&quot;</span></span><br><span class="line">   ...:         )</span><br><span class="line">   ...:</span><br><span class="line">        y_true=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        y_pred=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        AP@<span class="number">1</span>=<span class="number">0.0</span></span><br><span class="line">        y_true=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        y_pred=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        AP@<span class="number">2</span>=<span class="number">0.25</span></span><br><span class="line">        y_true=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        y_pred=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        AP@<span class="number">3</span>=<span class="number">0.38888888888888884</span></span><br></pre></td></tr></table></figure><p>请注意，我省略了输出结果中的许多数值，但你会明白其中的意思。这就是我们如何计算 AP@k 的方法，即每个样本的 AP@k。在机器学习中，我们对所有样本都感兴趣，这就是为什么我们有<strong>均值平均精确率 k 或 MAP@k</strong>。MAP@k 只是 AP@k 的平均值，可以通过以下 python 代码轻松计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mapk</span>(<span class="params">y_true, y_pred, k</span>):</span><br><span class="line">    <span class="comment"># 初始化AP@k列表</span></span><br><span class="line">    apk_values = []</span><br><span class="line">    <span class="comment"># 遍历0~（真实标签数-1）</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y_true)):</span><br><span class="line">        <span class="comment"># 将AP@K加入列表</span></span><br><span class="line">        apk_values.append(</span><br><span class="line">            apk(y_true[i], y_pred[i], k=k)</span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># 计算平均AP@k</span></span><br><span class="line"><span class="keyword">return</span> <span class="built_in">sum</span>(apk_values) / <span class="built_in">len</span>(apk_values)</span><br></pre></td></tr></table></figure><p>现在，我们可以针对相同的列表计算 k=1、2、3 和 4 时的 MAP@k。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">In [X]: y_true = [</span><br><span class="line">   ...:     [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">   ...:     [<span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">   ...:     [<span class="number">1</span>],</span><br><span class="line">   ...:     [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">   ...:     [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">   ...:     []</span><br><span class="line">   ...: ]</span><br><span class="line">In [X]: y_pred = [</span><br><span class="line">   ...:     [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">   ...:     [<span class="number">1</span>],</span><br><span class="line">   ...:     [<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">   ...:     [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">   ...:     [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">   ...:     [<span class="number">0</span>]</span><br><span class="line">   ...: ]</span><br><span class="line">In [X]: mapk(y_true, y_pred, k=<span class="number">1</span>)</span><br><span class="line">Out[X]: <span class="number">0.3333333333333333</span></span><br><span class="line">In [X]: mapk(y_true, y_pred, k=<span class="number">2</span>)</span><br><span class="line">Out[X]: <span class="number">0.375</span></span><br><span class="line">In [X]: mapk(y_true, y_pred, k=<span class="number">3</span>)</span><br><span class="line">Out[X]: <span class="number">0.3611111111111111</span></span><br><span class="line">In [X]: mapk(y_true, y_pred, k=<span class="number">4</span>)</span><br><span class="line">Out[X]: <span class="number">0.34722222222222215</span></span><br></pre></td></tr></table></figure><p>P@k、AP@k 和 MAP@k 的范围都是从 0 到 1，其中 1 为最佳。</p><p>请注意，有时您可能会在互联网上看到 P@k 和 AP@k 的不同实现方式。 例如，让我们来看看其中一种实现方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apk</span>(<span class="params">actual, predicted, k=<span class="number">10</span></span>):</span><br><span class="line">    <span class="comment"># 若预测标签长度大于k</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(predicted)&gt;k:</span><br><span class="line">        <span class="comment"># 取前k个标签</span></span><br><span class="line">predicted = predicted[:k]</span><br><span class="line"></span><br><span class="line">    score = <span class="number">0.0</span></span><br><span class="line">    num_hits = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i,p <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicted):</span><br><span class="line"><span class="keyword">if</span> p <span class="keyword">in</span> actual <span class="keyword">and</span> p <span class="keyword">not</span> <span class="keyword">in</span> predicted[:i]:</span><br><span class="line">num_hits += <span class="number">1.0</span></span><br><span class="line">score += num_hits / (i+<span class="number">1.0</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> actual:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line"><span class="keyword">return</span> score / <span class="built_in">min</span>(<span class="built_in">len</span>(actual), k)</span><br></pre></td></tr></table></figure><p>这种实现方式是 AP@k 的另一个版本，其中顺序很重要，我们要权衡预测结果。这种实现方式的结果与我的介绍略有不同。</p><p>现在，我们来看看<strong>多标签分类的对数损失</strong>。这很容易。您可以将目标转换为二元分类，然后对每一列使用对数损失。最后，你可以求出每列对数损失的平均值。这也被称为平均列对数损失。当然，还有其他方法可以实现这一点，你应该在遇到时加以探索。</p><p>我们现在可以说已经掌握了所有二元分类、多类分类和多标签分类指标，现在我们可以转向回归指标。</p><p>回归中最常见的指标是<strong>误差（Error）</strong>。误差很简单，也很容易理解。</p><script type="math/tex; mode=display">Error = True\ Value - Predicted\ Value</script><p><strong>绝对误差（Absolute error）</strong>只是上述误差的绝对值。</p><script type="math/tex; mode=display">Absolute\ Error = Abs(True\ Value - Predicted\ Value)</script><p>接下来我们讨论<strong>平均绝对误差（MAE）</strong>。它只是所有绝对误差的平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mean_absolute_error</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment">#初始化误差</span></span><br><span class="line">    error = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历y_true, y_pred</span></span><br><span class="line">    <span class="keyword">for</span> yt, yp <span class="keyword">in</span> <span class="built_in">zip</span>(y_true, y_pred):</span><br><span class="line">        <span class="comment"># 累加绝对误差</span></span><br><span class="line">        error += np.<span class="built_in">abs</span>(yt - yp)</span><br><span class="line">    <span class="comment"># 返回平均绝对误差</span></span><br><span class="line">    <span class="keyword">return</span> error / <span class="built_in">len</span>(y_true)</span><br></pre></td></tr></table></figure><p>同样，我们还有平方误差和<strong>均方误差 （MSE）</strong>。</p><script type="math/tex; mode=display">Squared\ Error = (True Value - Predicted\ Value)^2</script><p>均方误差（MSE）的计算方式如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean_squared_error</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 初始化误差</span></span><br><span class="line">    error = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历y_true, y_pred</span></span><br><span class="line">    <span class="keyword">for</span> yt, yp <span class="keyword">in</span> <span class="built_in">zip</span>(y_true, y_pred):</span><br><span class="line">        <span class="comment"># 累加误差平方和</span></span><br><span class="line">        error += (yt - yp) ** <span class="number">2</span></span><br><span class="line">    <span class="comment"># 计算均方误差</span></span><br><span class="line">    <span class="keyword">return</span> error / <span class="built_in">len</span>(y_true)</span><br></pre></td></tr></table></figure><p>MSE 和 <strong>RMSE（均方根误差）</strong>是评估回归模型最常用的指标。</p><script type="math/tex; mode=display">RMSE = SQRT(MSE)</script><p>同一类误差的另一种类型是<strong>平方对数误差</strong>。有人称其为 <strong>SLE</strong>，当我们取所有样本中这一误差的平均值时，它被称为 MSLE（平均平方对数误差），实现方法如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mean_squared_log_error</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 初始化误差</span></span><br><span class="line">    error = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历y_true, y_pred</span></span><br><span class="line">    <span class="keyword">for</span> yt, yp <span class="keyword">in</span> <span class="built_in">zip</span>(y_true, y_pred):</span><br><span class="line">        <span class="comment"># 计算平方对数误差</span></span><br><span class="line">        error += (np.log(<span class="number">1</span> + yt) - np.log(<span class="number">1</span> + yp)) ** <span class="number">2</span></span><br><span class="line">    <span class="comment"># 计算平均平方对数误差</span></span><br><span class="line">    <span class="keyword">return</span> error / <span class="built_in">len</span>(y_true)</span><br></pre></td></tr></table></figure><p><strong>均方根对数误差</strong>只是其平方根。它也被称为 <strong>RMSLE</strong>。</p><p>然后是百分比误差：</p><script type="math/tex; mode=display">Percentage\ Error = (( True\ Value – Predicted\ Value ) / True\ Value ) \times 100</script><p>同样可以转换为所有样本的平均百分比误差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean_percentage_error</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 初始化误差</span></span><br><span class="line">    error = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历y_true, y_pred</span></span><br><span class="line">    <span class="keyword">for</span> yt, yp <span class="keyword">in</span> <span class="built_in">zip</span>(y_true, y_pred):</span><br><span class="line">        <span class="comment"># 计算百分比误差</span></span><br><span class="line">        error += (yt - yp) / yt</span><br><span class="line">    <span class="comment"># 返回平均百分比误差</span></span><br><span class="line">    <span class="keyword">return</span> error / <span class="built_in">len</span>(y_true)</span><br></pre></td></tr></table></figure><p>绝对误差的绝对值（也是更常见的版本）被称为<strong>平均绝对百分比误差或 MAPE</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mean_abs_percentage_error</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 初始化误差</span></span><br><span class="line">    error = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历y_true, y_pred</span></span><br><span class="line">    <span class="keyword">for</span> yt, yp <span class="keyword">in</span> <span class="built_in">zip</span>(y_true, y_pred):</span><br><span class="line">        <span class="comment"># 计算绝对百分比误差</span></span><br><span class="line">        error += np.<span class="built_in">abs</span>(yt - yp) / yt</span><br><span class="line">    <span class="comment">#返回平均绝对百分比误差</span></span><br><span class="line">    <span class="keyword">return</span> error / <span class="built_in">len</span>(y_true)</span><br></pre></td></tr></table></figure><p>回归的最大优点是，只有几个最常用的指标，几乎可以应用于所有回归问题。与分类指标相比，回归指标更容易理解。</p><p>让我们来谈谈另一个回归指标 $R^2$（R 方），也称为<strong>判定系数</strong>。</p><p>简单地说，R 方表示模型与数据的拟合程度。R 方接近 1.0 表示模型与数据的拟合程度相当好，而接近 0 则表示模型不是那么好。当模型只是做出荒谬的预测时，R 方也可能是负值。</p><p>R 方的计算公式如下所示，但 Python 的实现总是能让一切更加清晰。</p><script type="math/tex; mode=display">R^2 = \frac{\sum^{N}_{i=1}(y_{t_i}-y_{p_i})^2}{\sum^{N}_{i=1}(y_{t_i} - y_{t_{mean}})}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">r2</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 计算平均真实值</span></span><br><span class="line">    mean_true_value = np.mean(y_true)</span><br><span class="line">    <span class="comment"># 初始化平方误差</span></span><br><span class="line">    numerator = <span class="number">0</span></span><br><span class="line">    denominator = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历y_true, y_pred</span></span><br><span class="line">    <span class="keyword">for</span> yt, yp <span class="keyword">in</span> <span class="built_in">zip</span>(y_true, y_pred):</span><br><span class="line">        numerator += (yt - yp) ** <span class="number">2</span></span><br><span class="line">        denominator += (yt - mean_true_value) ** <span class="number">2</span></span><br><span class="line">    ratio = numerator / denominator</span><br><span class="line">    <span class="comment"># 计算R方</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> – ratio</span><br></pre></td></tr></table></figure><p>还有更多的评价指标，这个清单永远也列不完。我可以写一本书，只介绍不同的评价指标。也许我会的。现在，这些评估指标几乎可以满足你想尝试解决的所有问题。请注意，我已经以最直接的方式实现了这些指标，这意味着它们不够高效。你可以通过正确使用 numpy 以非常高效的方式实现其中大部分指标。例如，看看平均绝对误差的实现，不需要任何循环。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mae_np</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line"><span class="keyword">return</span> np.mean(np.<span class="built_in">abs</span>(y_true - y_pred))</span><br></pre></td></tr></table></figure><p>我本可以用这种方法实现所有指标，但为了学习，最好还是看看底层实现。一旦你学会了纯 python 的底层实现，并且不使用大量 numpy，你就可以很容易地将其转换为 numpy，并使其变得更快。</p><p>然后是一些高级度量。</p><p>其中一个应用相当广泛的指标是<strong>二次加权卡帕</strong>，也称为 <strong>QWK</strong>。它也被称为科恩卡帕。<strong>QWK</strong> 衡量两个 “评分 “之间的 “一致性”。评分可以是 0 到 N 之间的任何实数，预测也在同一范围内。一致性可以定义为这些评级之间的接近程度。因此，它适用于有 N 个不同类别的分类问题。如果一致度高，分数就更接近 1.0。Cohen’s kappa 在 scikit-learn 中有很好的实现，关于该指标的详细讨论超出了本书的范围。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [X]: <span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">In [X]: y_true = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">In [X]: y_pred = [<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">In [X]: metrics.cohen_kappa_score(y_true, y_pred, weights=<span class="string">&quot;quadratic&quot;</span>)</span><br><span class="line">Out[X]: <span class="number">0.33333333333333337</span></span><br><span class="line">In [X]: metrics.accuracy_score(y_true, y_pred)</span><br><span class="line">Out[X]: <span class="number">0.4444444444444444</span></span><br></pre></td></tr></table></figure><p>您可以看到，尽管准确度很高，但 QWK 却很低。QWK 大于 0.85 即为非常好！</p><p>一个重要的指标是<strong>马修相关系数（MCC）</strong>。1 代表完美预测，-1 代表不完美预测，0 代表随机预测。MCC 的计算公式非常简单。</p><script type="math/tex; mode=display">MCC = \frac{TP \times TN - FP \times FN}{\sqrt{(TP + FP) \times (FN + TN) \times (FP + TN) \times (TP + FN)}}</script><p>我们看到，MCC 考虑了 TP、FP、TN 和 FN，因此可用于处理类偏斜的问题。您可以使用我们已经实现的方法在 python 中快速实现它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mcc</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 真阳性样本数</span></span><br><span class="line">    tp = true_positive(y_true, y_pred)</span><br><span class="line">    <span class="comment"># 真阴性样本数</span></span><br><span class="line">    tn = true_negative(y_true, y_pred)</span><br><span class="line">    <span class="comment"># 假阳性样本数</span></span><br><span class="line">    fp = false_positive(y_true, y_pred)</span><br><span class="line">    <span class="comment"># 假阴性样本数</span></span><br><span class="line">    fn = false_negative(y_true, y_pred)</span><br><span class="line">    numerator = (tp * tn) - (fp * fn)</span><br><span class="line">    denominator = (</span><br><span class="line">        (tp + fp) *</span><br><span class="line">        (fn + tn) *</span><br><span class="line">        (fp + tn) *</span><br><span class="line">        (tp + fn)</span><br><span class="line">    )</span><br><span class="line">denominator = denominator ** <span class="number">0.5</span></span><br><span class="line"><span class="keyword">return</span> numerator/denominator</span><br></pre></td></tr></table></figure><p>这些指标可以帮助你入门，几乎适用于所有机器学习问题。</p><p>需要注意的一点是，在评估非监督方法（例如某种聚类）时，最好创建或手动标记测试集，并将其与建模部分的所有内容分开。完成聚类后，就可以使用任何一种监督学习指标来评估测试集的性能了。</p><p>一旦我们了解了特定问题应该使用什么指标，我们就可以开始更深入地研究我们的模型，以求改进。</p>]]></content>
      
      
      <categories>
          
          <category> 杂学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>超参数优化</title>
      <link href="/%E6%9D%82%E5%AD%A6/AAAMLprob/%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/"/>
      <url>/%E6%9D%82%E5%AD%A6/AAAMLprob/%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h1 id="超参数优化"><a href="#超参数优化" class="headerlink" title="超参数优化"></a>超参数优化</h1><p>有了优秀的模型，就有了优化超参数以获得最佳得分模型的难题。那么，什么是超参数优化呢？假设您的机器学习项目有一个简单的流程。有一个数据集，你直接应用一个模型，然后得到结果。模型在这里的参数被称为超参数，即控制模型训练/拟合过程的参数。如果我们用 SGD 训练线性回归，模型的参数是斜率和偏差，超参数是学习率。你会发现我在本章和本书中交替使用这些术语。假设模型中有三个参数 a、b、c，所有这些参数都可以是 1 到 10 之间的整数。这些参数的 “正确 “组合将为您提供最佳结果。因此，这就有点像一个装有三拨密码锁的手提箱。不过，三拨密码锁只有一个正确答案。而模型有很多正确答案。那么，如何找到最佳参数呢？一种方法是对所有组合进行评估，看哪种组合能提高指标。让我们看看如何做到这一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化最佳准确度</span></span><br><span class="line">best_accuracy = <span class="number">0</span></span><br><span class="line"><span class="comment"># 初始化最佳参数的字典</span></span><br><span class="line">best_parameters = &#123;<span class="string">&quot;a&quot;</span>: <span class="number">0</span>, <span class="string">&quot;b&quot;</span>: <span class="number">0</span>, <span class="string">&quot;c&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line"><span class="comment"># 循环遍历 a 的取值范围 1~10</span></span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">    <span class="comment"># 循环遍历 b 的取值范围 1~10</span></span><br><span class="line"><span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">        <span class="comment"># 循环遍历 c 的取值范围 1~10</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">            <span class="comment"># 创建模型，使用 a、b、c 参数</span></span><br><span class="line">            model = MODEL(a, b, c)</span><br><span class="line">            <span class="comment"># 使用训练数据拟合模型</span></span><br><span class="line">            model.fit(training_data)</span><br><span class="line">            <span class="comment"># 使用模型对验证数据进行预测</span></span><br><span class="line">            preds = model.predict(validation_data)</span><br><span class="line">            <span class="comment"># 计算预测的准确度</span></span><br><span class="line">            accuracy = metrics.accuracy_score(targets, preds)</span><br><span class="line">             <span class="comment"># 如果当前准确度优于之前的最佳准确度，则更新最佳准确度和最佳参数</span></span><br><span class="line">            <span class="keyword">if</span> accuracy &gt; best_accuracy:</span><br><span class="line">                best_accuracy = accuracy</span><br><span class="line">                best_parameters[<span class="string">&quot;a&quot;</span>] = a</span><br><span class="line">                best_parameters[<span class="string">&quot;b&quot;</span>] = b</span><br><span class="line">best_parameters[<span class="string">&quot;c&quot;</span>] = c</span><br></pre></td></tr></table></figure><p>在上述代码中，我们从 1 到 10 对所有参数进行了拟合。因此，我们总共要对模型进行 1000 次（10 x 10 x 10）拟合。这可能会很昂贵，因为模型的训练需要很长时间。不过，在这种情况下应该没问题，但在现实世界中，并不是只有三个参数，每个参数也不是只有十个值。 大多数模型参数都是实数，不同参数的组合可以是无限的。</p><p>让我们看看 scikit-learn 的随机森林模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">RandomForestClassifier(</span><br><span class="line">    n_estimators=<span class="number">100</span>,</span><br><span class="line">criterion=<span class="string">&#x27;gini&#x27;</span>,</span><br><span class="line">max_depth=<span class="literal">None</span>,</span><br><span class="line">min_samples_split=<span class="number">2</span>,</span><br><span class="line">min_samples_leaf=<span class="number">1</span>,</span><br><span class="line">min_weight_fraction_leaf=<span class="number">0.0</span>,</span><br><span class="line">max_features=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">max_leaf_nodes=<span class="literal">None</span>,</span><br><span class="line">min_impurity_decrease=<span class="number">0.0</span>,</span><br><span class="line">min_impurity_split=<span class="literal">None</span>,</span><br><span class="line">bootstrap=<span class="literal">True</span>,</span><br><span class="line">oob_score=<span class="literal">False</span>,</span><br><span class="line">n_jobs=<span class="literal">None</span>,</span><br><span class="line">random_state=<span class="literal">None</span>,</span><br><span class="line">verbose=<span class="number">0</span>,</span><br><span class="line">warm_start=<span class="literal">False</span>,</span><br><span class="line">class_weight=<span class="literal">None</span>,</span><br><span class="line">ccp_alpha=<span class="number">0.0</span>,</span><br><span class="line">max_samples=<span class="literal">None</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>有 19 个参数，而所有这些参数的所有组合，以及它们可以承担的所有值，都将是无穷无尽的。通常情况下，我们没有足够的资源和时间来做这件事。因此，我们指定了一个参数网格。在这个网格上寻找最佳参数组合的搜索称为网格搜索。我们可以说，n_estimators 可以是 100、200、250、300、400、500；max_depth 可以是 1、2、5、7、11、15；criterion 可以是 gini 或 entropy。这些参数看起来并不多，但如果数据集过大，计算起来会耗费大量时间。我们可以像之前一样创建三个 for 循环，并在验证集上计算得分，这样就能实现网格搜索。还必须注意的是，如果要进行 k 折交叉验证，则需要更多的循环，这意味着需要更多的时间来找到完美的参数。因此，网格搜索并不流行。让我们以根据<strong>手机配置预测手机价格范围</strong>数据集为例，看看它是如何实现的。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page168_image.png" alt=""></p><p align="center"><b>图 1：手机配置预测手机价格范围数据集展示</b> </p><p>训练集中只有 2000 个样本。我们可以轻松地使用分层 kfold 和准确率作为评估指标。我们将使用具有上述参数范围的随机森林模型，并在下面的示例中了解如何进行网格搜索。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rf_grid_search.py</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> ensemble</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 读取数据</span></span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/mobile_train.csv&quot;</span>)</span><br><span class="line">    <span class="comment"># 删除 price_range 列</span></span><br><span class="line">X = df.drop(<span class="string">&quot;price_range&quot;</span>, axis=<span class="number">1</span>).values</span><br><span class="line">    <span class="comment"># 取目标变量 y（&quot;price_range&quot;列）</span></span><br><span class="line">y = df.price_range.values</span><br><span class="line">    <span class="comment"># 创建随机森林分类器，使用所有可用的 CPU 核心进行训练</span></span><br><span class="line">classifier = ensemble.RandomForestClassifier(n_jobs=-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 定义要进行网格搜索的参数网格</span></span><br><span class="line">param_grid = &#123;</span><br><span class="line">        <span class="string">&quot;n_estimators&quot;</span>: [<span class="number">100</span>, <span class="number">200</span>, <span class="number">250</span>, <span class="number">300</span>, <span class="number">400</span>, <span class="number">500</span>],</span><br><span class="line">        <span class="string">&quot;max_depth&quot;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">15</span>],</span><br><span class="line">        <span class="string">&quot;criterion&quot;</span>: [<span class="string">&quot;gini&quot;</span>, <span class="string">&quot;entropy&quot;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 创建 GridSearchCV 对象 model，用于在参数网格上进行网格搜索</span></span><br><span class="line">model = model_selection.GridSearchCV(</span><br><span class="line">        estimator=classifier,</span><br><span class="line">        param_grid=param_grid,</span><br><span class="line">        scoring=<span class="string">&quot;accuracy&quot;</span>,</span><br><span class="line">        verbose=<span class="number">10</span>,</span><br><span class="line">        n_jobs=<span class="number">1</span>,</span><br><span class="line">        cv=<span class="number">5</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 使用网格搜索对象 model 拟合数据，寻找最佳参数组合</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line">    <span class="comment"># 打印出最佳模型的最佳准确度分数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best score: <span class="subst">&#123;model.best_score_&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># 打印最佳参数集合</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best parameters set:&quot;</span>)</span><br><span class="line">best_parameters = model.best_estimator_.get_params()</span><br><span class="line"><span class="keyword">for</span> param_name <span class="keyword">in</span> <span class="built_in">sorted</span>(param_grid.keys()):</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\t<span class="subst">&#123;param_name&#125;</span>: <span class="subst">&#123;best_parameters[param_name]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>这里打印了很多内容，让我们看看最后几行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[CV]  criterion=entropy, max_depth=<span class="number">15</span>, n_estimators=<span class="number">500</span>, score=<span class="number">0.895</span>,</span><br><span class="line">total=  <span class="number">1.0</span>s</span><br><span class="line">[CV] criterion=entropy, max_depth=<span class="number">15</span>, n_estimators=<span class="number">500</span> ...............</span><br><span class="line">[CV]  criterion=entropy, max_depth=<span class="number">15</span>, n_estimators=<span class="number">500</span>, score=<span class="number">0.890</span>,</span><br><span class="line">total=  <span class="number">1.1</span>s</span><br><span class="line">[CV] criterion=entropy, max_depth=<span class="number">15</span>, n_estimators=<span class="number">500</span> ...............</span><br><span class="line">[CV]  criterion=entropy, max_depth=<span class="number">15</span>, n_estimators=<span class="number">500</span>, score=<span class="number">0.910</span>,</span><br><span class="line">total=  <span class="number">1.1</span>s</span><br><span class="line">[CV] criterion=entropy, max_depth=<span class="number">15</span>, n_estimators=<span class="number">500</span> ...............</span><br><span class="line">[CV]  criterion=entropy, max_depth=<span class="number">15</span>, n_estimators=<span class="number">500</span>, score=<span class="number">0.880</span>,</span><br><span class="line">total=  <span class="number">1.1</span>s</span><br><span class="line">[CV] criterion=entropy, max_depth=<span class="number">15</span>, n_estimators=<span class="number">500</span> ...............</span><br><span class="line">[CV]  criterion=entropy, max_depth=<span class="number">15</span>, n_estimators=<span class="number">500</span>, score=<span class="number">0.870</span>,</span><br><span class="line">total=  <span class="number">1.1</span>s</span><br><span class="line">[Parallel(n_jobs=<span class="number">1</span>)]: Done <span class="number">360</span> out of <span class="number">360</span> | elapsed: <span class="number">3.7</span><span class="built_in">min</span> finished</span><br><span class="line">Best score: <span class="number">0.889</span></span><br><span class="line">Best parameters <span class="built_in">set</span>:</span><br><span class="line">criterion: <span class="string">&#x27;entropy&#x27;</span></span><br><span class="line">max_depth: <span class="number">15</span></span><br><span class="line">n_estimators: <span class="number">500</span></span><br></pre></td></tr></table></figure><p>最后，我们可以看到，5 折交叉检验最佳得分是 0.889，我们的网格搜索得到了最佳参数。我们可以使用的下一个最佳方法是<strong>随机搜索</strong>。在随机搜索中，我们随机选择一个参数组合，然后计算交叉验证得分。这里消耗的时间比网格搜索少，因为我们不对所有不同的参数组合进行评估。我们选择要对模型进行多少次评估，这就决定了搜索所需的时间。代码与上面的差别不大。除 GridSearchCV 外，我们使用 RandomizedSearchCV。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">classifier = ensemble.RandomForestClassifier(n_jobs=-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 更改搜索空间</span></span><br><span class="line">    param_grid = &#123;</span><br><span class="line">        <span class="string">&quot;n_estimators&quot;</span>: np.arange(<span class="number">100</span>, <span class="number">1500</span>, <span class="number">100</span>),</span><br><span class="line">        <span class="string">&quot;max_depth&quot;</span>: np.arange(<span class="number">1</span>, <span class="number">31</span>),</span><br><span class="line">        <span class="string">&quot;criterion&quot;</span>: [<span class="string">&quot;gini&quot;</span>, <span class="string">&quot;entropy&quot;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 随机参数搜索</span></span><br><span class="line">model = model_selection.RandomizedSearchCV(</span><br><span class="line">        estimator=classifier,</span><br><span class="line">    param_distributions=param_grid,</span><br><span class="line">    n_iter=<span class="number">20</span>,</span><br><span class="line">    scoring=<span class="string">&quot;accuracy&quot;</span>,</span><br><span class="line">    verbose=<span class="number">10</span>,</span><br><span class="line">    n_jobs=<span class="number">1</span>,</span><br><span class="line">    cv=<span class="number">5</span></span><br><span class="line">)</span><br><span class="line">    <span class="comment"># 使用网格搜索对象 model 拟合数据，寻找最佳参数组合</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best score: <span class="subst">&#123;model.best_score_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best parameters set:&quot;</span>)</span><br><span class="line">best_parameters = model.best_estimator_.get_params()</span><br><span class="line"><span class="keyword">for</span> param_name <span class="keyword">in</span> <span class="built_in">sorted</span>(param_grid.keys()):</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\t<span class="subst">&#123;param_name&#125;</span>: <span class="subst">&#123;best_parameters[param_name]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>我们更改了随机搜索的参数网格，结果似乎有了些许改进。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Best score: <span class="number">0.8905</span></span><br><span class="line">Best parameters <span class="built_in">set</span>:</span><br><span class="line">    criterion: entropy</span><br><span class="line">max_depth: <span class="number">25</span></span><br><span class="line">n_estimators: <span class="number">300</span></span><br></pre></td></tr></table></figure><p>如果迭代次数较少，随机搜索比网格搜索更快。使用这两种方法，你可以为各种模型找到最优参数，只要它们有拟合和预测功能，这也是 scikit-learn 的标准。有时，你可能想使用管道。例如，假设我们正在处理一个多类分类问题。在这个问题中，训练数据由两列文本组成，你需要建立一个模型来预测类别。让我们假设你选择的管道是首先以半监督的方式应用 tf-idf，然后使用 SVD 和 SVM 分类器。现在的问题是，我们必须选择 SVD 的成分，还需要调整 SVM 的参数。下面的代码段展示了如何做到这一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> TruncatedSVD</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="comment"># 计算加权二次 Kappa 分数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">quadratic_weighted_kappa</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="keyword">return</span> metrics.cohen_kappa_score(</span><br><span class="line">        y_true,</span><br><span class="line">        y_pred,</span><br><span class="line">        weights=<span class="string">&quot;quadratic&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 读取训练集</span></span><br><span class="line">    train = pd.read_csv(<span class="string">&#x27;../input/train.csv&#x27;</span>)</span><br><span class="line">    <span class="comment"># 从测试数据中提取 id 列的值，并将其转换为整数类型，存储在变量 idx 中</span></span><br><span class="line">    idx = test.<span class="built_in">id</span>.values.astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从训练数据中删除 &#x27;id&#x27; 列</span></span><br><span class="line">    train = train.drop(<span class="string">&#x27;id&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 从测试数据中删除 &#x27;id&#x27; 列</span></span><br><span class="line">    test = test.drop(<span class="string">&#x27;id&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 从训练数据中提取目标变量 &#x27;relevance&#x27; ，存储在变量 y 中</span></span><br><span class="line">    y = train.relevance.values</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将训练数据中的文本特征 &#x27;text1&#x27; 和 &#x27;text2&#x27; 合并成一个新的特征列，并存储在列表 traindata 中</span></span><br><span class="line">    traindata = <span class="built_in">list</span>(train.apply(<span class="keyword">lambda</span> x:<span class="string">&#x27;%s %s&#x27;</span> % (x[<span class="string">&#x27;text1&#x27;</span>], x[<span class="string">&#x27;text2&#x27;</span>]),axis=<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 将测试数据中的文本特征 &#x27;text1&#x27; 和 &#x27;text2&#x27; 合并成一个新的特征列，并存储在列表 testdata 中</span></span><br><span class="line">    testdata = <span class="built_in">list</span>(test.apply(<span class="keyword">lambda</span> x:<span class="string">&#x27;%s %s&#x27;</span> % (x[<span class="string">&#x27;text1&#x27;</span>], x[<span class="string">&#x27;text2&#x27;</span>]),axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建一个 TfidfVectorizer 对象 tfv，用于将文本数据转换为 TF-IDF 特征</span></span><br><span class="line">    tfv = TfidfVectorizer(</span><br><span class="line">        min_df=<span class="number">3</span>,</span><br><span class="line">        max_features=<span class="literal">None</span>,</span><br><span class="line">        strip_accents=<span class="string">&#x27;unicode&#x27;</span>,</span><br><span class="line">        analyzer=<span class="string">&#x27;word&#x27;</span>,</span><br><span class="line">        token_pattern=<span class="string">r&#x27;\w&#123;1,&#125;&#x27;</span>,</span><br><span class="line">        ngram_range=(<span class="number">1</span>, <span class="number">3</span>),</span><br><span class="line">        use_idf=<span class="number">1</span>,</span><br><span class="line">        smooth_idf=<span class="number">1</span>,</span><br><span class="line">        sublinear_tf=<span class="number">1</span>,</span><br><span class="line">        stop_words=<span class="string">&#x27;english&#x27;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用训练数据拟合 TfidfVectorizer，将文本特征转换为 TF-IDF 特征</span></span><br><span class="line">tfv.fit(traindata)</span><br><span class="line">    <span class="comment"># 将训练数据中的文本特征转换为 TF-IDF 特征矩阵 X</span></span><br><span class="line">X =  tfv.transform(traindata)</span><br><span class="line">    <span class="comment"># 将测试数据中的文本特征转换为 TF-IDF 特征矩阵 X_test</span></span><br><span class="line">X_test = tfv.transform(testdata)</span><br><span class="line">    <span class="comment"># 创建 TruncatedSVD 对象 svd，用于进行奇异值分解</span></span><br><span class="line">svd = TruncatedSVD()</span><br><span class="line">    <span class="comment"># 创建 StandardScaler 对象 scl，用于进行特征缩放</span></span><br><span class="line">scl = StandardScaler()</span><br><span class="line">    <span class="comment"># 创建支持向量机分类器对象 svm_model</span></span><br><span class="line">svm_model = SVC()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建机器学习管道 clf，包含奇异值分解、特征缩放和支持向量机分类器</span></span><br><span class="line">    clf = pipeline.Pipeline(</span><br><span class="line">        [</span><br><span class="line">            (<span class="string">&#x27;svd&#x27;</span>, svd),</span><br><span class="line">            (<span class="string">&#x27;scl&#x27;</span>, scl),</span><br><span class="line">            (<span class="string">&#x27;svm&#x27;</span>, svm_model)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义要进行网格搜索的参数网格 param_grid</span></span><br><span class="line">param_grid = &#123;</span><br><span class="line">        <span class="string">&#x27;svd__n_components&#x27;</span> : [<span class="number">200</span>, <span class="number">300</span>],</span><br><span class="line">        <span class="string">&#x27;svm__C&#x27;</span>: [<span class="number">10</span>, <span class="number">12</span>]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建自定义的评分函数 kappa_scorer，用于评估模型性能</span></span><br><span class="line">kappa_scorer = metrics.make_scorer(</span><br><span class="line">        quadratic_weighted_kappa,</span><br><span class="line">        greater_is_better=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建 GridSearchCV 对象 model，用于在参数网格上进行网格搜索，寻找最佳参数组合</span></span><br><span class="line">    model = model_selection.GridSearchCV(</span><br><span class="line">        estimator=clf,</span><br><span class="line">        param_grid=param_grid,</span><br><span class="line">        scoring=kappa_scorer,</span><br><span class="line">        verbose=<span class="number">10</span>,</span><br><span class="line">        n_jobs=-<span class="number">1</span>,</span><br><span class="line">        refit=<span class="literal">True</span>,</span><br><span class="line">        cv=<span class="number">5</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">     <span class="comment"># 使用 GridSearchCV 对象 model 拟合数据，寻找最佳参数组合</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line">    <span class="comment"># 打印出最佳模型的最佳准确度分数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best score: %0.3f&quot;</span> % model.best_score_)</span><br><span class="line">    <span class="comment"># 打印最佳参数集合</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best parameters set:&quot;</span>)</span><br><span class="line">best_parameters = model.best_estimator_.get_params()</span><br><span class="line"><span class="keyword">for</span> param_name <span class="keyword">in</span> <span class="built_in">sorted</span>(param_grid.keys()):</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\t%s: %r&quot;</span> % (param_name, best_parameters[param_name]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取最佳模型</span></span><br><span class="line">best_model = model.best_estimator_</span><br><span class="line">    best_model.fit(X, y)</span><br><span class="line">    <span class="comment"># 使用最佳模型进行预测</span></span><br><span class="line">    preds = best_model.predict(...)</span><br></pre></td></tr></table></figure><p>这里显示的管道包括 SVD（奇异值分解）、标准缩放和 SVM（支持向量机）模型。请注意，由于没有训练数据，您无法按原样运行上述代码。当我们进入高级超参数优化技术时，我们可以使用不同类型的<strong>最小化算法</strong>来研究函数的最小化。这可以通过使用多种最小化函数来实现，如下坡单纯形算法、内尔德-梅德优化算法、使用贝叶斯技术和高斯过程寻找最优参数或使用遗传算法。我将在 “集合与堆叠（ensembling and stacking） “一章中详细介绍下坡单纯形算法和 Nelder-Mead 算法的应用。首先，让我们看看高斯过程如何用于超参数优化。这类算法需要一个可以优化的函数。大多数情况下，都是最小化这个函数，就像我们最小化损失一样。</p><p>因此，比方说，你想找到最佳参数以获得最佳准确度，显然，准确度越高越好。现在，我们不能最小化精确度，但我们可以将精确度乘以-1。这样，我们是在最小化精确度的负值，但事实上，我们是在最大化精确度。 在高斯过程中使用贝叶斯优化，可以使用 scikit-optimize (skopt) 库中的 gp_minimize 函数。让我们看看如何使用该函数调整随机森林模型的参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> ensemble</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> skopt <span class="keyword">import</span> gp_minimize</span><br><span class="line"><span class="keyword">from</span> skopt <span class="keyword">import</span> space</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">optimize</span>(<span class="params">params, param_names, x, y</span>):</span><br><span class="line">    <span class="comment"># 将参数名称和对应的值打包成字典</span></span><br><span class="line">    params = <span class="built_in">dict</span>(<span class="built_in">zip</span>(param_names, params))</span><br><span class="line">    <span class="comment"># 创建随机森林分类器模型，使用传入的参数配置</span></span><br><span class="line">    model = ensemble.RandomForestClassifier(**params)</span><br><span class="line">    <span class="comment"># 创建 StratifiedKFold 交叉验证对象，将数据分为 5 折</span></span><br><span class="line">    kf = model_selection.StratifiedKFold(n_splits=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化用于存储每个折叠的准确度的列表</span></span><br><span class="line">    accuracies = []</span><br><span class="line"></span><br><span class="line">     <span class="comment"># 循环遍历每个折叠的训练和测试数据</span></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> kf.split(X=x, y=y):</span><br><span class="line">        train_idx, test_idx = idx[<span class="number">0</span>], idx[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        xtrain = x[train_idx]</span><br><span class="line">        ytrain = y[train_idx]</span><br><span class="line"></span><br><span class="line">        xtest = x[test_idx]</span><br><span class="line">        ytest = y[test_idx]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在训练数据上拟合模型</span></span><br><span class="line">        model.fit(xtrain, ytrain)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用模型对测试数据进行预测</span></span><br><span class="line">        preds = model.predict(xtest)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算折叠的准确度</span></span><br><span class="line">        fold_accuracy = metrics.accuracy_score(ytest, preds)</span><br><span class="line">        accuracies.append(fold_accuracy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回平均准确度的负数（因为 skopt 使用负数来最小化目标函数）</span></span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span> * np.mean(accuracies)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 读取数据</span></span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/mobile_train.csv&quot;</span>)</span><br><span class="line">    <span class="comment"># 取特征矩阵 X（去掉&quot;price_range&quot;列）</span></span><br><span class="line">    X = df.drop(<span class="string">&quot;price_range&quot;</span>, axis=<span class="number">1</span>).values</span><br><span class="line">    <span class="comment"># 目标变量 y（&quot;price_range&quot;列）</span></span><br><span class="line">    y = df.price_range.values</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义超参数搜索空间 param_space</span></span><br><span class="line">    param_space = [</span><br><span class="line">space.Integer(<span class="number">3</span>, <span class="number">15</span>, name=<span class="string">&quot;max_depth&quot;</span>),</span><br><span class="line">        space.Integer(<span class="number">100</span>, <span class="number">1500</span>, name=<span class="string">&quot;n_estimators&quot;</span>),</span><br><span class="line">        space.Categorical([<span class="string">&quot;gini&quot;</span>, <span class="string">&quot;entropy&quot;</span>], name=<span class="string">&quot;criterion&quot;</span>),</span><br><span class="line">        space.Real(<span class="number">0.01</span>, <span class="number">1</span>, prior=<span class="string">&quot;uniform&quot;</span>, name=<span class="string">&quot;max_features&quot;</span>)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义超参数的名称列表 param_names</span></span><br><span class="line">    param_names = [</span><br><span class="line">        <span class="string">&quot;max_depth&quot;</span>,</span><br><span class="line">        <span class="string">&quot;n_estimators&quot;</span>,</span><br><span class="line">        <span class="string">&quot;criterion&quot;</span>,</span><br><span class="line">        <span class="string">&quot;max_features&quot;</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建函数 optimization_function，用于传递给 gp_minimize</span></span><br><span class="line">optimization_function = partial(</span><br><span class="line">        optimize,</span><br><span class="line">        param_names=param_names,</span><br><span class="line">        x=X,</span><br><span class="line">        y=y</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用 Bayesian Optimization（基于贝叶斯优化）来搜索最佳超参数</span></span><br><span class="line">    result = gp_minimize(</span><br><span class="line">        optimization_function,</span><br><span class="line">        dimensions=param_space,</span><br><span class="line">        n_calls=<span class="number">15</span>,</span><br><span class="line">        n_random_starts=<span class="number">10</span>,</span><br><span class="line">        verbose=<span class="number">10</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取最佳超参数的字典</span></span><br><span class="line">best_params = <span class="built_in">dict</span>(</span><br><span class="line">        <span class="built_in">zip</span>(</span><br><span class="line">            param_names,</span><br><span class="line">            result.x</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 打印出找到的最佳超参数</span></span><br><span class="line"><span class="built_in">print</span>(best_params)</span><br></pre></td></tr></table></figure><p>这同样会产生大量输出，最后一部分如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Iteration No: <span class="number">14</span> started. Searching <span class="keyword">for</span> the <span class="built_in">next</span> optimal point.</span><br><span class="line">Iteration No: <span class="number">14</span> ended. Search finished <span class="keyword">for</span> the <span class="built_in">next</span> optimal point.</span><br><span class="line">Time taken: <span class="number">4.7793</span></span><br><span class="line">Function value obtained: -<span class="number">0.9075</span></span><br><span class="line">Current minimum: -<span class="number">0.9075</span></span><br><span class="line">Iteration No: <span class="number">15</span> started. Searching <span class="keyword">for</span> the <span class="built_in">next</span> optimal point.</span><br><span class="line">Iteration No: <span class="number">15</span> ended. Search finished <span class="keyword">for</span> the <span class="built_in">next</span> optimal point.</span><br><span class="line">Time taken: <span class="number">49.4186</span></span><br><span class="line">Function value obtained: -<span class="number">0.9075</span></span><br><span class="line">Current minimum: -<span class="number">0.9075</span></span><br><span class="line">&#123;<span class="string">&#x27;max_depth&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">100</span>, <span class="string">&#x27;criterion&#x27;</span>: <span class="string">&#x27;entropy&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;max_features&#x27;</span>: <span class="number">1.0</span>&#125;</span><br></pre></td></tr></table></figure><p>看来我们已经成功突破了 0.90 的准确率。这真是太神奇了！<br>我们还可以通过以下代码段查看（绘制）我们是如何实现收敛的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skopt.plots <span class="keyword">import</span> plot_convergence</span><br><span class="line">plot_convergence(result)</span><br></pre></td></tr></table></figure><p>收敛图如图 2 所示。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page179_image.png" alt=""></p><p align="center"><b>图 2：随机森林参数优化的收敛图</b> </p><p>Scikit- optimize 就是这样一个库。 hyperopt 使用树状结构贝叶斯估计器（TPE）来找到最优参数。请看下面的代码片段，我在使用 hyperopt 时对之前的代码做了最小的改动。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> ensemble</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> hyperopt <span class="keyword">import</span> hp, fmin, tpe, Trials</span><br><span class="line"><span class="keyword">from</span> hyperopt.pyll.base <span class="keyword">import</span> scope</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">optimize</span>(<span class="params">params, x, y</span>):</span><br><span class="line">    model = ensemble.RandomForestClassifier(**params)</span><br><span class="line">    kf = model_selection.StratifiedKFold(n_splits=<span class="number">5</span>)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span> * np.mean(accuracies)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/mobile_train.csv&quot;</span>)</span><br><span class="line">    X = df.drop(<span class="string">&quot;price_range&quot;</span>, axis=<span class="number">1</span>).values</span><br><span class="line">    y = df.price_range.values</span><br><span class="line">    <span class="comment"># 定义搜索空间（整型、浮点数型、选择型）</span></span><br><span class="line">    param_space = &#123;</span><br><span class="line">        <span class="string">&quot;max_depth&quot;</span>: scope.<span class="built_in">int</span>(hp.quniform(<span class="string">&quot;max_depth&quot;</span>, <span class="number">1</span>, <span class="number">15</span>, <span class="number">1</span>)),</span><br><span class="line">        <span class="string">&quot;n_estimators&quot;</span>: scope.<span class="built_in">int</span>(</span><br><span class="line">            hp.quniform(<span class="string">&quot;n_estimators&quot;</span>, <span class="number">100</span>, <span class="number">1500</span>, <span class="number">1</span>)</span><br><span class="line">        ),</span><br><span class="line">        <span class="string">&quot;criterion&quot;</span>: hp.choice(<span class="string">&quot;criterion&quot;</span>, [<span class="string">&quot;gini&quot;</span>, <span class="string">&quot;entropy&quot;</span>]),</span><br><span class="line">        <span class="string">&quot;max_features&quot;</span>: hp.uniform(<span class="string">&quot;max_features&quot;</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 包装函数</span></span><br><span class="line">    optimization_function = partial(</span><br><span class="line">        optimize,</span><br><span class="line">        x=X,</span><br><span class="line">        y=y</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    trials = Trials()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最小化目标值</span></span><br><span class="line">    hopt = fmin(</span><br><span class="line">        fn=optimization_function,</span><br><span class="line">        space=param_space,</span><br><span class="line">        algo=tpe.suggest,</span><br><span class="line">        max_evals=<span class="number">15</span>,</span><br><span class="line">        trials=trials</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">#打印最佳参数</span></span><br><span class="line">    <span class="built_in">print</span>(hopt)</span><br></pre></td></tr></table></figure><p>正如你所看到的，这与之前的代码并无太大区别。你必须以不同的格式定义参数空间，还需要改变实际优化部分，用 hyperopt 代替 gp_minimize。结果相当不错！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">❯ python rf_hyperopt.py</span><br><span class="line"><span class="number">100</span>%|██████████████████| <span class="number">15</span>/<span class="number">15</span> [04:<span class="number">38</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">18.57</span>s/trial, best loss: -</span><br><span class="line"><span class="number">0.9095000000000001</span>]</span><br><span class="line">&#123;<span class="string">&#x27;criterion&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">11.0</span>, <span class="string">&#x27;max_features&#x27;</span>: <span class="number">0.821163568049807</span>,</span><br><span class="line"><span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">806.0</span>&#125;</span><br></pre></td></tr></table></figure><p>我们得到了比以前更好的准确度和一组可以使用的参数。请注意，最终结果中的标准是 1。这意味着选择了 1，即熵。 上述调整超参数的方法是最常见的，几乎适用于所有模型：线性回归、逻辑回归、基于树的方法、梯度提升模型（如 xgboost、lightgbm），甚至神经网络！</p><p>虽然这些方法已经存在，但学习时必须从手动调整超参数开始，即手工调整。手动调整可以帮助你学习基础知识，例如，在梯度提升中，当你增加深度时，你应该降低学习率。如果使用自动工具，就无法学习到这一点。请参考下表，了解应如何调整。RS* 表示随机搜索应该更好。</p><p>一旦你能更好地手动调整参数，你甚至可能不需要任何自动超参数调整。创建大型模型或引入大量特征时，也容易造成训练数据的过度拟合。为避免过度拟合，需要在训练数据特征中引入噪声或对代价函数进行惩罚。这种惩罚称为<strong>正则化</strong>，有助于泛化模型。在线性模型中，最常见的正则化类型是 L1 和 L2。L1 也称为 Lasso 回归，L2 称为 Ridge 回归。说到神经网络，我们会使用 dropout、添加增强、噪声等方法对模型进行正则化。利用超参数优化，还可以找到正确的惩罚方法。</p><div class="table-container"><table><thead><tr><th>Model</th><th>Optimize</th><th>Range of values</th></tr></thead><tbody><tr><td>Linear Regression</td><td>- fit_intercept<br /> - normalize</td><td>- True/False<br /> - True/False</td></tr><tr><td>Ridge</td><td>- alpha<br /> - fit_intercept<br /> - normalize</td><td>- 0.01, 0.1, 1.0, 10, 100<br /> - True/False<br /> - True/False</td></tr><tr><td>k-neighbors</td><td>- n_neighbors<br /> - p</td><td>- 2, 4, 8, 16, …<br /> - 2, 3, …</td></tr><tr><td>SVM</td><td>- C<br /> - gamma<br /> - class_weight</td><td>- 0.001, 0.01, …,10, 100, 1000<br /> - ‘auto’, RS*<br /> - ‘balanced’, None</td></tr><tr><td>Logistic Regression</td><td>- Penalyt<br /> - C</td><td>- L1 or L2<br /> - 0.001, 0.01, …, 10, …, 100</td></tr><tr><td>Lasso</td><td>- Alpha<br /> - Normalize</td><td>- 0.1, 1.0, 10<br /> - True/False</td></tr><tr><td>Random Forest</td><td>- n_estimators<br/> - max_depth<br/> - min_samples_split <br/> - min_samples_leaf <br/> - max features</td><td>- 120, 300, 500, 800, 1200<br /> - 5, 8, 15, 25, 30, None<br /> - 1, 2, 5, 10, 15, 100<br /> - log2, sqrt, None</td></tr><tr><td>XGBoost</td><td>- eta<br/> - gamma<br/> - max_depth<br/> - min_child_weight<br/> - subsample<br/> - colsample_bytree<br/> - lambda<br/> - alpha</td><td>- 0.01, 0.015, 0.025, 0.05, 0.1<br /> - 0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0<br /> - 3, 5, 7, 9, 12, 15, 17, 25<br /> - 1, 3, 5, 7<br /> - 0.6, 0.7, 0.8, 0.9, 1.0<br /> - 0.6, 0.7, 0.8, 0.9, 1.0<br /> - 0.01, 0.1, 1.0, RS<em><br /> - 0, 0.1, 0.5, 1.0, RS</em></td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 杂学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>交叉检验</title>
      <link href="/%E6%9D%82%E5%AD%A6/AAAMLprob/%E4%BA%A4%E5%8F%89%E6%A3%80%E9%AA%8C/"/>
      <url>/%E6%9D%82%E5%AD%A6/AAAMLprob/%E4%BA%A4%E5%8F%89%E6%A3%80%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="交叉检验"><a href="#交叉检验" class="headerlink" title="交叉检验"></a>交叉检验</h1><blockquote><p>博客就像与人进行长时间的对话一样，因此你所喜欢谈论的事情与你的激情密切相关是有道理的。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><ul><li>承诺声明</li><li>即将到来的内容的预览<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2></li><li>简单的定义</li><li>示例</li><li>过渡到下一节<h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2></li><li>每个步骤的详细说明<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1></li><li>提醒指南的有用性</li><li>重申你的话题的重要性</li><li>呼吁行动<h1 id="清单"><a href="#清单" class="headerlink" title="清单"></a>清单</h1>灵感 ⛅</li></ul><ul><li>[ ] 阅读启发我的文章和观看视频</li><li>[ ] 用子弹点列出我想写的主题</li><li>[ ] 重新排列这些子弹点以形成思路<br>草稿 ✏️</li><li>[ ] 将这些子弹点扩展成句子/文本</li><li>[ ] 审阅文档<br>准备发布 🌐</li><li>[ ] 拟定5个标题并选择一个</li><li>[ ] 校对全文的错别字</li><li>[ ] 预览文本</li><li>[ ] 发布或安排帖子</li><li>[ ] 在社交媒体上推广</li></ul></blockquote><h1 id="交叉检验-1"><a href="#交叉检验-1" class="headerlink" title="交叉检验"></a>交叉检验</h1><p>在上一章中，我们没有建立任何模型。原因很简单，在创建任何一种机器学习模型之前，我们必须知道什么是交叉检验，以及如何根据数据集选择最佳交叉检验数据集。</p><p>那么，什么是<strong>交叉检验</strong>，我们为什么要关注它？</p><h1 id="交叉检验-2"><a href="#交叉检验-2" class="headerlink" title="交叉检验"></a>交叉检验</h1><blockquote><p>博客就像与人进行长时间的对话一样，因此你所喜欢谈论的事情与你的激情密切相关是有道理的。</p><h2 id="引言-1"><a href="#引言-1" class="headerlink" title="引言"></a>引言</h2><ul><li>承诺声明</li><li>即将到来的内容的预览<h2 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h2></li><li>简单的定义</li><li>示例</li><li>过渡到下一节<h2 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h2></li><li>每个步骤的详细说明<h1 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h1></li><li>提醒指南的有用性</li><li>重申你的话题的重要性</li><li>呼吁行动<h1 id="清单-1"><a href="#清单-1" class="headerlink" title="清单"></a>清单</h1>灵感 ⛅</li></ul><ul><li>[ ] 阅读启发我的文章和观看视频</li><li>[ ] 用子弹点列出我想写的主题</li><li>[ ] 重新排列这些子弹点以形成思路<br>草稿 ✏️</li><li>[ ] 将这些子弹点扩展成句子/文本</li><li>[ ] 审阅文档<br>准备发布 🌐</li><li>[ ] 拟定5个标题并选择一个</li><li>[ ] 校对全文的错别字</li><li>[ ] 预览文本</li><li>[ ] 发布或安排帖子</li><li>[ ] 在社交媒体上推广</li></ul></blockquote><p>关于什么是交叉检验，# 交叉检验</p><blockquote><p>博客就像与人进行长时间的对话一样，因此你所喜欢谈论的事情与你的激情密切相关是有道理的。</p><h2 id="引言-2"><a href="#引言-2" class="headerlink" title="引言"></a>引言</h2><ul><li>承诺声明</li><li>即将到来的内容的预览<h2 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h2></li><li>简单的定义</li><li>示例</li><li>过渡到下一节<h2 id="步骤-2"><a href="#步骤-2" class="headerlink" title="步骤"></a>步骤</h2></li><li>每个步骤的详细说明<h1 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h1></li><li>提醒指南的有用性</li><li>重申你的话题的重要性</li><li>呼吁行动<h1 id="清单-2"><a href="#清单-2" class="headerlink" title="清单"></a>清单</h1>灵感 ⛅</li></ul><ul><li>[ ] 阅读启发我的文章和观看视频</li><li>[ ] 用子弹点列出我想写的主题</li><li>[ ] 重新排列这些子弹点以形成思路<br>草稿 ✏️</li><li>[ ] 将这些子弹点扩展成句子/文本</li><li>[ ] 审阅文档<br>准备发布 🌐</li><li>[ ] 拟定5个标题并选择一个</li><li>[ ] 校对全文的错别字</li><li>[ ] 预览文本</li><li>[ ] 发布或安排帖子</li><li>[ ] 在社交媒体上推广<br>我们可以找到多种定义。我的定义只有一句话：交叉检验是构建机器学习模型过程中的一个步骤，它可以帮助我们确保模型准确拟合数据，同时确保我们不会过拟合。但这又引出了另一个词：<strong>过拟合</strong>。</li></ul></blockquote><p>要解释过拟合，我认为最好先看一个数据集。有一个相当有名的红酒质量数据集（<strong>red wine quality dataset</strong>）。这个数据集有 11 个不同的特征，这些特征决定了红酒的质量。</p><p>这些属性包括：</p><ul><li>固定酸度（fixed acidity）</li><li>挥发性酸度（volatile acidity）</li><li>柠檬酸（citric acid）</li><li>残留糖（residual sugar）</li><li>氯化物（chlorides）</li><li>游离二氧化硫（free sulfur dioxide）</li><li>二氧化硫总量（total sulfur dioxide）</li><li>密度（density）</li><li>PH 值（pH）</li><li>硫酸盐（sulphates）</li><li>酒精（alcohol）</li></ul><p>根据这些不同特征，我们需要预测红葡萄酒的质量，质量值介于 0 到 10 之间。</p><p>让我们看看这些数据是怎样的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">&quot;winequality-red.csv&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page14_image.png" alt=""></p><p align="center"><b>图 1:红葡萄酒质量数据集简单展示</b> </p><p>我们可以将这个问题视为分类问题，也可以视为回归问题。为了简单起见，我们选择分类。然而，这个数据集值包含 6 种质量值。因此，我们将所有质量值映射到 0 到 5 之间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个映射字典，用于将质量值从 0 到 5 进行映射</span></span><br><span class="line">quality_mapping = &#123;</span><br><span class="line"> <span class="number">3</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="number">4</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="number">5</span>: <span class="number">2</span>,</span><br><span class="line"> <span class="number">6</span>: <span class="number">3</span>,</span><br><span class="line"> <span class="number">7</span>: <span class="number">4</span>,</span><br><span class="line"> <span class="number">8</span>: <span class="number">5</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 你可以使用 pandas 的 map 函数以及任何字典，</span></span><br><span class="line"><span class="comment"># 来转换给定列中的值为字典中的值</span></span><br><span class="line">df.loc[:, <span class="string">&quot;quality&quot;</span>] = df.quality.<span class="built_in">map</span>(quality_mapping)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>当我们看大这些数据并将其视为一个分类问题时，我们脑海中会浮现出很多可以应用的算法，也许，我们可以使用神经网络。但是，如果我们从一开始就深入研究神经网络，那就有点牵强了。所以，让我们从简单的、我们也能可视化的东西开始：决策树。</p><p>在开始了解什么是过拟合之前，我们先将数据分为两部分。这个数据集有 1599 个样本。我们保留 1000 个样本用于训练，599 个样本作为一个单独的集合。</p><p>以下代码可以轻松完成划分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 frac=1 的 sample 方法来打乱 dataframe</span></span><br><span class="line"><span class="comment"># 由于打乱后索引会改变，所以我们重置索引</span></span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选取前 1000 行作为训练数据</span></span><br><span class="line">df_train = df.head(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选取最后的 599 行作为测试/验证数据</span></span><br><span class="line">df_test = df.tail(<span class="number">599</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在，我们将在训练集上使用 scikit-learn 训练一个决策树模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从 scikit-learn 导入需要的模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化一个决策树分类器，设置最大深度为 3</span></span><br><span class="line">clf = tree.DecisionTreeClassifier(max_depth=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择你想要训练模型的列</span></span><br><span class="line"><span class="comment"># 这些列作为模型的特征</span></span><br><span class="line">cols = [<span class="string">&#x27;fixed acidity&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;volatile acidity&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;citric acid&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;residual sugar&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;chlorides&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;free sulfur dioxide&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;total sulfur dioxide&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;density&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pH&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;sulphates&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;alcohol&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用之前映射的质量以及提供的特征来训练模型</span></span><br><span class="line">clf.fit(df_train[cols], df_train.quality)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>请注意，我将决策树分类器的最大深度（max_depth）设为 3。该模型的所有其他参数均保持默认值。现在，我们在训练集和测试集上测试该模型的准确性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在训练集上生成预测</span></span><br><span class="line">train_predictions = clf.predict(df_train[cols])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上生成预测</span></span><br><span class="line">test_predictions = clf.predict(df_test[cols])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算训练数据集上预测的准确度</span></span><br><span class="line">train_accuracy = metrics.accuracy_score(</span><br><span class="line"> df_train.quality, train_predictions</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算测试数据集上预测的准确度</span></span><br><span class="line">test_accuracy = metrics.accuracy_score(</span><br><span class="line"> df_test.quality, test_predictions</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>训练和测试的准确率分别为 58.9%和 54.25%。现在，我们将最大深度（max_depth）增加到 7，并重复上述过程。这样，训练准确率为 76.6%，测试准确率为 57.3%。在这里，我们使用准确率，主要是因为它是最直接的指标。对于这个问题来说，它可能不是最好的指标。我们可以根据最大深度（max_depth）的不同值来计算这些准确率，并绘制曲线图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意：这段代码在 Jupyter 笔记本中编写</span></span><br><span class="line"><span class="comment"># 导入 scikit-learn 的 tree 和 metrics</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="comment"># 导入 matplotlib 和 seaborn</span></span><br><span class="line"><span class="comment"># 用于绘图</span></span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置全局标签文本的大小</span></span><br><span class="line">matplotlib.rc(<span class="string">&#x27;xtick&#x27;</span>, labelsize=<span class="number">20</span>)</span><br><span class="line">matplotlib.rc(<span class="string">&#x27;ytick&#x27;</span>, labelsize=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确保图表直接在笔记本内显示</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化用于存储训练和测试准确度的列表</span></span><br><span class="line"><span class="comment"># 我们从 50% 的准确度开始</span></span><br><span class="line">train_accuracies = [<span class="number">0.5</span>]</span><br><span class="line">test_accuracies = [<span class="number">0.5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历几个不同的树深度值</span></span><br><span class="line"><span class="keyword">for</span> depth <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">25</span>):</span><br><span class="line">    <span class="comment"># 初始化模型</span></span><br><span class="line">    clf = tree.DecisionTreeClassifier(max_depth=depth)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择用于训练的列/特征</span></span><br><span class="line">    cols = [</span><br><span class="line">        <span class="string">&#x27;fixed acidity&#x27;</span>, <span class="string">&#x27;volatile acidity&#x27;</span>, <span class="string">&#x27;citric acid&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;residual sugar&#x27;</span>, <span class="string">&#x27;chlorides&#x27;</span>, <span class="string">&#x27;free sulfur dioxide&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;total sulfur dioxide&#x27;</span>, <span class="string">&#x27;density&#x27;</span>, <span class="string">&#x27;pH&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;sulphates&#x27;</span>, <span class="string">&#x27;alcohol&#x27;</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在给定特征上拟合模型</span></span><br><span class="line">    clf.fit(df_train[cols], df_train.quality)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建训练和测试预测</span></span><br><span class="line">    train_predictions = clf.predict(df_train[cols])</span><br><span class="line">    test_predictions = clf.predict(df_test[cols])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算训练和测试准确度</span></span><br><span class="line">    train_accuracy = metrics.accuracy_score(</span><br><span class="line">        df_train.quality, train_predictions</span><br><span class="line">    )</span><br><span class="line">    test_accuracy = metrics.accuracy_score(</span><br><span class="line">        df_test.quality, test_predictions</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加准确度到列表</span></span><br><span class="line">    train_accuracies.append(train_accuracy)</span><br><span class="line">    test_accuracies.append(test_accuracy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 matplotlib 和 seaborn 创建两个图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">sns.set_style(<span class="string">&quot;whitegrid&quot;</span>)</span><br><span class="line">plt.plot(train_accuracies, label=<span class="string">&quot;train accuracy&quot;</span>)</span><br><span class="line">plt.plot(test_accuracies, label=<span class="string">&quot;test accuracy&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;upper left&quot;</span>, prop=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">15</span>&#125;)</span><br><span class="line">plt.xticks(<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">26</span>, <span class="number">5</span>))</span><br><span class="line">plt.xlabel(<span class="string">&quot;max_depth&quot;</span>, size=<span class="number">20</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;accuracy&quot;</span>, size=<span class="number">20</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这将生成如图 2 所示的曲线图。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page19_image.png" alt=""></p><p align="center"><b>图 2：不同 max_depth 训练和测试准确率。</b> </p><p>我们可以看到，当最大深度（max_depth）的值为 14 时，测试数据的得分最高。随着我们不断增加这个参数的值，测试准确率会保持不变或变差，但训练准确率会不断提高。这说明，随着最大深度（max_depth）的增加，决策树模型对训练数据的学习效果越来越好，但测试数据的性能却丝毫没有提高。</p><p><strong>这就是所谓的过拟合</strong>。</p><p>模型在训练集上完全拟合，而在测试集上却表现不佳。这意味着模型可以很好地学习训练数据，但无法泛化到未见过的样本上。在上面的数据集中，我们可以建立一个最大深度（max_depth）非常高的模型，它在训练数据上会有出色的结果，但这种模型并不实用，因为它在真实世界的样本或实时数据上不会提供类似的结果。</p><p>有人可能会说，这种方法并没有过拟合，因为测试集的准确率基本保持不变。过拟合的另一个定义是，当我们不断提高训练损失时，测试损失也在增加。这种情况在神经网络中非常常见。</p><p>每当我们训练一个神经网络时，都必须在训练期间监控训练集和测试集的损失。如果我们有一个非常大的网络来处理一个非常小的数据集（即样本数非常少），我们就会观察到，随着我们不断训练，训练集和测试集的损失都会减少。但是，在某个时刻，测试损失会达到最小值，之后，即使训练损失进一步减少，测试损失也会开始增加。我们必须在验证损失达到最小值时停止训练。</p><p><strong>这是对过拟合最常见的解释</strong>。</p><p>奥卡姆剃刀用简单的话说，就是不要试图把可以用简单得多的方法解决的事情复杂化。换句话说，最简单的解决方案就是最具通用性的解决方案。一般来说，只要你的模型不符合奥卡姆剃刀原则，就很可能是过拟合。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page20_image.png" alt=""></p><p align="center"><b>图 3：过拟合的最一般定义</b> </p><p>现在我们可以回到交叉检验。</p><p>在解释过拟合时，我决定将数据分为两部分。我在其中一部分上训练模型，然后在另一部分上检查其性能。这也是交叉检验的一种，通常被称为 “暂留集”（<strong>hold-out set</strong>）。当我们拥有大量数据，而模型推理是一个耗时的过程时，我们就会使用这种（交叉）验证。</p><p>交叉检验有许多不同的方法，它是建立一个良好的机器学习模型的最关键步骤。<strong>选择正确的交叉检验</strong>取决于所处理的数据集，在一个数据集上适用的交叉检验也可能不适用于其他数据集。不过，有几种类型的交叉检验技术最为流行和广泛使用。</p><p>其中包括：</p><ul><li>k 折交叉检验</li><li>分层 k 折交叉检验</li><li>暂留交叉检验</li><li>留一交叉检验</li><li>分组 k 折交叉检验</li></ul><p>交叉检验是将训练数据分层几个部分，我们在其中一部分上训练模型，然后在其余部分上进行测试。请看图 4。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page21_image.png" alt=""></p><p align="center"><b>图 4：将数据集拆分为训练集和验证集</b> </p><p>图 4 和图 5 说明，当你得到一个数据集来构建机器学习模型时，你会把它们分成<strong>两个不同的集：训练集和验证集</strong>。很多人还会将其分成第三组，称之为测试集。不过，我们将只使用两个集。如你所见，我们将样本和与之相关的目标进行了划分。我们可以将数据分为 k 个互不关联的不同集合。这就是所谓的 <strong>k 折交叉检验</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page21_image_1.png" alt=""></p><p align="center"><b>图 5：K 折交叉检验</b> </p><p>我们可以使用 scikit-learn 中的 KFold 将任何数据分割成 k 个相等的部分。每个样本分配一个从 0 到 k-1 的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 pandas 和 scikit-learn 的 model_selection 模块</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 训练数据存储在名为 train.csv 的 CSV 文件中</span></span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;train.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 我们创建一个名为 kfold 的新列，并用 -1 填充</span></span><br><span class="line">    df[<span class="string">&quot;kfold&quot;</span>] = -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接下来的步骤是随机打乱数据的行</span></span><br><span class="line">    df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从 model_selection 模块初始化 kfold 类</span></span><br><span class="line">    kf = model_selection.KFold(n_splits=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 填充新的 kfold 列（enumerate的作用是返回一个迭代器）</span></span><br><span class="line">    <span class="keyword">for</span> fold, (trn_, val_) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kf.split(X=df)):</span><br><span class="line">        df.loc[val_, <span class="string">&#x27;kfold&#x27;</span>] = fold</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存带有 kfold 列的新 CSV 文件</span></span><br><span class="line">    df.to_csv(<span class="string">&quot;train_folds.csv&quot;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>几乎所有类型的数据集都可以使用此流程。例如，当数据图像时，您可以创建一个包含图像 ID、图像位置和图像标签的 CSV，然后使用上述流程。</p><p>另一种重要的交叉检验类型是<strong>分层 k 折交叉检验</strong>。如果你有一个偏斜的二元分类数据集，其中正样本占 90%，负样本只占 10%，那么你就不应该使用随机 k 折交叉。对这样的数据集使用简单的 k 折交叉检验可能会导致折叠样本全部为负样本。在这种情况下，我们更倾向于使用分层 k 折交叉检验。分层 k 折交叉检验可以保持每个折中标签的比例不变。因此，在每个折叠中，都会有相同的 90% 正样本和 10% 负样本。因此，无论您选择什么指标进行评估，都会在所有折叠中得到相似的结果。</p><p>修改创建 k 折交叉检验的代码以创建分层 k 折交叉检验也很容易。我们只需将 model_selection.KFold 更改为 model_selection.StratifiedKFold ，并在 kf.split(…) 函数中指定要分层的目标列。我们假设 CSV 数据集有一列名为 “target” ，并且是一个分类问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 pandas 和 scikit-learn 的 model_selection 模块</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 训练数据保存在名为 train.csv 的 CSV 文件中</span></span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;train.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加一个新列 kfold，并用 -1 初始化</span></span><br><span class="line">    df[<span class="string">&quot;kfold&quot;</span>] = -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机打乱数据行</span></span><br><span class="line">    df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取目标变量</span></span><br><span class="line">    y = df.target.values</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化 StratifiedKFold 类，设置折数（folds）为 5</span></span><br><span class="line">    kf = model_selection.StratifiedKFold(n_splits=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用 StratifiedKFold 对象的 split 方法来获取训练和验证索引</span></span><br><span class="line">    <span class="keyword">for</span> f, (t_, v_) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kf.split(X=df, y=y)):</span><br><span class="line">        df.loc[v_, <span class="string">&#x27;kfold&#x27;</span>] = f</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存包含 kfold 列的新 CSV 文件</span></span><br><span class="line">    df.to_csv(<span class="string">&quot;train_folds.csv&quot;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>对于葡萄酒数据集，我们来看看标签的分布情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b = sns.countplot(x=<span class="string">&#x27;quality&#x27;</span>, data=df)</span><br><span class="line">b.set_xlabel(<span class="string">&quot;quality&quot;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">b.set_ylabel(<span class="string">&quot;count&quot;</span>, fontsize=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><p>请注意，我们继续上面的代码。因此，我们已经转换了目标值。从图 6 中我们可以看出，质量偏差很大。有些类别有很多样本，有些则没有那么多。如果我们进行简单的 k 折交叉检验，那么每个折叠中的目标值分布都不会相同。因此，在这种情况下，我们选择分层 k 折交叉检验。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page24_image.png" alt=""></p><p align="center"><b>图 6：葡萄酒数据集中 "质量" 分布情况</b> </p><p>规则很简单，如果是标准分类问题，就盲目选择分层 k 折交叉检验。</p><p>但如果数据量很大，该怎么办呢？假设我们有 100 万个样本。5 倍交叉检验意味着在 800k 个样本上进行训练，在 200k 个样本上进行验证。根据我们选择的算法，对于这样规模的数据集来说，训练甚至验证都可能非常昂贵。在这种情况下，我们可以选择<strong>暂留交叉检验</strong>。</p><p>创建保持结果的过程与分层 k 折交叉检验相同。对于拥有 100 万个样本的数据集，我们可以创建 10 个折叠而不是 5 个，并保留其中一个折叠作为保留样本。这意味着，我们将有 10 万个样本被保留下来，我们将始终在这个样本集上计算损失、准确率和其他指标，并在 90 万个样本上进行训练。</p><p>在处理时间序列数据时，暂留交叉检验也非常常用。假设我们要解决的问题是预测一家商店 2020 年的销售额，而我们得到的是 2015-2019 年的所有数据。在这种情况下，你可以选择 2019 年的所有数据作为保留数据，然后在 2015 年至 2018 年的所有数据上训练你的模型。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page25_image.png" alt=""></p><p align="center"><b>图 7：时间序列数据示例</b> </p><p>在图 7 所示的示例中，假设我们的任务是预测从时间步骤 31 到 40 的销售额。我们可以保留 21 至 30 步的数据，然后从 0 步到 20 步训练模型。需要注意的是，在预测 31 步至 40 步时，应将 21 步至 30 步的数据纳入模型，否则，模型的性能将大打折扣。</p><p>在很多情况下，我们必须处理小型数据集，而创建大型验证集意味着模型学习会丢失大量数据。在这种情况下，我们可以选择留一交叉检验，相当于特殊的 k 则交叉检验其中 k=N ，N 是数据集中的样本数。这意味着在所有的训练折叠中，我们将对除 1 之外的所有数据样本进行训练。这种类型的交叉检验的折叠数与数据集中的样本数相同。</p><p>需要注意的是，如果模型的速度不够快，这种类型的交叉检验可能会耗费大量时间，但由于这种交叉检验只适用于小型数据集，因此并不重要。</p><p>现在我们可以转向回归问题了。回归问题的好处在于，除了分层 k 折交叉检验之外，我们可以在回归问题上使用上述所有交叉检验技术。也就是说，我们不能直接使用分层 k 折交叉检验，但有一些方法可以稍稍改变问题，从而在回归问题中使用分层 k 折交叉检验。大多数情况下，简单的 k 折交叉检验适用于任何回归问题。但是，如果发现目标分布不一致，就可以使用分层 k 折交叉检验。</p><p>要在回归问题中使用分层 k 折交叉检验，我们必须先将目标划分为若干个分层，然后再以处理分类问题的相同方式使用分层 k 折交叉检验。选择合适的分层数有几种选择。如果样本量很大（&gt; 10k，&gt; 100k），那么就不需要考虑分层的数量。只需将数据分为 10 或 20 层即可。如果样本数不多，则可以使用 Sturge’s Rule 这样的简单规则来计算适当的分层数。</p><p>Sturge’s Rule：</p><script type="math/tex; mode=display">Number of Bins = 1 + log_2(N)</script><p>其中 $N$ 是数据集中的样本数。该函数如图 8 所示。</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page26_image.png" alt=""></p><p align="center"><b>图 8：利用斯特格法则绘制样本与箱数对比图</b> </p><p>让我们制作一个回归数据集样本，并尝试应用分层 k 折交叉检验，如下面的 python 代码段所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># stratified-kfold for regression</span></span><br><span class="line"><span class="comment"># 为回归问题进行分层K-折交叉验证</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入需要的库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建分折（folds）的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_folds</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="comment"># 创建一个新列叫做kfold，并用-1来填充</span></span><br><span class="line">    data[<span class="string">&quot;kfold&quot;</span>] = -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机打乱数据的行</span></span><br><span class="line">    data = data.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用Sturge规则计算bin的数量</span></span><br><span class="line">    num_bins = <span class="built_in">int</span>(np.floor(<span class="number">1</span> + np.log2(<span class="built_in">len</span>(data))))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用pandas的cut函数进行目标变量（target）的分箱</span></span><br><span class="line">    data.loc[:, <span class="string">&quot;bins&quot;</span>] = pd.cut(</span><br><span class="line">        data[<span class="string">&quot;target&quot;</span>], bins=num_bins, labels=<span class="literal">False</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化StratifiedKFold类</span></span><br><span class="line">    kf = model_selection.StratifiedKFold(n_splits=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 填充新的kfold列</span></span><br><span class="line">    <span class="comment"># 注意：我们使用的是bins而不是实际的目标变量（target）！</span></span><br><span class="line">    <span class="keyword">for</span> f, (t_, v_) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kf.split(X=data, y=data.bins.values)):</span><br><span class="line">        data.loc[v_, <span class="string">&#x27;kfold&#x27;</span>] = f</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 删除bins列</span></span><br><span class="line">    data = data.drop(<span class="string">&quot;bins&quot;</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回包含folds的数据</span></span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主程序开始</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 创建一个带有15000个样本、100个特征和1个目标变量的样本数据集</span></span><br><span class="line">    X, y = datasets.make_regression(</span><br><span class="line">        n_samples=<span class="number">15000</span>, n_features=<span class="number">100</span>, n_targets=<span class="number">1</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 使用numpy数组创建一个数据框</span></span><br><span class="line">    df = pd.DataFrame(</span><br><span class="line">        X,</span><br><span class="line">        columns=[<span class="string">f&quot;f_<span class="subst">&#123;i&#125;</span>&quot;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>])]</span><br><span class="line">    )</span><br><span class="line">    df.loc[:, <span class="string">&quot;target&quot;</span>] = y</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建folds</span></span><br><span class="line">    df = create_folds(df)</span><br></pre></td></tr></table></figure><p>交叉检验是构建机器学习模型的第一步，也是最基本的一步。如果要做特征工程，首先要拆分数据。如果要建立模型，首先要拆分数据。如果你有一个好的交叉检验方案，其中验证数据能够代表训练数据和真实世界的数据，那么你就能建立一个具有高度通用性的好的机器学习模型。</p><p>本章介绍的交叉检验类型几乎适用于所有机器学习问题。不过，你必须记住，交叉检验也在很大程度上取决于数据，你可能需要根据你的问题和数据采用新的交叉检验形式。</p><p>例如，假设我们有一个问题，希望建立一个模型，从患者的皮肤图像中检测出皮肤癌。我们的任务是建立一个二元分类器，该分类器接收输入图像并预测其良性或恶性的概率。</p><p>在这类数据集中，训练数据集中可能有同一患者的多张图像。因此，要在这里建立一个良好的交叉检验系统，必须有分层的 k 折交叉检验，但也必须确保训练数据中的患者不会出现在验证数据中。幸运的是，scikit-learn 提供了一种称为 GroupKFold 的交叉检验类型。 在这里，患者可以被视为组。 但遗憾的是，scikit-learn 无法将 GroupKFold 与 StratifiedKFold 结合起来。所以你需要自己动手。我把它作为一个练习留给读者的练习。</p>]]></content>
      
      
      <categories>
          
          <category> 杂学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python打包成.exe</title>
      <link href="/%E6%9D%82%E5%AD%A6/python%E6%89%93%E5%8C%85%E6%88%90.exe/"/>
      <url>/%E6%9D%82%E5%AD%A6/python%E6%89%93%E5%8C%85%E6%88%90.exe/</url>
      
        <content type="html"><![CDATA[<h1 id="在-Python-开发中将脚本打包成可执行文件（-exe）"><a href="#在-Python-开发中将脚本打包成可执行文件（-exe）" class="headerlink" title="在 Python 开发中将脚本打包成可执行文件（.exe）"></a>在 Python 开发中将脚本打包成可执行文件（.exe）</h1><p>在 Python 开发中，将 Python 脚本打包成可执行文件（.exe）是一种常见需求。这样做可以使程序在没有安装 Python 解释器的环境下运行，同时也方便了程序的发布和分发。本文将介绍几种常见的方法来将 Python 代码打包成可执行文件。</p><h2 id="一、使用-pyinstaller-打包"><a href="#一、使用-pyinstaller-打包" class="headerlink" title="一、使用 pyinstaller 打包"></a>一、使用 pyinstaller 打包</h2><p><code>pyinstaller</code> 是一个流行的 Python 打包工具，它能够将 Python 脚本打包成各种平台的可执行文件，包括 Windows、Linux 和 macOS。使用 <code>pyinstaller</code> 可以非常简单地将 Python 代码打包成独立的可执行文件。</p><h3 id="安装-pyinstaller"><a href="#安装-pyinstaller" class="headerlink" title="安装 pyinstaller"></a>安装 <code>pyinstaller</code></h3><p>首先，您需要安装 <code>pyinstaller</code>。可以通过以下命令进行安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyinstaller</span><br></pre></td></tr></table></figure><h3 id="使用-pyinstaller-打包"><a href="#使用-pyinstaller-打包" class="headerlink" title="使用 pyinstaller 打包"></a>使用 <code>pyinstaller</code> 打包</h3><p>接下来，您可以使用 <code>pyinstaller</code> 来打包您的 Python 脚本。以下是一些常用的打包命令：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打包单个文件</span></span><br><span class="line">pyinstaller your_script.py</span><br><span class="line"><span class="comment"># 打包多个 py 文件</span></span><br><span class="line">pyinstaller [主文件] -p [其他文件<span class="number">1</span>] -p [其他文件<span class="number">2</span>]</span><br><span class="line"><span class="comment"># 打包时去除 cmd 框</span></span><br><span class="line">pyinstaller -F XXX.py --noconsole</span><br><span class="line"><span class="comment"># 打包加入 exe 图标</span></span><br><span class="line">pyinstaller -F -i picturename.ico -w XXX.py</span><br><span class="line"><span class="comment"># 打包去除控制台</span></span><br><span class="line">pyinstaller -w xxx.py</span><br><span class="line"><span class="comment"># 打包方便查看报错，可看到控制台</span></span><br><span class="line">pyinstaller -c xxx.py</span><br></pre></td></tr></table></figure><p>如果遇到错误 <code>AttributeError: module &#39;enum&#39; has no attribute &#39;IntFlag&#39;</code>，请检查是否安装了 <code>enum34</code> 包，并卸载它以解决问题。<br>执行以上命令后，<code>pyinstaller</code> 将在当前目录下生成一个 <code>dist</code> 文件夹，其中包含了打包好的可执行文件。</p><h3 id="处理-gradio-库依赖"><a href="#处理-gradio-库依赖" class="headerlink" title="处理 gradio 库依赖"></a>处理 <code>gradio</code> 库依赖</h3><p>如果您在程序中使用了 <code>gradio</code> 库，您可能需要在打包时特别注意。如果在打包后双击程序时出现闪退，您可以在命令行中运行程序以查看具体的报错原因。<br>如果遇到错误 <code>FileNotFoundError: [Errno 2] No such file or directory</code>，这通常是因为 <code>pyinstaller</code> 没有正确识别 <code>gradio</code> 相关的依赖项。您可以通过以下命令来修正这个问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyinstaller -F python_file_name --collect-data=gradio_client --collect-data=gradio</span><br></pre></td></tr></table></figure><p>如果出现 <code>FileNotFoundError: [Errno 2] No such file or directory: gradio\blocks_events.pyc</code>，则需要修改 <code>spec</code> 文件来指定对 <code>gradio</code> 库下的代码进行编译。具体操作如下：</p><ol><li>生成 <code>spec</code> 文件：<code>pyi-makespec --collect-data=gradio_client --collect-data=gradio python_file_name</code></li><li>打开与要打包的 <code>py</code> 代码同名的 <code>spec</code> 文件，在 <code>A = Analysis&#123;&#125;</code> 添加对 <code>gradio</code> 的编译：<code>module_collection_mode=&#123; &#39;gradio&#39;: &#39;py&#39;,&#125;</code></li><li>删除目录下的 <code>build</code> 文件夹，再次执行 <code>pyinstaller python_file_name.spec</code><br>然后，您可以进入 <code>dist</code> 目录，找到生成的 <code>exe</code> 文件。</li></ol><h3 id="使用-spec-文件生成单个-exe-文件"><a href="#使用-spec-文件生成单个-exe-文件" class="headerlink" title="使用 spec 文件生成单个 exe 文件"></a>使用 <code>spec</code> 文件生成单个 <code>exe</code> 文件</h3><p>您可以使用以下 <code>spec</code> 文件来生成单个 <code>exe</code> 文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"># -*- mode: python ; coding: utf-8 -*-</span><br><span class="line">from PyInstaller.utils.hooks import collect_data_files</span><br><span class="line"></span><br><span class="line">datas = []</span><br><span class="line">datas += collect_data_files(&#x27;gradio_client&#x27;)</span><br><span class="line">datas += collect_data_files(&#x27;gradio&#x27;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = Analysis(</span><br><span class="line">    [&#x27;08.onnxgradio.py&#x27;],</span><br><span class="line">    pathex=[],</span><br><span class="line">    binaries=[],</span><br><span class="line">    datas=datas,</span><br><span class="line">    hiddenimports=[],</span><br><span class="line">    hookspath=[],</span><br><span class="line">    hooksconfig=&#123;&#125;,</span><br><span class="line">    runtime_hooks=[],</span><br><span class="line">    excludes=[],</span><br><span class="line">    noarchive=False,</span><br><span class="line">module_collection_mode=&#123; &#x27;gradio&#x27;: &#x27;py&#x27;,&#125;</span><br><span class="line">)</span><br><span class="line">pyz = PYZ(a.pure)</span><br><span class="line"></span><br><span class="line">exe = EXE(</span><br><span class="line">    pyz,</span><br><span class="line">    a.scripts,</span><br><span class="line">    a.binaries,</span><br><span class="line">    a.datas,</span><br><span class="line">    [],</span><br><span class="line">    name=&#x27;08.onnxgradio&#x27;,</span><br><span class="line">    debug=False,</span><br><span class="line">    bootloader_ignore_signals=False,</span><br><span class="line">    strip=False,</span><br><span class="line">    upx=True,</span><br><span class="line">    upx_exclude=[],</span><br><span class="line">    runtime_tmpdir=None,</span><br><span class="line">    console=True,</span><br><span class="line">    disable_windowed_traceback=False,</span><br><span class="line">    argv_emulation=False,</span><br><span class="line">    target_arch=None,</span><br><span class="line">    codesign_identity=None,</span><br><span class="line">    entitlements_file=None,</span><br><span class="line">)</span><br><span class="line">coll = COLLECT(</span><br><span class="line">    exe,</span><br><span class="line">    a.binaries,</span><br><span class="line">    a.datas,</span><br><span class="line">    strip=False,</span><br><span class="line">    upx=True,</span><br><span class="line">    upx_exclude=[],</span><br><span class="line">    name=&#x27;08.onnxgradio&#x27;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="二、cx-Freeze"><a href="#二、cx-Freeze" class="headerlink" title="二、cx_Freeze"></a>二、cx_Freeze</h2><p><code>cx_Freeze</code> 是另一个常用的 Python 打包工具，可以将 <code>Python</code> 脚本打包成可执行文件，并且支持跨平台。使用 <code>cx_Freeze</code> 也可以将 Python 代码打包成独立的可执行文件。</p><p>安装 <code>cx_Freeze</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install cx-Freeze</span><br></pre></td></tr></table></figure><p>使用 <code>cx_Freeze 打包</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cxfreeze your_script.py --target-dir dist</span><br></pre></td></tr></table></figure><p>执行以上命令后，<code>cx_Freeze</code> 将会在指定的目录下生成可执行文件。</p>]]></content>
      
      
      <categories>
          
          <category> 杂学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数学物理方程第一章</title>
      <link href="/uncategorized/%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86%E6%96%B9%E7%A8%8B%E7%AC%AC%E4%B8%80%E7%AB%A0/"/>
      <url>/uncategorized/%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86%E6%96%B9%E7%A8%8B%E7%AC%AC%E4%B8%80%E7%AB%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="书名-数学物理方程"><a href="#书名-数学物理方程" class="headerlink" title="书名-数学物理方程"></a>书名-数学物理方程</h2><h2 id="新知识概览"><a href="#新知识概览" class="headerlink" title="新知识概览"></a>新知识概览</h2><h1 id="一些典型方程和定解条件的推导"><a href="#一些典型方程和定解条件的推导" class="headerlink" title="一些典型方程和定解条件的推导"></a>一些典型方程和定解条件的推导</h1><h3 id="基本方程的建立"><a href="#基本方程的建立" class="headerlink" title="基本方程的建立"></a>基本方程的建立</h3><p>简要介绍本课的新知识，包括概念、定义和基本原理。</p><h3 id="初值条件与边界条件"><a href="#初值条件与边界条件" class="headerlink" title="初值条件与边界条件"></a>初值条件与边界条件</h3><p>问题所具有的特定条件也用数学形式表达出来，这是因为任何一个具体的物理现象都是处在特定条件之下的.</p><h3 id="定解问题的提法"><a href="#定解问题的提法" class="headerlink" title="定解问题的提法"></a>定解问题的提法</h3><p>由于每一个物理过程都处在特定的条件之下，所以我们的任务是要求出偏微分方程的适合某些特定条件的解.初值条件和边界条件都称为定解条件.把某个偏微分方程和相应的定解条件结合在一起，就构成了一个定解问题。</p><h2 id="学习笔记"><a href="#学习笔记" class="headerlink" title="学习笔记"></a>学习笔记</h2><h3 id="基本方程的建立-1"><a href="#基本方程的建立-1" class="headerlink" title="基本方程的建立"></a>基本方程的建立</h3><h4 id="例-1-弦的振动"><a href="#例-1-弦的振动" class="headerlink" title="例 1 弦的振动"></a>例 1 弦的振动</h4><p>设有一根均匀柔软的 细弦，平 衡时沿 直线拉紧，而且除 受不 随时间 而 变的张力作用及弦本身的重力外，不受外力影响.下面研究弦作微小横向振动的规律.所谓“横向”是指全部运动出现在一个平面上，而 且弦上的点沿垂直于 x 轴的方向运动(图 1-1).所谓“微小”是指振动的幅度及弦在任意位置处切线的倾角都很小，以至它们的高于一次方的项都可略而不计.</p><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/sxwlfc1.1.png" alt="图1-1"><br>用牛顿运动定律，作用于弧段上任一方向上的力的总和等于这段弧的 质量 乘该方向 上的加速度.在 x 轴方向，弧段 MM’受力的总和为<code>-Tcos α+T&#39;cos α&#39;</code>,由于弦只作横向振动，所以</p><script type="math/tex; mode=display">T'\cos\alpha'-T\cos\alpha=0.\quad(1.1)</script><p>按照上述弦振动微小的假设，可知在振动过程中弦上 M 点与 M’点处切线的倾角都很小，<code>α≈0,α&#39;=0</code>,从而由</p><script type="math/tex; mode=display">\cos\alpha=1-\frac{\alpha^{2}}{2!}+\frac{\alpha^{4}}{4!}-\cdots</script><p>可知，当我们略去$\alpha$与$\alpha^\prime$的所有高于一次方的各项时，就有$\cos \alpha \approx 1$, $\cos \alpha ^{\prime }\approx 1$,<br>代人<code>(1.1)</code>式，便可近似得到</p><script type="math/tex; mode=display">T=T^\prime.</script><p>在$u$ 方向，弧段$\widehat MM^\prime$受力的总和为-$T\sin\alpha+T^{\prime}\sin\alpha^{\prime}-\rho g$ds,其中-$\rho g$ds 是弧段$\widehat{MM}^{\prime}$的重力.又因当$\alpha\approx0,\alpha^\prime\approx0$时</p><script type="math/tex; mode=display">\sin\alpha=\frac{\tan\alpha}{\sqrt{1+\tan^{2}\alpha}}\approx\tan\alpha=\frac{\partial u(x,t)}{\partial x},</script><script type="math/tex; mode=display">\sin\alpha^{\prime}\approx\tan\alpha^{\prime}=\frac{\partial u\left(x+\mathrm{d}x,t\right)}{\partial x},</script><script type="math/tex; mode=display">\mathrm{d}s=\sqrt{1+\left[\frac{\partial u\left(x,t\right)}{\partial x}\right]^{2}}\mathrm{d}x\approx\mathrm{d}x,</script><p>且小弧段在时刻$\iota$沿$u$方向运动的加速度近似为$\frac{\partial^2u(x,t)}{\partial t^2}$,小弧段的质量为$\rho$ds ,所以</p><script type="math/tex; mode=display">-T\sin\alpha+T'\sin\alpha'-\rho g\mathrm{d}s\approx\rho\mathrm{d}s\frac{\partial^{2}u(x,t)}{\partial t^{^2}}</script><p>或</p><script type="math/tex; mode=display">T\Big[\frac{\partial u\left(x+\mathrm{d}x,t\right)}{\partial x}-\frac{\partial u\left(x,t\right)}{\partial x}\Big]-\rho g\mathrm{d}x\approx\rho\frac{\partial^{2}u\left(x,t\right)}{\partial t^{2}}\mathrm{d}x,(1.2)</script><p>上式左边方括号内的部分是由于$x$产生 d$x$ 的变化而引起的$\frac\partial u(x,t){\partial x}$的改变量，可用微分近似代替，即</p><script type="math/tex; mode=display">\frac{\partial u\left(x+\mathrm{d}x,t\right)}{\partial x}-\frac{\partial u\left(x,t\right)}{\partial u}\approx\frac{\partial}{\partial x}\biggl[\frac{\partial u\left(x,t\right)}{\partial x}\biggr]\mathrm{d}x</script><script type="math/tex; mode=display">=\frac{\partial^{2}u\left(x,t\right)}{\partial x^{2}}\mathrm{d}x,</script><p>于是</p><script type="math/tex; mode=display">\left[T\frac{\partial^{2}u\left(x,t\right)}{\partial x^{2}}-\rho g\right]\mathrm{d}x\approx\rho\frac{\partial^{2}u\left(x,t\right)}{\partial t^{2}}\mathrm{d}x</script><p>或</p><script type="math/tex; mode=display">\frac{T}{\rho}\frac{\partial^{2}u\left(x,t\right)}{\partial x^{2}}\approx\frac{\partial^{2}u\left(x,t\right)}{\partial t^{2}}+g.</script><p>一般说来，张力较大时弦振动速度变化很快，即$\frac{\partial^2u}{\partial t^2}$要比 $g$ 大得多，所以又可以<br>把$g$略去.经过这样逐步略去一些次要的量，抓住主要的量，在$u(x,t)$关于${x,t}$都是二次连续可微的前提下，最后得出$u(x,t)$应近似地满足方程</p><script type="math/tex; mode=display">\frac{\partial^{2}u}{\partial t^{2}}=a^{2}\frac{\partial^{2}u}{\partial x^{2}},(1.3)</script><p>这里的$a^{2}=\frac{T}{\rho}.(1.3)$式称为一维波动方程.<br>如果在振动过程中，弦上另外还受到一个与弦的振动方向平行的外力，且假定在时刻$\iota$弦上$x$点处的外力密度为$F(x,t)$,显然，这时(1.1)及(1.2)分别为</p><script type="math/tex; mode=display">T^{\prime}\cos\alpha^{\prime}-T\cos\alpha=0,</script><script type="math/tex; mode=display">F\mathrm{d}s-T\mathrm{sin}\alpha+T'\mathrm{sin}\alpha'-\rho g\mathrm{d}s\approx\rho\mathrm{d}s\frac{\partial^{2}u}{\partial t^{2}}.</script><p>利用上面的推导方法并略去弦本身的质量，可得弦的强迫振动方程为</p><script type="math/tex; mode=display">\frac{\partial^{2}u}{\partial t^{2}}=a^{2}\frac{\partial^{2}u}{\partial x^{2}}+f(x,\iota),(1.3)^{\prime}</script><p>其中$f(x,t)=\frac1aF(x,t)$ ,表示 $t$ 时刻单位质量的弦在 $x$ 点处所受的外力密度.<br>方程( 1.3)与$(1.3)^{\prime}$的差别在于$(1.3)^{\prime}$的右端多了一个与未知函数$u$无关的项 $f(x,t)$,这个项称为自由项.包括非零自由项的方程称为非齐次方程，自由项恒等于零的方程称为齐次方程.(1.3)为齐次一维波动方程，$(1.3)^{\prime}$为非齐次一维波动方程.</p><h3 id="初值条件与边界条件-1"><a href="#初值条件与边界条件-1" class="headerlink" title="初值条件与边界条件"></a>初值条件与边界条件</h3><p>在将物理问题转化为数学表达式时，除了需要表达物理规律本身，还需要将问题的特定条件用数学形式表达出来。这些特定条件包括初始状态和边界约束，分别称为初值条件和边界条件。例如在弦振动问题中，虽然我们已经得到了描述弦振动的普遍方程，但如果没有考虑弦的初始状态和两端点所受的约束，我们就无法准确地描述一个特定情况下的弦振动。因此，为了精确描述和研究具体的物理现象，我们需要将这些特定条件纳入数学模型中。<br>下面具体说明初值条件和边界条件的表达形式.先谈初值条件，对于弦振动问题来说，初值条件就是弦在开始时刻的位移及速度，若以$\varphi(x),\psi(x)$分别表示初位移和初速度，则初值条件可以表达为</p><script type="math/tex; mode=display">\begin{cases}u\bigg|_{t=0}=\varphi(x),\\\\\frac{\partial u}{\partial t}\bigg|_{t=0}=\psi(x).\end{cases},(1.22)</script><h3 id="定解问题的提法-1"><a href="#定解问题的提法-1" class="headerlink" title="定解问题的提法"></a>定解问题的提法</h3><p>在工程技术中，经常会遇到需要解决偏微分方程的问题。一个函数如果满足某个偏微分方程，并且具有所需的连续偏导数，使得代入方程后成为恒等式，那么这个函数就是该方程的解。<br>由于物理过程总是在特定条件下发生，我们需要找到满足特定条件的偏微分方程的解，这些特定条件包括<strong>初值条件</strong>和<strong>边界条件</strong>，统称为<strong>定解条件</strong>。<br>定解问题可以分为三类：</p><ul><li><strong>初值问题（或称柯西问题）</strong>：只有初值条件。</li><li><strong>边值问题</strong>：只有边界条件。</li><li><strong>混合问题</strong>：既有初值条件又有边界条件。<br>定解问题的合理性可以从三个方面检验：</li><li><strong>存在性:</strong> 即看所归结出来的定解问题是否有解；</li><li><strong>唯一性:</strong> 即看是否只有一个解；</li><li><strong>稳定性:</strong> 即看当定解条件有微小变动时，解是否相应地只有微小的变动，如果确实如此，此解便称为稳定的<br>如果定解问题存在唯一且稳定的解，则称为<strong>适定的</strong>。然而，讨论定解问题的适定性通常非常困难，因此本书将重点放在讨论定解问题的解法上，而不过多涉及适定性的讨论，因为书中所讨论的定解问题都是经典的，它们的适定性已经得到了证明。</li></ul><h2 id="练习题与答案"><a href="#练习题与答案" class="headerlink" title="练习题与答案"></a>练习题与答案</h2><h3 id="无"><a href="#无" class="headerlink" title="无"></a>无</h3><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li>工程数学 数学物理方程与特征函数</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 数学物理方程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>九种常见激活函数</title>
      <link href="/%E6%9D%82%E5%AD%A6/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
      <url>/%E6%9D%82%E5%AD%A6/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>神经网络中的激活函数是神经网络中至关重要的一环，它们决定了神经网络的非线性特性，能够让神经网络学习非常复杂的函数。激活函数的种类也非常丰富，从最早的 sigmoid、tanh，到后来的 ReLU、LeakyReLU、ELU 等，再到最近的 GELU、SE-ReLU、SiLU 等，每种激活函数都有其独特的优点和适用场景。</p><p>在本篇文章中，我们将对神经网络中常见的激活函数进行总结和介绍，同时也会介绍一些新兴的激活函数，帮助读者了解它们的特点和使用方法，以便在实际应用中能够选择合适的激活函数来提升神经网络的性能。</p><h3 id="常见的激活函数"><a href="#常见的激活函数" class="headerlink" title="常见的激活函数"></a>常见的激活函数</h3><p>以下是一些常见的激活函数：</p><ul><li><strong>Sigmoid</strong></li><li><strong>Tanh</strong></li><li><strong>ReLU</strong></li><li><strong>LeakyReLU</strong></li><li><strong>ELU</strong></li></ul><h3 id="新兴的激活函数"><a href="#新兴的激活函数" class="headerlink" title="新兴的激活函数"></a>新兴的激活函数</h3><p>以下是一些新兴的激活函数：</p><ul><li><strong>GELU</strong></li><li><strong>SE-ReLU</strong></li><li><strong>SiLU</strong></li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>神经网络中的激活函数是非常重要的组成部分，它的作用是将神经元的输入信号转换为输出信号，从而实现神经网络的非线性映射。激活函数的意义在于它能够引入非线性特性，使得神经网络可以拟合非常复杂的函数，从而提高了神经网络的表达能力和预测性能。</p><p>具体来说，激活函数的作用有以下几个方面：</p><ol><li><strong>引入非线性特性</strong>：激活函数能够将神经元的输入信号转换为输出信号，从而引入非线性特性，使得神经网络可以拟合非常复杂的函数。</li><li><strong>压缩输出范围</strong>：激活函数能够将神经元的输出范围压缩到一定的范围内，这有助于防止神经元输出的值过大或过小，从而提高了神经网络的稳定性和泛化性能。</li><li><strong>增加网络深度</strong>：激活函数能够增加神经网络的深度，从而提高了神经网络的表达能力和预测性能。</li><li><strong>改善梯度消失问题</strong>：激活函数能够改善神经网络中的梯度消失问题，从而提高了神经网络的训练效率和收敛速度。</li></ol><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><h3 id="sigmoid-函数"><a href="#sigmoid-函数" class="headerlink" title="sigmoid 函数"></a>sigmoid 函数</h3><p>sigmoid 函数是神经网络中最早也是最常用的激活函数之一，它的特点是将输入值映射到 0 到 1 之间的连续范围内，输出值具有良好的可解释性，但是它在梯度消失和输出饱和等问题上表现不佳。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.</span> / (<span class="number">1.</span> + np.exp(-x))</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/sigmoid.jpg" alt="Sigmoid"></p><h3 id="ReLU-函数"><a href="#ReLU-函数" class="headerlink" title="ReLU 函数"></a>ReLU 函数</h3><p>ReLU 函数是当前最常用的激活函数之一，它的特点是简单、快速，并且在许多情况下表现出色。ReLU 函数将负数输入映射到 0，将正数输入保留不变，因此在训练过程中可以避免梯度消失的问题。但是 ReLU 函数在输入为负数时输出为 0，这可能导致神经元死亡，因此后续的改进版本 LeakyReLU 得到了广泛的应用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ReLU</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/ReLU.jpg" alt="ReLU"></p><h3 id="LeakyReLU-函数"><a href="#LeakyReLU-函数" class="headerlink" title="LeakyReLU 函数"></a>LeakyReLU 函数</h3><p>LeakyReLU 函数是 ReLU 函数的改进版本，它在输入为负数时输出一个小的负数，从而避免了 ReLU 函数可能导致神经元死亡的问题。LeakyReLU 函数的优点是简单、快速，并且在许多情况下表现出色，但是其超参数需要手动调整，因此在实际应用中需要进行一定的调试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">LeakyReLU</span>(<span class="params">x, alpha=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(alpha*x, x)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/LeakyReLU.jpg" alt="LeakyReLU"></p><h3 id="Tanh-函数"><a href="#Tanh-函数" class="headerlink" title="Tanh 函数"></a>Tanh 函数</h3><p>Tanh 函数是一种具有 S 形状的激活函数，其特点是将输入值映射到-1 到 1 之间的连续范围内，输出值也具有良好的可解释性。Tanh 函数在某些情况下可以表现出色，但是它也存在梯度消失和输出饱和等问题，因此在深度神经网络中使用并不广泛。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Tanh</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.tanh(x)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/tanh.jpg" alt="Tanh"></p><h3 id="Softmax-函数"><a href="#Softmax-函数" class="headerlink" title="Softmax 函数"></a>Softmax 函数</h3><p>Softmax 函数是一种常用于多分类问题的激活函数，它将输入值映射到 0 到 1 之间的概率分布，可以将神经网络的输出转换为各个类别的概率值。Softmax 函数的优点是简单、易于理解，并且在多分类问题中表现出色，但是它也存在梯度消失和输出饱和等问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Softmax</span>(<span class="params">x</span>):</span><br><span class="line">    exp_x = np.exp(x)</span><br><span class="line">    <span class="keyword">return</span> exp_x / np.<span class="built_in">sum</span>(exp_x, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/softmax.jpg" alt="SoftMax"></p><h3 id="GELU-函数"><a href="#GELU-函数" class="headerlink" title="GELU 函数"></a>GELU 函数</h3><p>GELU 函数是一种近年来提出的激活函数，它的特点是在 ReLU 函数的基础</p><p>上引入了高斯误差线性单元，从而在某些情况下能够表现出色。GELU 函数具有平滑的非线性特性，可以避免 ReLU 函数可能导致的神经元死亡问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">GELU</span>(<span class="params">x</span>):</span><br><span class="line">    cdf = <span class="number">0.5</span> * (<span class="number">1.0</span> + np.tanh((np.sqrt(<span class="number">2</span> / np.pi) * (x + <span class="number">0.044715</span> * np.power(x, <span class="number">3</span>)))))</span><br><span class="line">    <span class="keyword">return</span> x * cdf</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/GeLU.jpg" alt="GeLU"></p><h3 id="SE-ReLU-函数"><a href="#SE-ReLU-函数" class="headerlink" title="SE_ReLU 函数"></a>SE_ReLU 函数</h3><p>SE_ReLU 函数是一种近年来提出的激活函数，它的特点是在 ReLU 函数的基础上引入了 Sigmoid 函数和 Exponential 函数，从而能够增加神经元的表达能力。SE_ReLU 函数具有非常好的平滑性和可解释性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">SE_ReLU</span>(<span class="params">x, alpha=<span class="number">0.1</span>, beta=<span class="number">10</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.where(x &gt; <span class="number">0</span>, x + alpha * x * np.exp(-beta * x), x)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/SE_ReLU.jpg" alt="SE_ReLU"></p><h3 id="SiLU-函数"><a href="#SiLU-函数" class="headerlink" title="SiLU 函数"></a>SiLU 函数</h3><p>SiLU 函数是一种近年来提出的激活函数，它的特点是在 sigmoid 函数的基础上引入了自身的输入，从而能够表现出更好的非线性特性。SiLU 函数具有非常好的平滑性和可解释性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">SE_ReLU</span>(<span class="params">x, alpha=<span class="number">0.1</span>, beta=<span class="number">10</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.where(x &gt; <span class="number">0</span>, x + alpha * x * np.exp(-beta * x), x)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/SiLU.jpg" alt="SiLU"></p><h3 id="DynamicShiftMax-amp-DynamicReLU-A-amp-DynamicReLU-B-函数"><a href="#DynamicShiftMax-amp-DynamicReLU-A-amp-DynamicReLU-B-函数" class="headerlink" title="DynamicShiftMax &amp; DynamicReLU_A &amp; DynamicReLU_B 函数"></a>DynamicShiftMax &amp; DynamicReLU_A &amp; DynamicReLU_B 函数</h3><p>DynamicShiftMax 函数是一种近年来提出的激活函数，它的特点是在 ReLU 函数的基础上引入了动态偏移量，从而能够增加神经元的表达能力。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">DynamicShiftMax</span>(<span class="params">x, alpha=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(x, x + alpha * np.<span class="built_in">max</span>(x, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">DynamicReLU_A</span>(<span class="params">x, alpha=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(x, x + alpha * np.mean(x, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">DynamicReLU_B</span>(<span class="params">x, alpha=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(x, x + alpha * np.std(x, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/DSM%26DRLA%26DRLB.jpg" alt="DSM%26DRLA%26DRLB"></p><h2 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h2><p>我们采用控制变量法进行激活函数的推理速度测试，x 为输入，范围为-1 到 1 之间的十万个数据，运行次数为 100 计算激活函数的计算耗时。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    x = np.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">100000</span>)</span><br><span class="line">    t1 = time.time()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        y = sigmoid(x)</span><br><span class="line">    t2 = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">float</span>(t2 - t1))</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/FET.jpg" alt="FunctionExecutionTime"></p><p>完整代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.</span> / (<span class="number">1.</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ReLU</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">LeakyReLU</span>(<span class="params">x, alpha=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(alpha * x, x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Tanh</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.tanh(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Softmax</span>(<span class="params">x</span>):</span><br><span class="line">    exp_x = np.exp(x)</span><br><span class="line">    <span class="keyword">return</span> exp_x / np.<span class="built_in">sum</span>(exp_x, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">GELU</span>(<span class="params">x</span>):</span><br><span class="line">    cdf = <span class="number">0.5</span> * (<span class="number">1.0</span> + np.tanh((np.sqrt(<span class="number">2</span> / np.pi) * (x + <span class="number">0.044715</span> * np.power(x, <span class="number">3</span>)))))</span><br><span class="line">    <span class="keyword">return</span> x * cdf</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">SE_ReLU</span>(<span class="params">x, alpha=<span class="number">0.1</span>, beta=<span class="number">10</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.where(x &gt; <span class="number">0</span>, x + alpha * x * np.exp(-beta * x), x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">SE_ReLU</span>(<span class="params">x, alpha=<span class="number">0.1</span>, beta=<span class="number">10</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.where(x &gt; <span class="number">0</span>, x + alpha * x * np.exp(-beta * x), x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">DynamicShiftMax</span>(<span class="params">x, alpha=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(x, x + alpha * np.<span class="built_in">max</span>(x, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">DynamicReLU_A</span>(<span class="params">x, alpha=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(x, x + alpha * np.mean(x, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">DynamicReLU_B</span>(<span class="params">x, alpha=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(x, x + alpha * np.std(x, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">name = [sigmoid, ReLU, LeakyReLU, Tanh, Softmax, GELU, SE_ReLU, DynamicShiftMax, DynamicReLU_A, DynamicReLU_B]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    x = np.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1000000</span>)</span><br><span class="line">    times = []  <span class="comment"># 创建一个空列表来存储函数名称和时间</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> name:</span><br><span class="line">        t1 = time.perf_counter()  <span class="comment"># 使用perf_counter</span></span><br><span class="line">        y = n(x)</span><br><span class="line">        t2 = time.perf_counter()  <span class="comment"># 使用perf_counter</span></span><br><span class="line">        times.append((n.__name__, <span class="built_in">float</span>(t2 - t1)))  <span class="comment"># 将函数名称和时间作为元组添加到列表中</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n, t <span class="keyword">in</span> times:  <span class="comment"># 遍历列表并打印每个函数名称和时间</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;n&#125;</span>: <span class="subst">&#123;t&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用plot绘制times列表中的数据</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">plt.bar(*<span class="built_in">zip</span>(*times)) <span class="comment"># 使用zip(*times)将元组列表转换为两个元组列表</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Time (s)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Activation Functions&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 杂学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>anzhiyu添加新页面基于已有模板</title>
      <link href="/Markdown/anzhiyu%E6%B7%BB%E5%8A%A0%E6%96%B0%E9%A1%B5%E9%9D%A2%E5%9F%BA%E4%BA%8E%E5%B7%B2%E6%9C%89%E6%A8%A1%E6%9D%BF/"/>
      <url>/Markdown/anzhiyu%E6%B7%BB%E5%8A%A0%E6%96%B0%E9%A1%B5%E9%9D%A2%E5%9F%BA%E4%BA%8E%E5%B7%B2%E6%9C%89%E6%A8%A1%E6%9D%BF/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><code>anzhiyu</code>主题仅支持添加现有模板，例如<code>album</code>、<code>essay</code>等。如果您使用以下命令创建新页面：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page pageName</span><br></pre></td></tr></table></figure><p>由于<code>./themes/anzhiyu/layout/page.pug</code>中的以下代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">block content</span><br><span class="line">  #page</span><br><span class="line">    if top_img === false &amp;&amp; !page.top_single</span><br><span class="line">      h1.page-title= page.title</span><br><span class="line">    case page.type</span><br><span class="line">      when &#x27;tags&#x27;</span><br><span class="line">        include includes/page/tags.pug</span><br><span class="line">      when &#x27;link&#x27;</span><br><span class="line">        include includes/page/flink.pug</span><br><span class="line">      when &#x27;categories&#x27;</span><br><span class="line">        include includes/page/categories.pug</span><br><span class="line">      when &#x27;essay&#x27;</span><br><span class="line">        include includes/page/essay.pug</span><br><span class="line">      when &#x27;room&#x27;</span><br><span class="line">        include includes/page/room.pug</span><br><span class="line">      when &#x27;about&#x27;</span><br><span class="line">        include includes/page/about.pug</span><br><span class="line">      when &#x27;album&#x27;</span><br><span class="line">        include includes/page/album.pug</span><br><span class="line">      when &#x27;fcircle&#x27;</span><br><span class="line">        include includes/page/fcircle.pug</span><br><span class="line">      when &#x27;album_detail&#x27;</span><br><span class="line">        include includes/page/album_detail.pug</span><br><span class="line">      when &#x27;music&#x27;</span><br><span class="line">        include includes/page/music.pug</span><br><span class="line">      when &#x27;equipment&#x27;</span><br><span class="line">        include includes/page/equipment.pug</span><br><span class="line">      default</span><br><span class="line">        include includes/page/default-page.pug</span><br></pre></td></tr></table></figure><p>新页面将使用<code>default-page</code>的 JavaScript 和 CSS，这可能会导致外观不尽如人意（除非您有能力自行编写）。因此，我建议您基于现有模板进行修改。在这里，我们将使用模板库中的<code>equipment</code>页面作为示例进行自定义修改。</p><h4 id="具体方法如下："><a href="#具体方法如下：" class="headerlink" title="具体方法如下："></a>具体方法如下：</h4><ol><li><p>找到以下文件：</p><ul><li><code>themes\anzhiyu\layout\page.pug</code></li><li><code>themes\anzhiyu\layout\includes\page\equipment.pug</code></li><li><code>themes\anzhiyu\source\css\_page\equipment.styl</code></li></ul></li><li><p>在<code>page.pug</code>中添加以下代码：</p></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#page</span><br><span class="line">  if top_img === false &amp;&amp; !page.top_single</span><br><span class="line">    h1.page-title= page.title</span><br><span class="line">  case page.type</span><br><span class="line">    when &#x27;tags&#x27;</span><br><span class="line">      include includes/page/tags.pug</span><br><span class="line">    when &#x27;link&#x27;</span><br><span class="line">      include includes/page/flink.pug</span><br><span class="line">    when &#x27;categories&#x27;</span><br><span class="line">      include includes/page/categories.pug</span><br><span class="line">    when &#x27;essay&#x27;</span><br><span class="line">      include includes/page/essay.pug</span><br><span class="line">    when &#x27;room&#x27;</span><br><span class="line">      include includes/page/room.pug</span><br><span class="line">    when &#x27;about&#x27;</span><br><span class="line">      include includes/page/about.pug</span><br><span class="line">    when &#x27;album&#x27;</span><br><span class="line">      include includes/page/album.pug</span><br><span class="line">    when &#x27;fcircle&#x27;</span><br><span class="line">      include includes/page/fcircle.pug</span><br><span class="line">    when &#x27;album_detail&#x27;</span><br><span class="line">      include includes/page/album_detail.pug</span><br><span class="line">    when &#x27;music&#x27;</span><br><span class="line">      include includes/page/music.pug</span><br><span class="line">    when &#x27;equipment&#x27;</span><br><span class="line">      include includes/page/equipment.pug</span><br><span class="line">    when &#x27;pageName&#x27;  // 新加的代码</span><br><span class="line">      include includes/page/pageName.pug  // 新加的代码</span><br><span class="line">    default</span><br><span class="line">      include includes/page/default-page.pug</span><br></pre></td></tr></table></figure><ol><li><p>接着，对<code>equipment.pug</code>进行操作：</p><ul><li>新建一个文件<code>pageName.pug</code>。</li><li>复制<code>equipment.pug</code>中的所有代码。</li><li>将所有的<code>equipment</code>关键字替换为<code>pageName</code></li></ul></li><li><p>对<code>equipment.styl</code>重复上述步骤。</p></li></ol><p>完成以上步骤后，您就可以开始编辑新的<code>pageName.yml</code>文件了。</p><ol><li>直接把<code>equipment.yml</code>复制一份到新文件<code>pageName.yml</code>，并且把所有的<code>equipment</code>字样全部替换为<code>pageName</code>，就可以根据<code>equipment</code>的编辑模式对<code>pageName</code>页面进行编辑了。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 前端 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>页面配置</title>
      <link href="/Markdown/%E9%A1%B5%E9%9D%A2%E9%85%8D%E7%BD%AE/"/>
      <url>/Markdown/%E9%A1%B5%E9%9D%A2%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h3 id="🧱-Front-matter-的基本认识"><a href="#🧱-Front-matter-的基本认识" class="headerlink" title="🧱 Front-matter 的基本认识"></a>🧱 Front-matter 的基本认识</h3><p>Front-matter 是 markdown 文件最上方以 —- 分隔的区域，用于指定个别档案的变数。其中又分为两种 markdown 里</p><ul><li><strong>Page Front-matter</strong> 用于页面配置</li><li><strong>Post Front-matter</strong> 用于文章页配置</li></ul><h4 id="Page-Front-matter"><a href="#Page-Front-matter" class="headerlink" title="Page Front-matter"></a>Page Front-matter</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">title:</span> <span class="string">【必需】页面标题</span></span><br><span class="line"><span class="attr">date:</span> <span class="string">【必需】页面创建日期</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">【必需】标签、分类、关于、音乐馆、友情链接、相册、相册详情、朋友圈、即刻页面需要配置</span></span><br><span class="line"><span class="attr">updated:</span> <span class="string">【可选】页面更新日期</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">【可选】页面描述</span></span><br><span class="line"><span class="attr">keywords:</span> <span class="string">【可选】页面关键字</span></span><br><span class="line"><span class="attr">comments:</span> <span class="string">【可选】显示页面评论模块(默认</span> <span class="literal">true</span><span class="string">)</span></span><br><span class="line"><span class="attr">top_img:</span> <span class="string">【可选】页面顶部图片</span></span><br><span class="line"><span class="attr">mathjax:</span> <span class="string">【可选】显示</span> <span class="string">mathjax(当设置</span> <span class="string">mathjax</span> <span class="string">的</span> <span class="attr">per_page:</span> <span class="literal">false</span> <span class="string">时，才需要配置，默认</span> <span class="literal">false</span><span class="string">)</span></span><br><span class="line"><span class="attr">katex:</span> <span class="string">【可选】显示</span> <span class="string">katex(当设置</span> <span class="string">katex</span> <span class="string">的</span> <span class="attr">per_page:</span> <span class="literal">false</span> <span class="string">时，才需要配置，默认</span> <span class="literal">false</span><span class="string">)</span></span><br><span class="line"><span class="attr">aside:</span> <span class="string">【可选】显示侧边栏</span> <span class="string">(默认</span> <span class="literal">true</span><span class="string">)</span></span><br><span class="line"><span class="attr">aplayer:</span> <span class="string">【可选】在需要的页面加载</span> <span class="string">aplayer</span> <span class="string">的</span> <span class="string">js</span> <span class="string">和</span> <span class="string">css,请参考文章下面的音乐</span> <span class="string">配置</span></span><br><span class="line"><span class="attr">highlight_shrink:</span> <span class="string">【可选】配置代码框是否展开(true/false)(默认为设置中</span> <span class="string">highlight_shrink</span> <span class="string">的配置)</span></span><br><span class="line"><span class="attr">top_single_background:</span> <span class="string">【可选】部分页面的顶部模块背景图片</span></span><br></pre></td></tr></table></figure><h4 id="Post-Front-matter"><a href="#Post-Front-matter" class="headerlink" title="Post Front-matter"></a>Post Front-matter</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">title:</span> <span class="string">【必需】文章标题</span></span><br><span class="line"><span class="attr">date:</span> <span class="string">【必需】文章创建日期</span></span><br><span class="line"><span class="attr">updated:</span> <span class="string">【可选】文章更新日期</span></span><br><span class="line"><span class="attr">tags:</span> <span class="string">【可选】文章标签</span></span><br><span class="line"><span class="attr">categories:</span> <span class="string">【可选】文章分类</span></span><br><span class="line"><span class="attr">keywords:</span> <span class="string">【可选】文章关键字</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">【可选】文章描述</span></span><br><span class="line"><span class="attr">top_img:</span> <span class="string">【可选】文章顶部图片</span></span><br><span class="line"><span class="attr">cover:</span> <span class="string">【可选】文章缩略图(如果没有设置</span> <span class="string">top_img,文章页顶部将显示缩略图，可设为</span> <span class="literal">false</span><span class="string">/图片地址/留空)</span></span><br><span class="line"><span class="attr">comments:</span> <span class="string">【可选】显示文章评论模块(默认</span> <span class="literal">true</span><span class="string">)</span></span><br><span class="line"><span class="attr">toc:</span> <span class="string">【可选】显示文章</span> <span class="string">TOC(默认为设置中</span> <span class="string">toc</span> <span class="string">的</span> <span class="string">enable</span> <span class="string">配置)</span></span><br><span class="line"><span class="attr">toc_number:</span> <span class="string">【可选】显示</span> <span class="string">toc_number(默认为设置中</span> <span class="string">toc</span> <span class="string">的</span> <span class="string">number</span> <span class="string">配置)</span></span><br><span class="line"><span class="attr">toc_style_simple:</span> <span class="string">【可选】显示</span> <span class="string">toc</span> <span class="string">简洁模式</span></span><br><span class="line"><span class="attr">copyright:</span> <span class="string">【可选】显示文章版权模块(默认为设置中</span> <span class="string">post_copyright</span> <span class="string">的</span> <span class="string">enable</span> <span class="string">配置)</span></span><br><span class="line"><span class="attr">copyright_author:</span> <span class="string">【可选】文章版权模块的文章作者</span></span><br><span class="line"><span class="attr">copyright_author_href:</span> <span class="string">【可选】文章版权模块的文章作者链接</span></span><br><span class="line"><span class="attr">copyright_url:</span> <span class="string">【可选】文章版权模块的文章链接链接</span></span><br><span class="line"><span class="attr">copyright_info:</span> <span class="string">【可选】文章版权模块的版权声明文字</span></span><br><span class="line"><span class="attr">mathjax:</span> <span class="string">【可选】显示</span> <span class="string">mathjax(当设置</span> <span class="string">mathjax</span> <span class="string">的</span> <span class="attr">per_page:</span> <span class="literal">false</span> <span class="string">时，才需要配置，默认</span> <span class="literal">false</span><span class="string">)</span></span><br><span class="line"><span class="attr">katex:</span> <span class="string">【可选】显示</span> <span class="string">katex(当设置</span> <span class="string">katex</span> <span class="string">的</span> <span class="attr">per_page:</span> <span class="literal">false</span> <span class="string">时，才需要配置，默认</span> <span class="literal">false</span><span class="string">)</span></span><br><span class="line"><span class="attr">aplayer:</span> <span class="string">【可选】在需要的页面加载</span> <span class="string">aplayer</span> <span class="string">的</span> <span class="string">js</span> <span class="string">和</span> <span class="string">css,请参考文章下面的音乐</span> <span class="string">配置</span></span><br><span class="line"><span class="attr">highlight_shrink:</span> <span class="string">【可选】配置代码框是否展开(true/false)(默认为设置中</span> <span class="string">highlight_shrink</span> <span class="string">的配置)</span></span><br><span class="line"><span class="attr">aside:</span> <span class="string">【可选】显示侧边栏</span> <span class="string">(默认</span> <span class="literal">true</span><span class="string">)</span></span><br><span class="line"><span class="attr">swiper_index:</span> <span class="string">【可选】首页轮播图配置</span> <span class="string">index</span> <span class="string">索引，数字越小越靠前</span></span><br><span class="line"><span class="attr">top_group_index:</span> <span class="string">【可选】首页右侧卡片组配置,</span> <span class="string">数字越小越靠前</span></span><br><span class="line"><span class="attr">ai:</span> <span class="string">【可选】文章ai摘要</span></span><br><span class="line"><span class="attr">main_color:</span> <span class="string">【可选】文章主色，必须是16进制颜色且有6位，不可缩减，例如#ffffff</span> <span class="string">不可写成#fff</span></span><br></pre></td></tr></table></figure><p><strong>使用方法</strong>：</p><ol><li>在 Markdown 文件的最上方添加 Front-matter 区域，以<code>---</code>分隔。</li><li>根据需要配置 Page Front-matter 或 Post Front-matter 中的参数。</li><li>可选参数可以根据个人需求添加，不必全部包含。</li><li>特定页面的 Front-matter 配置（如 swiper_index 和 top_group_index）可以实现轮播图和推荐卡片的显示。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 前端 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
