<!DOCTYPE html><html lang="en" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>LTPG66 | LTPG66</title><meta name="keywords" content="None"><meta name="author" content="little_penguin66"><meta name="copyright" content="little_penguin66"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#18171d"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="LTPG66"><meta name="application-name" content="LTPG66"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#18171d"><meta property="og:type" content="article"><meta property="og:title" content="LTPG66"><meta property="og:url" content="http://example.com/uncategorized/AAAMLprob/%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F/index.html"><meta property="og:site_name" content="LTPG66"><meta property="og:description" content="处理分类变量很多人在处理分类变量时都会遇到很多困难，因此这值得用整整一章的篇幅来讨论。在本章中，我将讲述不同类型的分类数据，以及如何处理分类变量问题。 什么是分类变量？ 分类变量&amp;#x2F;特征是指任何特征类型，可分为两大类：  无序 有序  无序变量是指有两个或两个以上类别的变量，这些类别没有任何相关顺序。"><meta property="og:locale" content="en"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/mcpfp%20-%20little_penguin66.png"><meta property="article:author" content="little_penguin66"><meta property="article:tag" content="None"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/mcpfp%20-%20little_penguin66.png"><meta name="description" content="处理分类变量很多人在处理分类变量时都会遇到很多困难，因此这值得用整整一章的篇幅来讨论。在本章中，我将讲述不同类型的分类数据，以及如何处理分类变量问题。 什么是分类变量？ 分类变量&amp;#x2F;特征是指任何特征类型，可分为两大类：  无序 有序  无序变量是指有两个或两个以上类别的变量，这些类别没有任何相关顺序。"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/favicon.ico"><link rel="canonical" href="http://example.com/uncategorized/AAAMLprob/%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/swiper/swiper.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2023/09/03/125766904/ee23df8517f3c3e3efc4145658269c06_5714860933110284659.png"},
  postHeadAiDescription: {"enable":true,"gptName":"ChatGLM","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: undefined,
  LA51: undefined,
  greetingBox: {"enable":true,"default":"晚上好👋","list":[{"greeting":"晚安😴","startTime":0,"endTime":5},{"greeting":"早上好鸭👋, 祝你一天好心情！","startTime":6,"endTime":9},{"greeting":"上午好👋, 状态很好，鼓励一下～","startTime":10,"endTime":10},{"greeting":"11点多啦, 在坚持一下就吃饭啦～","startTime":11,"endTime":11},{"greeting":"午安👋","startTime":12,"endTime":14},{"greeting":"🌈充实的一天辛苦啦！","startTime":14,"endTime":18},{"greeting":"19点喽, 奖励一顿丰盛的大餐吧🍔。","startTime":19,"endTime":19},{"greeting":"晚上好👋, 在属于自己的时间好好放松😌~","startTime":20,"endTime":24}]},
  twikooEnvId: 'https://jrz66-comment.hf.space',
  commentBarrageConfig:{"enable":true,"maxBarrage":1,"barrageTime":4000,"accessToken":"","mailMd5":""},
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: undefined,
  authorStatus: {"skills":["🤖️ 数码科技爱好者","🔍 分享与热心帮助","🏠 智能家居小能手","🔨 设计开发一条龙","🤝 专修交互与设计","🏃 脚踏实地行动派","🧱 团队小组发动机","💢 壮汉人狠话不多"]},
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"Author: little_penguin66","link":"Link: ","source":"Source: LTPG66","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.","copySuccess":"Copy success, copy and reprint please mark the address of this article"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: {"enable":true,"delay":100,"shiftDelay":200},
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'LTPG66',
  title: 'LTPG66',
  postAI: '',
  pageFillDescription: '处理分类变量处理分类变量很多人在处理分类变量时都会遇到很多困难因此这值得用整整一章的篇幅来讨论在本章中我将讲述不同类型的分类数据以及如何处理分类变量问题什么是分类变量分类变量特征是指任何特征类型可分为两大类无序有序无序变量是指有两个或两个以上类别的变量这些类别没有任何相关顺序例如如果将性别分为两组即男性和女性则可将其视为名义变量有序变量则有等级或类别并有特定的顺序例如一个顺序分类变量可以是一个具有低中高三个不同等级的特征顺序很重要就定义而言我们也可以将分类变量分为二元变量即只有两个类别的分类变量有些人甚至把分类变量称为循环变量周期变量以周期的形式存在例如一周中的天数周日周一周二周三周四周五和周六周六过后又是周日这就是一个循环另一个例子是一天中的小时数如果我们将它们视为类别的话分类变量有很多不同的定义很多人也谈到要根据分类变量的类型来处理不同的分类变量不过我认为没有必要这样做所有涉及分类变量的问题都可以用同样的方法处理开始之前我们需要一个数据集一如既往要了解分类变量最好的免费数据集之一是分类特征编码挑战赛中的共有两个挑战我们将使用第二个挑战的数据因为它比前一个版本有更多变量难度也更大让我们来看看数据图部分数据展示数据集由各种分类变量组成无序有序循环二元在图中我们只看到所有存在的变量和目标变量的子集这是一个二元分类问题目标变量对于我们学习分类变量来说并不十分重要但最终我们将建立一个端到端模型因此让我们看看图中的目标变量分布我们看到目标是偏斜的因此对于这个二元分类问题来说最好的指标是曲线下面积我们也可以使用精确度和召回率但结合了这两个指标因此我们将使用来评估我们在该数据集上建立的模型图标签计数轴表示标签轴表示标签计数总体而言有个二元变量个无序变量个有序变量个循环变量个目标变量让我们来看看数据集中的特征它包括个不同的类别冰冻温暖寒冷较热热非常热我们必须知道计算机无法理解文本数据因此我们需要将这些类别转换为数字一个简单的方法是创建一个字典将这些值映射为从到的数字其中是给定特征中类别的总数映射字典现在我们可以读取数据集并轻松地将这些类别转换为数字读取数据取列并使用映射将类别转换为数字映射前的数值计数映射后的数值计数这种分类变量的编码方式被称为标签编码我们将每个类别编码为一个数字标签我们也可以使用中的进行编码读取数据将缺失值填充为编码转换数据你会看到我使用了的原因是的无法处理值而列中有值我们可以在许多基于树的模型中直接使用它决策树随机森林提升树或任何一种提升树模型这种编码方式不能用于线性模型支持向量机或神经网络因为它们希望数据是标准化的对于这些类型的模型我们可以对数据进行二值化处理这只是将类别转换为数字然后再转换为二值化表示这样我们就把一个特征分成了三个在本例中特征或列如果我们有更多的类别最终可能会分成更多的列如果我们用稀疏格式存储大量二值化变量就可以轻松地存储这些变量稀疏格式不过是一种在内存中存储数据的表示或方式在这种格式中你并不存储所有的值而只存储重要的值在上述二进制变量的情况中最重要的就是有的地方很难想象这样的格式但举个例子就会明白假设上面的数据帧中只有一个特征目前我们只看到数据集中的三个样本让我们将其转换为二值表示法即每个样本有三个项目这三个项目就是三个特征因此我们的特征存储在一个有行列的矩阵中矩阵的每个元素占用个字节因此这个数组的总内存需求为字节我们还可以使用一个简单的代码段来检查这一点这段代码将打印出就像我们之前计算的那样但我们需要存储这个矩阵的所有元素吗如前所述我们只对感兴趣并不重要因为任何与相乘的元素都是而与任何元素相加或相减也没有任何区别只用表示矩阵的一种方法是某种字典方法其中键是行和列的索引值是这样的符号占用的内存要少得多因为它只需存储四个值在本例中使用的总内存为字节任何数组都可以通过简单的代码转换为稀疏矩阵这将打印比我们的密集数组少了这么多稀疏矩阵的总大小是三个值的总和这将打印出个元素仍然少于我们的密集数组遗憾的是我不会详细介绍这些元素你可以在文档中了解更多当我们拥有更大的数组时比如说拥有数千个样本和数万个特征的数组大小差异就会变得非常大例如我们使用基于计数特征的文本数据集生成符合伯努利分布的随机数组维度为将随机矩阵转换为洗漱矩阵这将打印因此密集阵列需要或大约内存而稀疏阵列只占用内存这就是为什么当我们的特征中有大量零时我们更喜欢稀疏阵列而不是密集阵列的原因请注意稀疏矩阵有多种不同的表示方法这里我只展示了其中一种可能也是最常用的方法深入探讨这些方法超出了本书的范围因此留给读者一个练习尽管二值化特征的稀疏表示比其密集表示所占用的内存要少得多但对于分类变量来说还有一种转换所占用的内存更少这就是所谓的独热编码独热编码也是一种二值编码因为只有和两个值但必须注意的是它并不是二值表示法我们可以通过下面的例子来理解它的表示法假设我们用一个向量来表示变量的每个类别这个向量的大小与变量的类别数相同在这种特定情况下每个向量的大小都是并且除了一个位置外其他位置都是让我们来看看这个特殊的向量表我们看到向量的大小是即向量中有个元素这个数字是怎么来的呢如果你仔细观察就会发现如前所述有个类别在进行独热编码时向量的大小必须与我们要查看的类别数相同每个向量都有一个其余所有值都是现在让我们用这些特征来代替之前的二值化特征看看能节省多少内存如果你还记得以前的数据它看起来如下每个样本有个特征但在这种情况下独热向量的大小为因此我们有个特征而不是个因此我们有个特征而在这个数组中只有个使用计算大小与二值化大小计算脚本非常相似你需要改变的只是数组让我们看看这段代码打印内存大小为我们可以看到密集矩阵的大小远远大于二值化矩阵的大小不过稀疏数组的大小要更小让我们用更大的数组来试试在本例中我们将使用中的将包含个类别的特征数组转换为密集矩阵和稀疏矩阵生成符合均匀分布的随机整数维度为独热编码非稀疏矩阵将随机数组展平独热编码稀疏矩阵将随机数组展平上面代码打印的输出这里的密集阵列大小约为稀疏阵列为如果可以选择你会选择哪个在我看来选择很简单不是吗这三种方法标签编码稀疏矩阵独热编码是处理分类变量的最重要方法不过你还可以用很多其他不同的方法来处理分类变量将分类变量转换为数值变量就是其中的一个例子假设我们回到之前的分类特征数据原始数据中的在数据中的值为热的有多少我们可以通过计算数据的形状轻松计算出这个值其中列的值为我们可以看到有条记录具有此值我们还可以使用中的计算所有类别的该值如果我们只是将列替换为其计数值那么我们就将其转换为一种数值特征了我们可以使用的函数和来创建新列或替换这一列你可以添加所有特征的计数也可以替换它们或者根据多个列及其计数进行分组例如以下代码通过对和列分组进行计数请注意我已经从输出中删除了一些行以便在一页中容纳这些行这是另一种可以作为功能添加的计数您现在一定已经注意到我使用列进行计数不过你也可以通过对列的组合进行分组对其他列进行计数还有一个小窍门就是从这些分类变量中创建新特征你可以从现有的特征中创建新的分类特征而且可以毫不费力地做到这一点在这里我们用下划线将和合并然后将这些列转换为字符串类型请注意也会转换为字符串不过没关系我们也可以将视为一个新的类别这样我们就有了一个由这两个特征组合而成的新特征您还可以将三列以上或四列甚至更多列组合在一起那么我们应该把哪些类别结合起来呢这并没有一个简单的答案这取决于您的数据和特征类型一些领域知识对于创建这样的特征可能很有用但是如果你不担心内存和的使用你可以采用一种贪婪的方法即创建许多这样的组合然后使用一个模型来决定哪些特征是有用的并保留它们我们将在本书稍后部分介绍这种方法无论何时获得分类变量都要遵循以下简单步骤填充值这一点非常重要使用的或映射字典进行标签编码将它们转换为整数如果没有填充值可能需要在这一步中进行处理创建独热编码是的你可以跳过二值化建模我指的是机器学习在分类特征中处理数据非常重要否则您可能会从的中得到臭名昭著的错误信息包含以前未见过的标签这仅仅意味着在转换测试数据时数据中出现了值这是因为你在训练时忘记了处理它们处理值的一个简单方法就是丢弃它们虽然简单但并不理想值中可能包含很多信息如果只是丢弃这些值就会丢失这些信息在很多情况下大部分数据都是值因此不能丢弃值的行样本处理值的另一种方法是将其作为一个全新的类别这是处理值最常用的方法如果使用还可以通过非常简单的方式实现请看我们之前查看过的数据的列填入值后就变成了哇这一列中有个值而我们之前甚至都没有考虑使用它们增加了这个新类别后类别总数从个增加到了个这没关系因为现在我们在建立模型时也会考虑相关信息越多模型就越好假设没有任何值我们可以看到这一列中的所有类别都有显著的计数其中没有罕见类别即只在样本总数中占很小比例的类别现在让我们假设您在生产中部署了使用这一列的模型当模型或项目上线时您在列中得到了一个在训练中不存在的类别在这种情况下模型管道会抛出一个错误您对此无能为力如果出现这种情况那么可能是生产中的管道出了问题如果这是预料之中的那么您就必须修改您的模型管道并在这六个类别中加入一个新类别这个新类别被称为罕见类别罕见类别是一种不常见的类别可以包括许多不同的类别您也可以尝试使用近邻模型来预测未知类别请记住如果您预测了这个类别它就会成为训练数据中的一个类别图具有不同特征且无标签的数据集示意图其中一个特征可能会在测试集或实时数据中出现新值当我们有一个如图所示的数据集时我们可以建立一个简单的模型对除之外的所有特征进行训练这样你将创建一个模型在不知道或训练中没有时预测它我不敢说这样的模型是否能带来出色的性能但也许能处理测试集或实时数据中的缺失值就像机器学习中的其他事情一样不尝试一下是说不准的如果你有一个固定的测试集你可以将测试数据添加到训练中以了解给定特征中的类别这与半监督学习非常相似即使用无法用于训练的数据来改进模型这也会照顾到在训练数据中出现次数极少但在测试数据中大量存在的稀有值你的模型将更加稳健很多人认为这种想法会过度拟合可能过拟合也可能不过拟合有一个简单的解决方法如果你在设计交叉验证时能够在测试数据上运行模型时复制预测过程那么它就永远不会过拟合这意味着第一步应该是分离折叠在每个折叠中你应该应用与测试数据相同的预处理假设您想合并训练数据和测试数据那么在每个折叠中您必须合并训练数据和验证数据并确保验证数据集复制了测试集在这种特定情况下您必须以这样一种方式设计验证集使其包含训练集中未见的类别图对训练集和测试集进行简单合并以了解测试集中存在但训练集中不存在的类别或训练集中罕见的类别只要看一下图和下面的代码就能很容易理解其工作原理读取训练集读取测试集将测试集列全部置为将训练集测试集沿行拼接将除和列的其他特征列名取出遍历特征标签编码将空值替换为并将该列格式变为转换数值根据列将训练集与测试集分开当您遇到已经有测试数据集的问题时这个技巧就会起作用必须注意的是这一招在实时环境中不起作用例如假设您所在的公司提供实时竞价解决方案系统会对在线看到的每个用户进行竞价以购买广告空间这种模式可使用的功能可能包括网站中浏览的页面我们假设这些特征是用户访问的最后五个类别页面在这种情况下如果网站引入了新的类别我们将无法再准确预测在这种情况下我们的模型就会失效这种情况可以通过使用未知类别来避免在我们的数据集中列中已经有了未知类别我们可以将视为未知因此如果在实时测试过程中我们获得了以前从未见过的新类别我们就会将其标记为这与自然语言处理问题非常相似我们总是基于固定的词汇建立模型增加词汇量就会增加模型的大小像这样的转换器模型是在个单词英语的基础上训练的因此当有新词输入时我们会将其标记为未知因此您可以假设测试数据与训练数据具有相同的类别也可以在训练数据中引入罕见或未知类别以处理测试数据中的新类别让我们看看填入值后列的值计数我们看到有些数值只出现了几千次有些则出现了近次也经常出现请注意我已经从输出中删除了一些值现在我们可以定义将一个值称为罕见的标准了比方说在这一列中稀有值的要求是计数小于这样看来和就可以被标记为稀有值了使用根据计数阈值替换类别非常简单让我们来看看它是如何实现的我们认为只要某个类别的值小于就将其替换为罕见因此现在在测试数据时所有未见过的新类别都将被映射为而所有缺失值都将被映射为这种方法还能确保即使有新的类别模型也能在实际环境中正常工作现在我们已经具备了处理任何带有分类变量问题所需的一切条件让我们尝试建立第一个模型并逐步提高其性能在构建任何类型的模型之前交叉检验至关重要我们已经看到了标签目标分布知道这是一个目标偏斜的二元分类问题因此我们将使用来分割数据读取数据文件添加列并置为打乱数据顺序重置索引将目标列取出分层折交叉检验区分折叠保存文件现在我们可以检查新的折叠查看每个折叠的样本数所有折叠都有个样本这是意料之中的因为训练数据有个样本而我们做了次折叠到目前为止一切顺利现在我们还可以检查每个折叠的目标分布我们看到在每个折叠中目标的分布都是一样的这正是我们所需要的它也可以是相似的并不一定要一直相同现在当我们建立模型时每个折叠中的标签分布都将相同我们可以建立的最简单的模型之一是对所有数据进行独热编码并使用逻辑回归读取分层折交叉检验数据取除外的其他特征列遍历特征列表将空值置为取训练集列中不为的样本重置索引取验证集列中为的样本重置索引独热编码将训练集验证集沿行合并转换训练集转换测试集逻辑回归使用训练集训练模型使用验证集得到预测标签计算指标运行折叠那么发生了什么呢我们创建了一个函数将数据分为训练和验证两部分给定折叠数处理值对所有数据进行单次编码并训练一个简单的逻辑回归模型当我们运行这部分代码时会产生如下输出有一些警告逻辑回归似乎没有收敛到最大迭代次数我们没有调整参数所以没有问题我们看到为现在让我们对代码进行简单修改运行所有折叠循环运行折请注意我们并没有做很大的改动所以我只显示了部分代码行其中一些代码行有改动这就打印出了请注意我使用忽略了所有警告我们看到分数在所有褶皱中都相当稳定平均为对于我们的第一个模型来说相当不错很多人在遇到这种问题时会首先使用基于树的模型比如随机森林在这个数据集中应用随机森林时我们可以使用标签编码将每一列中的每个特征都转换为整数而不是之前讨论过的独热编码这种编码与独热编码并无太大区别让我们来看看标签编码随机森林模型我们使用中的随机森林并取消了独热编码我们使用标签编码代替独热编码得分如下哇巨大的差异随机森林模型在没有任何超参数调整的情况下表现要比简单的逻辑回归差很多这就是为什么我们总是应该先从简单模型开始的原因随机森林模型的粉丝会从这里开始而忽略逻辑回归模型认为这是一个非常简单的模型不能带来比随机森林更好的价值这种人将会犯下大错在我们实现随机森林的过程中与逻辑回归相比折叠需要更长的时间才能完成因此我们不仅损失了还需要更长的时间来完成训练请注意使用随机森林进行推理也很耗时而且占用的空间也更大如果我们愿意也可以尝试在稀疏的独热编码数据上运行随机森林但这会耗费大量时间我们还可以尝试使用奇异值分解来减少稀疏的独热编码矩阵这是自然语言处理中提取主题的常用方法独热编码奇异值分解我们对全部数据进行独热编码然后用训练数据和验证数据在稀疏矩阵上拟合的这样我们将高维稀疏矩阵减少到个特征然后拟合随机森林分类器以下是该模型的输出结果我们发现情况更糟看来解决这个问题的最佳方法是使用逻辑回归和独热编码随机森林似乎耗时太多也许我们可以试试如果你不知道它是最流行的梯度提升算法之一由于它是一种基于树的算法我们将使用标签编码数据标签编码模型必须指出的是在这段代码中我对参数做了一些修改的默认最大深度是我把它改成了还把估计器数量从改成了该模型的折交叉检验得分如下我们可以看到在不做任何调整的情况下我们的得分比普通随机森林要高得多您还可以尝试一些特征工程放弃某些对模型没有任何价值的列等但似乎我们能做的不多无法证明模型的改进让我们把数据集换成另一个有大量分类变量的数据集另一个有名的数据集是美国成人人口普查数据这个数据集包含一些特征而你的任务是预测工资等级让我们来看看这个数据集图显示了该数据集中的一些列图部分数据集展示该数据集有以下几列年龄工作类别学历教育程度教育程度婚姻状况职业关系种族性别资本收益资本损失每周小时数原籍国收入这些特征大多不言自明那些不明白的我们可以不考虑让我们先尝试建立一个模型我们看到收入列是一个字符串让我们对这一列进行数值统计我们可以看到有个实例的收入超过万美元这占样本总数的因此我们将保持与猫数据集相同的评估方法即在开始建模之前为了简单起见我们将去掉几列特征即学历年龄资本收益资本损失每周小时数让我们试着用逻辑回归和独热编码器看看会发生什么第一步总是要进行交叉验证我不会在这里展示这部分代码留待读者练习需要删除的列映射使用映射替换取除列的其他列名将空值替换为取训练集列中不为的样本重置索引取验证集列中为的样本重置索引独热编码将训练集测试集沿行合并转换训练集转换验证集构建逻辑回归模型使用训练集训练模型使用验证集得到预测标签计算指标运行折当我们运行这段代码时我们会得到对于一个如此简单的模型来说这是一个非常不错的现在让我们在不调整任何超参数的情况下尝试一下标签编码的标签编码模型运行折让我们运行上面代码这看起来已经相当不错了让我们看看增加到和增加到时的得分看起来并没有改善这表明一个数据集的参数不能移植到另一个数据集我们必须再次尝试调整参数但我们将在接下来的章节中详细说明现在让我们尝试在不调整参数的情况下将数值特征纳入模型加入数值特征将空值置为标签编码模型因此我们保留数字列只是不对其进行标签编码这样我们的最终特征矩阵就由数字列原样和编码分类列组成了任何基于树的算法都能轻松处理这种混合请注意在使用基于树的模型时我们不需要对数据进行归一化处理不过这一点非常重要在使用线性模型如逻辑回归时不容忽视现在让我们运行这个脚本哇哦这是一个很好的分数现在我们可以尝试添加一些功能我们将提取所有分类列并创建所有二度组合请看下面代码段中的函数了解如何实现这一点生成两个特征的组合特征工程这是从分类列中创建特征的一种非常幼稚的方法我们应该仔细研究数据看看哪些组合最合理如果使用这种方法最终可能会创建大量特征在这种情况下就需要使用某种特征选择来选出最佳特征稍后我们将详细介绍特征选择现在让我们来看看分数看来即使不改变任何超参数只增加一些特征我们也能提高一些折叠得分让我们看看将增加到是否有帮助我们再次改进了我们的模型请注意我们还没有使用稀有值二值化独热编码和标签编码特征的组合以及其他几种方法从分类特征中进行特征工程的另一种方法是使用目标编码但是您必须非常小心因为这可能会使您的模型过度拟合目标编码是一种将给定特征中的每个类别映射到其平均目标值的技术但必须始终以交叉验证的方式进行这意味着首先要创建折叠然后使用这些折叠为数据的不同列创建目标编码特征方法与在折叠上拟合和预测模型的方法相同因此如果您创建了个折叠您就必须创建次目标编码这样最终您就可以为每个折叠中的变量创建编码而这些变量并非来自同一个折叠然后在拟合模型时必须再次使用相同的折叠未见测试数据的目标编码可以来自全部训练数据也可以是所有个折叠的平均值让我们看看如何在同一个成人数据集上使用目标编码以便进行比较标签编码目标编码必须指出的是在上述片段中我在进行目标编码时并没有删除分类列我保留了所有特征并在此基础上添加了目标编码特征此外我还使用了平均值您可以使用平均值中位数标准偏差或目标的任何其他函数让我们看看结果不错看来我们又有进步了不过使用目标编码时必须非常小心因为它太容易出现过度拟合当我们使用目标编码时最好使用某种平滑方法或在编码值中添加噪声的贡献库中有带平滑的目标编码你也可以创建自己的平滑平滑会引入某种正则化有助于避免模型过度拟合这并不难处理分类特征是一项复杂的任务许多资源中都有大量信息本章应该能帮助你开始解决分类变量的任何问题不过对于大多数问题来说除了独热编码和标签编码之外你不需要更多的东西要进一步改进模型你可能需要更多在本章的最后我们不能不在这些数据上使用神经网络因此让我们来看看一种称为实体嵌入的技术在实体嵌入中类别用向量表示在二值化和独热编码方法中我们都是用向量来表示类别的但是如果我们有数以万计的类别怎么办这将会产生巨大的矩阵我们将需要很长时间来训练复杂的模型因此我们可以用带有浮点值的向量来表示它们这个想法非常简单每个分类特征都有一个嵌入层因此一列中的每个类别现在都可以映射到一个嵌入层就像在自然语言处理中将单词映射到嵌入层一样然后根据其维度重塑这些嵌入层使其扁平化然后将所有扁平化的输入嵌入层连接起来然后添加一堆密集层和一个输出层就大功告成了图类别转换为浮点或嵌入向量出于某种原因我发现使用可以非常容易地做到这一点因此让我们来看看如何使用实现它此外这是本书中唯一一个使用的示例将其转换为使用数据集也非常容易创建空的输入列表和输出列表用于存储模型的输入和输出遍历分类特征列表中的每个特征计算特征中唯一值的数量计算嵌入维度最大不超过创建模型的输入层每个特征对应一个输入创建嵌入层将分类特征映射到低维度的连续向量对嵌入层进行空间丢弃将嵌入层的形状重新调整为一维将输入和输出添加到对应的列表中使用层将所有的嵌入层输出连接在一起对连接后的数据进行批量归一化添加一个具有个神经元的密集层并使用激活函数对该层的输出进行再次进行批量归一化添加另一个具有个神经元的密集层并使用激活函数对该层的输出进行再次进行批量归一化输出层具有个神经元用于二进制分类并使用激活函数创建模型将输入和输出传递给构造函数编译模型指定损失函数和优化器返回创建的模型你会发现这种方法效果最好而且如果你有速度也超快这种方法还可以进一步改进而且你无需担心特征工程因为神经网络会自行处理在处理大量分类特征数据集时这绝对值得一试当嵌入大小与唯一类别的数量相同时我们就可以使用独热编码本章基本上都是关于特征工程的让我们在下一章中看看如何在数字特征和不同类型特征的组合方面进行更多的特征工程',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-05-05 23:41:50',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/mcpfp%20-%20little_penguin66.png"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><div class="back-home-button"><i class="anzhiyufont anzhiyu-icon-grip-vertical"></i><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://littlepenguin66.github.io/" title="博客"><img class="back-menu-item-icon" src="/img/favicon.png" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://github.com/littlepenguin66" title="Github"><img class="back-menu-item-icon" src="anzhiyu-icon-github" alt="Github"/><span class="back-menu-item-text">Github</span></a></div></div></div></div><a id="site-name" href="/" accesskey="h"><div class="title">LTPG66</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 留言</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size: 0.9em;"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size: 0.9em;"></i><span> 闲言碎语</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size: 0.9em;"></i><span> 随便逛逛</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="搜索🔍" accesskey="s"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span> Search</span></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/WechatPay.jpg" target="_blank"><img class="post-qr-code-img" alt="微信" src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/WechatPay.jpg"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AliPay.jpg" target="_blank"><img class="post-qr-code-img" alt="支付宝" src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AliPay.jpg"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> Newest Comments</span></div><div class="aside-list"><span>loading...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/Python/" style="font-size: 1.05rem;">Python<sup>1</sup></a><a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 1.05rem;">前端<sup>2</sup></a><a href="/tags/%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86%E6%96%B9%E7%A8%8B/" style="font-size: 1.05rem;">数学物理方程<sup>1</sup></a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">机器学习<sup>1</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>Archives</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/05/"><span class="card-archive-list-date">May 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">12</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/04/"><span class="card-archive-list-date">April 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item on" id="consoleCommentBarrage" onclick="anzhiyu.switchCommentBarrage()" title="热评开关"><a class="commentBarrage"><i class="anzhiyufont anzhiyu-icon-message"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div><div class="console-btn-item" id="consoleKeyboard" onclick="anzhiyu.keyboardToggle()" title="快捷键开关"><a class="keyboard-switch"><i class="anzhiyufont anzhiyu-icon-keyboard"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="article-meta tags"></span></div></div><h1 class="post-title" itemprop="name headline">No title</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2024-05-05T15:43:25.108Z" title="Created 2024-05-05 23:43:25">2024-05-05</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2024-05-05T15:41:50.675Z" title="Updated 2024-05-05 23:41:50">2024-05-05</time></span></div><div class="meta-secondline"><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为北京"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>北京</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src=""></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="http://example.com/uncategorized/AAAMLprob/%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F/"><header><h1 id="CrawlerTitle" itemprop="name headline">No title</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">little_penguin66</span><time itemprop="dateCreated datePublished" datetime="2024-05-05T15:43:25.108Z" title="Created 2024-05-05 23:43:25">2024-05-05</time><time itemprop="dateCreated datePublished" datetime="2024-05-05T15:41:50.675Z" title="Updated 2024-05-05 23:41:50">2024-05-05</time></header><h1 id="处理分类变量"><a href="#处理分类变量" class="headerlink" title="处理分类变量"></a>处理分类变量</h1><p>很多人在处理分类变量时都会遇到很多困难，因此这值得用整整一章的篇幅来讨论。在本章中，我将讲述不同类型的分类数据，以及如何处理分类变量问题。</p>
<p><strong>什么是分类变量？</strong></p>
<p>分类变量/特征是指任何特征类型，可分为两大类：</p>
<ul>
<li>无序</li>
<li>有序</li>
</ul>
<p><strong>无序变量</strong>是指有两个或两个以上类别的变量，这些类别没有任何相关顺序。例如，如果将性别分为两组，即男性和女性，则可将其视为名义变量。</p>
<p><strong>有序变量</strong>则有 “等级 “或类别，并有特定的顺序。例如，一个顺序分类变量可以是一个具有低、中、高三个不同等级的特征。顺序很重要。</p>
<p>就定义而言，我们也可以将分类变量分为<strong>二元变量</strong>，即只有两个类别的分类变量。有些人甚至把分类变量称为 “<strong>循环</strong> “变量。周期变量以 “周期 “的形式存在，例如一周中的天数： 周日、周一、周二、周三、周四、周五和周六。周六过后，又是周日。这就是一个循环。另一个例子是一天中的小时数，如果我们将它们视为类别的话。</p>
<p>分类变量有很多不同的定义，很多人也谈到要根据分类变量的类型来处理不同的分类变量。不过，我认为没有必要这样做。所有涉及分类变量的问题都可以用同样的方法处理。</p>
<p>开始之前，我们需要一个数据集（一如既往）。要了解分类变量，最好的免费数据集之一是 Kaggle 分类特征编码挑战赛中的 <em>cat-in-the-dat</em>。共有两个挑战，我们将使用第二个挑战的数据，因为它比前一个版本有更多变量，难度也更大。</p>
<p>让我们来看看数据。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page85_image.png" alt=""></p>
<p align="center"><b>图 1：Cat-in-the-dat-ii challenge部分数据展示</b> </p>

<p>数据集由各种分类变量组成：</p>
<ul>
<li>无序</li>
<li>有序</li>
<li>循环</li>
<li>二元</li>
</ul>
<p>在图 1 中，我们只看到所有存在的变量和目标变量的子集。</p>
<p>这是一个二元分类问题。</p>
<p>目标变量对于我们学习分类变量来说并不十分重要，但最终我们将建立一个端到端模型，因此让我们看看图 2 中的目标变量分布。我们看到目标是<strong>偏斜</strong>的，因此对于这个二元分类问题来说，最好的指标是 ROC 曲线下面积（AUC）。我们也可以使用精确度和召回率，但 AUC 结合了这两个指标。因此，我们将使用 AUC 来评估我们在该数据集上建立的模型。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page86_image.png" alt=""></p>
<p align="center"><b>图 2：标签计数。X 轴表示标签，Y 轴表示标签计数</b> </p>

<p>总体而言，有：</p>
<ul>
<li>5 个二元变量</li>
<li>10 个无序变量</li>
<li>6 个有序变量</li>
<li>2 个循环变量</li>
<li>1 个目标变量</li>
</ul>
<p>让我们来看看数据集中的 <strong>ord_2</strong> 特征。它包括 6 个不同的类别：</p>
<ul>
<li>冰冻</li>
<li>温暖</li>
<li>寒冷</li>
<li>较热</li>
<li>热</li>
<li>非常热</li>
</ul>
<p>我们必须知道，计算机无法理解文本数据，因此我们需要将这些类别转换为数字。一个简单的方法是创建一个字典，将这些值映射为从 0 到 N-1 的数字，其中 N 是给定特征中类别的总数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 映射字典</span></span><br><span class="line">mapping = &#123;</span><br><span class="line"><span class="string">&quot;Freezing&quot;</span>: <span class="number">0</span>,</span><br><span class="line"><span class="string">&quot;Warm&quot;</span>: <span class="number">1</span>,</span><br><span class="line"><span class="string">&quot;Cold&quot;</span>: <span class="number">2</span>,</span><br><span class="line"><span class="string">&quot;Boiling Hot&quot;</span>: <span class="number">3</span>,</span><br><span class="line"><span class="string">&quot;Hot&quot;</span>: <span class="number">4</span>,</span><br><span class="line"><span class="string">&quot;Lava Hot&quot;</span>: <span class="number">5</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>现在，我们可以读取数据集，并轻松地将这些类别转换为数字。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">df = pd.read_csv(<span class="string">&quot;../input/cat_train.csv&quot;</span>)</span><br><span class="line"><span class="comment"># 取*ord_2*列，并使用映射将类别转换为数字</span></span><br><span class="line">df.loc[:, <span class="string">&quot;*ord_2*&quot;</span>] = df.*ord_2*.<span class="built_in">map</span>(mapping)</span><br></pre></td></tr></table></figure>
<p>映射前的数值计数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df.*ord_2*.value_counts()</span><br><span class="line">Freezing <span class="number">142726</span></span><br><span class="line">Warm <span class="number">124239</span></span><br><span class="line">Cold           <span class="number">97822</span></span><br><span class="line">Boiling Hot    <span class="number">84790</span></span><br><span class="line">Hot            <span class="number">67508</span></span><br><span class="line">Lava Hot       <span class="number">64840</span></span><br><span class="line">Name: *ord_2*, dtype: int64</span><br></pre></td></tr></table></figure>
<p>映射后的数值计数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.0</span>   <span class="number">142726</span></span><br><span class="line"><span class="number">1.0</span>   <span class="number">124239</span></span><br><span class="line"><span class="number">2.0</span>    <span class="number">97822</span></span><br><span class="line"><span class="number">3.0</span>    <span class="number">84790</span></span><br><span class="line"><span class="number">4.0</span>    <span class="number">67508</span></span><br><span class="line"><span class="number">5.0</span>    <span class="number">64840</span></span><br><span class="line">Name: *ord_2*, dtype: int64</span><br></pre></td></tr></table></figure>
<p>这种分类变量的编码方式被称为标签编码（Label Encoding）我们将每个类别编码为一个数字标签。</p>
<p>我们也可以使用 scikit-learn 中的 LabelEncoder 进行编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">df = pd.read_csv(<span class="string">&quot;../input/cat_train.csv&quot;</span>)</span><br><span class="line"><span class="comment"># 将缺失值填充为&quot;NONE&quot;</span></span><br><span class="line">df.loc[:, <span class="string">&quot;*ord_2*&quot;</span>] = df.*ord_2*.fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line"><span class="comment"># LabelEncoder编码</span></span><br><span class="line">lbl_enc = preprocessing.LabelEncoder()</span><br><span class="line"><span class="comment"># 转换数据</span></span><br><span class="line">df.loc[:, <span class="string">&quot;*ord_2*&quot;</span>] = lbl_enc.fit_transform(df.*ord_2*.values)</span><br></pre></td></tr></table></figure>
<p>你会看到我使用了 pandas 的 fillna。原因是 scikit-learn 的 LabelEncoder 无法处理 NaN 值，而 <em>ord_2</em> 列中有 NaN 值。</p>
<p>我们可以在许多基于树的模型中直接使用它：</p>
<ul>
<li>决策树</li>
<li>随机森林</li>
<li>提升树</li>
<li>或任何一种提升树模型</li>
<li>XGBoost</li>
<li>GBM</li>
<li>LightGBM</li>
</ul>
<p>这种编码方式不能用于线性模型、支持向量机或神经网络，因为它们希望数据是标准化的。</p>
<p>对于这些类型的模型，我们可以对数据进行二值化（binarize）处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Freezing    --&gt; <span class="number">0</span> --&gt; <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line">Warm        --&gt; <span class="number">1</span> --&gt; <span class="number">0</span> <span class="number">0</span> <span class="number">1</span></span><br><span class="line">Cold        --&gt; <span class="number">2</span> --&gt; <span class="number">0</span> <span class="number">1</span> <span class="number">0</span></span><br><span class="line">Boiling Hot --&gt; <span class="number">3</span> --&gt; <span class="number">0</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line">Hot         --&gt; <span class="number">4</span> --&gt; <span class="number">1</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line">Lava Hot    --&gt; <span class="number">5</span> --&gt; <span class="number">1</span> <span class="number">0</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>这只是将类别转换为数字，然后再转换为二值化表示。这样，我们就把一个特征分成了三个（在本例中）特征（或列）。如果我们有更多的类别，最终可能会分成更多的列。</p>
<p>如果我们用稀疏格式存储大量二值化变量，就可以轻松地存储这些变量。稀疏格式不过是一种在内存中存储数据的表示或方式，在这种格式中，你并不存储所有的值，而只存储重要的值。在上述二进制变量的情况中，最重要的就是有 1 的地方。</p>
<p>很难想象这样的格式，但举个例子就会明白。</p>
<p>假设上面的数据帧中只有一个特征：<em>ord_2</em>。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Index</th>
<th style="text-align:center">Feature</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">Warm</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">Hot</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">Lava hot</td>
</tr>
</tbody>
</table>
</div>
<p>目前，我们只看到数据集中的三个样本。让我们将其转换为二值表示法，即每个样本有三个项目。</p>
<p>这三个项目就是三个特征。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Index</th>
<th style="text-align:center">Feature_0</th>
<th style="text-align:center">Feature_1</th>
<th style="text-align:center">Feature_2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
</div>
<p>因此，我们的特征存储在一个有 3 行 3 列（3x3）的矩阵中。矩阵的每个元素占用 8 个字节。因此，这个数组的总内存需求为 8x3x3 = 72 字节。</p>
<p>我们还可以使用一个简单的 python 代码段来检查这一点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">example = np.array(</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(example.nbytes)</span><br></pre></td></tr></table></figure>
<p>这段代码将打印出 72，就像我们之前计算的那样。但我们需要存储这个矩阵的所有元素吗？如前所述，我们只对 1 感兴趣。0 并不重要，因为任何与 0 相乘的元素都是 0，而 0 与任何元素相加或相减也没有任何区别。只用 1 表示矩阵的一种方法是某种字典方法，其中键是行和列的索引，值是 1：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">0</span>, <span class="number">2</span>)      <span class="number">1</span></span><br><span class="line">(<span class="number">1</span>, <span class="number">0</span>)      <span class="number">1</span></span><br><span class="line">(<span class="number">2</span>, <span class="number">0</span>)      <span class="number">1</span></span><br><span class="line">(<span class="number">2</span>, <span class="number">2</span>)      <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>这样的符号占用的内存要少得多，因为它只需存储四个值（在本例中）。使用的总内存为 8x4 = 32 字节。任何 numpy 数组都可以通过简单的 python 代码转换为稀疏矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line"></span><br><span class="line">example = np.array(</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">sparse_example = sparse.csr_matrix(example)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sparse_example.data.nbytes)</span><br></pre></td></tr></table></figure>
<p>这将打印 32，比我们的密集数组少了这么多！稀疏 csr 矩阵的总大小是三个值的总和。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    sparse_example.data.nbytes +</span><br><span class="line">    sparse_example.indptr.nbytes +</span><br><span class="line">    sparse_example.indices.nbytes</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这将打印出 64 个元素，仍然少于我们的密集数组。遗憾的是，我不会详细介绍这些元素。你可以在 scipy 文档中了解更多。当我们拥有更大的数组时，比如说拥有数千个样本和数万个特征的数组，大小差异就会变得非常大。例如，我们使用基于计数特征的文本数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line">n_rows = <span class="number">10000</span></span><br><span class="line">n_cols = <span class="number">100000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成符合伯努利分布的随机数组，维度为[10000, 100000]</span></span><br><span class="line">example = np.random.binomial(<span class="number">1</span>, p=<span class="number">0.05</span>, size=(n_rows, n_cols))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of dense array: <span class="subst">&#123;example.nbytes&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 将随机矩阵转换为洗漱矩阵</span></span><br><span class="line">sparse_example = sparse.csr_matrix(example)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of sparse array: <span class="subst">&#123;sparse_example.data.nbytes&#125;</span>&quot;</span>)</span><br><span class="line">full_size = (</span><br><span class="line">    sparse_example.data.nbytes +</span><br><span class="line">    sparse_example.indptr.nbytes +</span><br><span class="line">    sparse_example.indices.nbytes</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Full size of sparse array: <span class="subst">&#123;full_size&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这将打印：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Size of dense array: <span class="number">8000000000</span></span><br><span class="line">Size of sparse array: <span class="number">399932496</span></span><br><span class="line">Full size of sparse array: <span class="number">599938748</span></span><br></pre></td></tr></table></figure>
<p>因此，密集阵列需要 ~8000MB 或大约 8GB 内存。而稀疏阵列只占用 399MB 内存。</p>
<p>这就是为什么当我们的特征中有大量零时，我们更喜欢稀疏阵列而不是密集阵列的原因。</p>
<p>请注意，稀疏矩阵有多种不同的表示方法。这里我只展示了其中一种（可能也是最常用的）方法。深入探讨这些方法超出了本书的范围，因此留给读者一个练习。</p>
<p>尽管二值化特征的稀疏表示比其密集表示所占用的内存要少得多，但对于分类变量来说，还有一种转换所占用的内存更少。这就是所谓的 “<strong>独热编码</strong>“。</p>
<p>独热编码也是一种二值编码，因为只有 0 和 1 两个值。但必须注意的是，它并不是二值表示法。我们可以通过下面的例子来理解它的表示法。</p>
<p>假设我们用一个向量来表示 <em>ord_2</em> 变量的每个类别。这个向量的大小与 <em>ord_2</em> 变量的类别数相同。在这种特定情况下，每个向量的大小都是 6，并且除了一个位置外，其他位置都是 0。让我们来看看这个特殊的向量表。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Freezing</th>
<th style="text-align:center">0</th>
<th style="text-align:center">0</th>
<th style="text-align:center">0</th>
<th style="text-align:center">0</th>
<th style="text-align:center">0</th>
<th style="text-align:center">1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Warm</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">Cold</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">Boiling Hot</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">Hot</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">Lava Hot</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
</div>
<p>我们看到向量的大小是 1x6，即向量中有 6 个元素。这个数字是怎么来的呢？如果你仔细观察，就会发现如前所述，有 6 个类别。在进行独热编码时，向量的大小必须与我们要查看的类别数相同。每个向量都有一个 1，其余所有值都是 0。现在，让我们用这些特征来代替之前的二值化特征，看看能节省多少内存。</p>
<p>如果你还记得以前的数据，它看起来如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Index</th>
<th style="text-align:center">Feature</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">Warm</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">Hot</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">Lava hot</td>
</tr>
</tbody>
</table>
</div>
<p>每个样本有 3 个特征。但在这种情况下，独热向量的大小为 6。因此，我们有 6 个特征，而不是 3 个。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Index</th>
<th style="text-align:center">F_0</th>
<th style="text-align:center">F_1</th>
<th style="text-align:center">F_2</th>
<th style="text-align:center">F_3</th>
<th style="text-align:center">F_4</th>
<th style="text-align:center">F_5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
</div>
<p>因此，我们有 6 个特征，而在这个 3x6 数组中，只有 3 个 1。使用 numpy 计算大小与二值化大小计算脚本非常相似。你需要改变的只是数组。让我们看看这段代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line">example = np.array(</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of dense array: <span class="subst">&#123;example.nbytes&#125;</span>&quot;</span>)</span><br><span class="line">sparse_example = sparse.csr_matrix(example)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of sparse array: <span class="subst">&#123;sparse_example.data.nbytes&#125;</span>&quot;</span>)</span><br><span class="line">full_size = (</span><br><span class="line">    sparse_example.data.nbytes +</span><br><span class="line">    sparse_example.indptr.nbytes +</span><br><span class="line">    sparse_example.indices.nbytes</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Full size of sparse array: <span class="subst">&#123;full_size&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>打印内存大小为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Size of dense array: <span class="number">144</span></span><br><span class="line">Size of sparse array: <span class="number">24</span></span><br><span class="line">Full size of sparse array: <span class="number">52</span></span><br></pre></td></tr></table></figure>
<p>我们可以看到，密集矩阵的大小远远大于二值化矩阵的大小。不过，稀疏数组的大小要更小。让我们用更大的数组来试试。在本例中，我们将使用 scikit-learn 中的 OneHotEncoder 将包含 1001 个类别的特征数组转换为密集矩阵和稀疏矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成符合均匀分布的随机整数，维度为[1000000, 10000000]</span></span><br><span class="line">example = np.random.randint(<span class="number">1000</span>, size=<span class="number">1000000</span>)</span><br><span class="line"><span class="comment"># 独热编码，非稀疏矩阵</span></span><br><span class="line">ohe = preprocessing.OneHotEncoder(sparse=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 将随机数组展平</span></span><br><span class="line">ohe_example = ohe.fit_transform(example.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of dense array: <span class="subst">&#123;ohe_example.nbytes&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 独热编码，稀疏矩阵</span></span><br><span class="line">ohe = preprocessing.OneHotEncoder(sparse=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 将随机数组展平</span></span><br><span class="line">ohe_example = ohe.fit_transform(example.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of sparse array: <span class="subst">&#123;ohe_example.data.nbytes&#125;</span>&quot;</span>)</span><br><span class="line">full_size = (</span><br><span class="line">    ohe_example.data.nbytes +</span><br><span class="line">    ohe_example.indptr.nbytes +</span><br><span class="line">    ohe_example.indices.nbytes</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Full size of sparse array: <span class="subst">&#123;full_size&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>上面代码打印的输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Size of dense array: <span class="number">8000000000</span></span><br><span class="line">Size of sparse array: <span class="number">8000000</span></span><br><span class="line">Full size of sparse array: <span class="number">16000004</span></span><br></pre></td></tr></table></figure>
<p>这里的密集阵列大小约为 8GB，稀疏阵列为 8MB。如果可以选择，你会选择哪个？在我看来，选择很简单，不是吗？</p>
<p>这三种方法（标签编码、稀疏矩阵、独热编码）是处理分类变量的最重要方法。不过，你还可以用很多其他不同的方法来处理分类变量。将分类变量转换为数值变量就是其中的一个例子。</p>
<p>假设我们回到之前的分类特征数据（原始数据中的 cat-in-the-dat-ii）。在数据中，<em>ord_2</em> 的值为“热“的 id 有多少？</p>
<p>我们可以通过计算数据的形状（shape）轻松计算出这个值，其中 <em>ord_2</em> 列的值为 <em>Boiling Hot</em>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df[df.ord_2 == <span class="string">&quot;Boiling Hot&quot;</span>].shape</span><br><span class="line">Out[X]: (<span class="number">84790</span>, <span class="number">25</span>)</span><br></pre></td></tr></table></figure>
<p>我们可以看到，有 84790 条记录具有此值。我们还可以使用 pandas 中的 <em>groupby</em> 计算所有类别的该值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.groupby([<span class="string">&quot;ord_2&quot;</span>])[<span class="string">&quot;id&quot;</span>].count()</span><br><span class="line">Out[X]:</span><br><span class="line">ord_2</span><br><span class="line">Boiling Hot    <span class="number">84790</span></span><br><span class="line">Cold <span class="number">97822</span></span><br><span class="line">Freezing <span class="number">142726</span></span><br><span class="line">Hot <span class="number">67508</span></span><br><span class="line">Lava Hot <span class="number">64840</span></span><br><span class="line">Warm <span class="number">124239</span></span><br><span class="line">Name: <span class="built_in">id</span>, dtype: int64</span><br></pre></td></tr></table></figure>
<p>如果我们只是将 <em>ord_2</em> 列替换为其计数值，那么我们就将其转换为一种数值特征了。我们可以使用 pandas 的<em>transform</em>函数和 <em>groupby</em> 来创建新列或替换这一列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.groupby([<span class="string">&quot;ord_2&quot;</span>])[<span class="string">&quot;id&quot;</span>].transform(<span class="string">&quot;count&quot;</span>)</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>         <span class="number">67508.0</span></span><br><span class="line"><span class="number">1</span>        <span class="number">124239.0</span></span><br><span class="line"><span class="number">2</span>        <span class="number">142726.0</span></span><br><span class="line"><span class="number">3</span>         <span class="number">64840.0</span></span><br><span class="line"><span class="number">4</span>         <span class="number">97822.0</span></span><br><span class="line">...</span><br><span class="line"><span class="number">599995</span>   <span class="number">142726.0</span></span><br><span class="line"><span class="number">599996</span>    <span class="number">84790.0</span></span><br><span class="line"><span class="number">599997</span>   <span class="number">142726.0</span></span><br><span class="line"><span class="number">599998</span>   <span class="number">124239.0</span></span><br><span class="line"><span class="number">599999</span>    <span class="number">84790.0</span></span><br><span class="line">Name: <span class="built_in">id</span>, Length: <span class="number">600000</span>, dtype: float64</span><br></pre></td></tr></table></figure>
<p>你可以添加所有特征的计数，也可以替换它们，或者根据多个列及其计数进行分组。例如，以下代码通过对 <em>ord_1</em> 和 <em>ord_2</em> 列分组进行计数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.groupby(</span><br><span class="line">   ...:     [</span><br><span class="line">   ...:        <span class="string">&quot;ord_1&quot;</span>,</span><br><span class="line">   ...:        <span class="string">&quot;ord_2&quot;</span></span><br><span class="line">   ...:     ]</span><br><span class="line">   ...: )[<span class="string">&quot;id&quot;</span>].count().reset_index(name=<span class="string">&quot;count&quot;</span>)</span><br><span class="line">Out[X]:</span><br><span class="line">ord_1        ord_2  count</span><br><span class="line"><span class="number">0</span>  Contributor  Boiling Hot <span class="number">15634</span></span><br><span class="line"><span class="number">1</span>  Contributor         Cold <span class="number">17734</span></span><br><span class="line"><span class="number">2</span>  Contributor     Freezing <span class="number">26082</span></span><br><span class="line"><span class="number">3</span>  Contributor          Hot <span class="number">12428</span></span><br><span class="line"><span class="number">4</span>  Contributor     Lava Hot <span class="number">11919</span></span><br><span class="line"><span class="number">5</span>  Contributor         Warm <span class="number">22774</span></span><br><span class="line"><span class="number">6</span>       Expert  Boiling Hot <span class="number">19477</span></span><br><span class="line"><span class="number">7</span>       Expert         Cold <span class="number">22956</span></span><br><span class="line"><span class="number">8</span>       Expert     Freezing <span class="number">33249</span></span><br><span class="line"><span class="number">9</span>       Expert          Hot <span class="number">15792</span></span><br><span class="line"><span class="number">10</span>      Expert     Lava Hot <span class="number">15078</span></span><br><span class="line"><span class="number">11</span>      Expert         Warm <span class="number">28900</span></span><br><span class="line"><span class="number">12</span> Grandmaster  Boiling Hot <span class="number">13623</span></span><br><span class="line"><span class="number">13</span> Grandmaster         Cold <span class="number">15464</span></span><br><span class="line"><span class="number">14</span> Grandmaster     Freezing <span class="number">22818</span></span><br><span class="line"><span class="number">15</span> Grandmaster          Hot <span class="number">10805</span></span><br><span class="line"><span class="number">16</span> Grandmaster     Lava Hot <span class="number">10363</span></span><br><span class="line"><span class="number">17</span> Grandmaster         Warm <span class="number">19899</span></span><br><span class="line"><span class="number">18</span>      Master  Boiling Hot <span class="number">10800</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>请注意，我已经从输出中删除了一些行，以便在一页中容纳这些行。这是另一种可以作为功能添加的计数。您现在一定已经注意到，我使用 id 列进行计数。不过，你也可以通过对列的组合进行分组，对其他列进行计数。</p>
<p>还有一个小窍门，就是从这些分类变量中创建新特征。你可以从现有的特征中创建新的分类特征，而且可以毫不费力地做到这一点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df[<span class="string">&quot;new_feature&quot;</span>] = (</span><br><span class="line">   ...:     df.ord_1.astype(<span class="built_in">str</span>)</span><br><span class="line">   ...:     + <span class="string">&quot;_&quot;</span></span><br><span class="line">   ...:     + df.ord_2.astype(<span class="built_in">str</span>)</span><br><span class="line">   ...: )</span><br><span class="line">In [X]: df.new_feature</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>                Contributor_Hot</span><br><span class="line"><span class="number">1</span>               Grandmaster_Warm</span><br><span class="line"><span class="number">2</span>                   nan_Freezing</span><br><span class="line"><span class="number">3</span>                Novice_Lava Hot</span><br><span class="line"><span class="number">4</span>               Grandmaster_Cold</span><br><span class="line">               ...</span><br><span class="line"><span class="number">599999</span>   Contributor_Boiling Hot</span><br><span class="line">Name: new_feature, Length: <span class="number">600000</span>, dtype: <span class="built_in">object</span></span><br></pre></td></tr></table></figure>
<p>在这里，我们用下划线将 <em>ord_1</em> 和 <em>ord_2</em> 合并，然后将这些列转换为字符串类型。请注意，NaN 也会转换为字符串。不过没关系。我们也可以将 NaN 视为一个新的类别。这样，我们就有了一个由这两个特征组合而成的新特征。您还可以将三列以上或四列甚至更多列组合在一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df[<span class="string">&quot;new_feature&quot;</span>] = (</span><br><span class="line">   ...:     df.ord_1.astype(<span class="built_in">str</span>)</span><br><span class="line">   ...:     + <span class="string">&quot;_&quot;</span></span><br><span class="line">   ...:     + df.ord_2.astype(<span class="built_in">str</span>)</span><br><span class="line">   ...:     + <span class="string">&quot;_&quot;</span></span><br><span class="line">   ...:     + df.ord_3.astype(<span class="built_in">str</span>)</span><br><span class="line">   ...: )</span><br><span class="line">In [X]: df.new_feature</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>                Contributor_Hot_c</span><br><span class="line"><span class="number">1</span>               Grandmaster_Warm_e</span><br><span class="line"><span class="number">2</span>                   nan_Freezing_n</span><br><span class="line"><span class="number">3</span>                Novice_Lava Hot_a</span><br><span class="line"><span class="number">4</span>               Grandmaster_Cold_h</span><br><span class="line">               ...</span><br><span class="line"><span class="number">599999</span>   Contributor_Boiling Hot_b</span><br><span class="line">Name: new_feature, Length: <span class="number">600000</span>, dtype: <span class="built_in">object</span></span><br></pre></td></tr></table></figure>
<p>那么，我们应该把哪些类别结合起来呢？这并没有一个简单的答案。这取决于您的数据和特征类型。一些领域知识对于创建这样的特征可能很有用。但是，如果你不担心内存和 CPU 的使用，你可以采用一种贪婪的方法，即创建许多这样的组合，然后使用一个模型来决定哪些特征是有用的，并保留它们。我们将在本书稍后部分介绍这种方法。</p>
<p>无论何时获得分类变量，都要遵循以下简单步骤：</p>
<ul>
<li>填充 NaN 值（这一点非常重要！）。</li>
<li>使用 scikit-learn 的 LabelEncoder 或映射字典进行标签编码，将它们转换为整数。如果没有填充 NaN 值，可能需要在这一步中进行处理</li>
<li>创建独热编码。是的，你可以跳过二值化！</li>
<li>建模！我指的是机器学习。</li>
</ul>
<p>在分类特征中处理 NaN 数据非常重要，否则您可能会从 scikit-learn 的 LabelEncoder 中得到臭名昭著的错误信息：</p>
<p>ValueError: y 包含以前未见过的标签： [Nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan)</p>
<p>这仅仅意味着，在转换测试数据时，数据中出现了 NaN 值。这是因为你在训练时忘记了处理它们。<strong>处理 NaN 值</strong>的一个简单方法就是丢弃它们。虽然简单，但并不理想。NaN 值中可能包含很多信息，如果只是丢弃这些值，就会丢失这些信息。在很多情况下，大部分数据都是 NaN 值，因此不能丢弃 NaN 值的行/样本。处理 NaN 值的另一种方法是将其作为一个全新的类别。这是处理 NaN 值最常用的方法。如果使用 pandas，还可以通过非常简单的方式实现。</p>
<p>请看我们之前查看过的数据的 <em>ord_2</em> 列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.ord_2.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line">Freezing <span class="number">142726</span></span><br><span class="line">Warm <span class="number">124239</span></span><br><span class="line">Cold           <span class="number">97822</span></span><br><span class="line">Boiling Hot    <span class="number">84790</span></span><br><span class="line">Hot            <span class="number">67508</span></span><br><span class="line">Lava Hot       <span class="number">64840</span></span><br><span class="line">Name: ord_2, dtype: int64</span><br></pre></td></tr></table></figure>
<p>填入 NaN 值后，就变成了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.ord_2.fillna(<span class="string">&quot;NONE&quot;</span>).value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line">Freezing <span class="number">142726</span></span><br><span class="line">Warm <span class="number">124239</span></span><br><span class="line">Cold           <span class="number">97822</span></span><br><span class="line">Boiling Hot    <span class="number">84790</span></span><br><span class="line">Hot            <span class="number">67508</span></span><br><span class="line">Lava Hot       <span class="number">64840</span></span><br><span class="line">NONE           <span class="number">18075</span></span><br><span class="line">Name: ord_2, dtype: int64</span><br></pre></td></tr></table></figure>
<p>哇！这一列中有 18075 个 NaN 值，而我们之前甚至都没有考虑使用它们。增加了这个新类别后，类别总数从 6 个增加到了 7 个。这没关系，因为现在我们在建立模型时，也会考虑 NaN。相关信息越多，模型就越好。</p>
<p>假设 <em>ord_2</em> 没有任何 NaN 值。我们可以看到，这一列中的所有类别都有显著的计数。其中没有 “罕见 “类别，即只在样本总数中占很小比例的类别。现在，让我们假设您在生产中部署了使用这一列的模型，当模型或项目上线时，您在 <em>ord_2</em> 列中得到了一个在训练中不存在的类别。在这种情况下，模型管道会抛出一个错误，您对此无能为力。如果出现这种情况，那么可能是生产中的管道出了问题。如果这是预料之中的，那么您就必须修改您的模型管道，并在这六个类别中加入一个新类别。</p>
<p>这个新类别被称为 “罕见 “类别。罕见类别是一种不常见的类别，可以包括许多不同的类别。您也可以尝试使用近邻模型来 “预测 “未知类别。请记住，如果您预测了这个类别，它就会成为训练数据中的一个类别。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page101_image.png" alt=""></p>
<p align="center"><b>图 3：具有不同特征且无标签的数据集示意图，其中一个特征可能会在测试集或实时数据中出现新值</b> </p>

<p>当我们有一个如图 3 所示的数据集时，我们可以建立一个简单的模型，对除 “f3 “之外的所有特征进行训练。这样，你将创建一个模型，在不知道或训练中没有 “f3 “时预测它。我不敢说这样的模型是否能带来出色的性能，但也许能处理测试集或实时数据中的缺失值，就像机器学习中的其他事情一样，不尝试一下是说不准的。</p>
<p>如果你有一个固定的测试集，你可以将测试数据添加到训练中，以了解给定特征中的类别。这与半监督学习非常相似，即使用无法用于训练的数据来改进模型。这也会照顾到在训练数据中出现次数极少但在测试数据中大量存在的稀有值。你的模型将更加稳健。</p>
<p>很多人认为这种想法会过度拟合。可能过拟合，也可能不过拟合。有一个简单的解决方法。如果你在设计交叉验证时，能够在测试数据上运行模型时复制预测过程，那么它就永远不会过拟合。这意味着第一步应该是分离折叠，在每个折叠中，你应该应用与测试数据相同的预处理。假设您想合并训练数据和测试数据，那么在每个折叠中，您必须合并训练数据和验证数据，并确保验证数据集复制了测试集。在这种特定情况下，您必须以这样一种方式设计验证集，使其包含训练集中 “未见 “的类别。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page102_image.png" alt=""></p>
<p align="center"><b>图 4：对训练集和测试集进行简单合并，以了解测试集中存在但训练集中不存在的类别或训练集中罕见的类别</b> </p>

<p>只要看一下图 4 和下面的代码，就能很容易理解其工作原理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="comment"># 读取训练集</span></span><br><span class="line">train = pd.read_csv(<span class="string">&quot;../input/cat_train.csv&quot;</span>)</span><br><span class="line"><span class="comment"># 读取测试集</span></span><br><span class="line">test = pd.read_csv(<span class="string">&quot;../input/cat_test.csv&quot;</span>)</span><br><span class="line"><span class="comment"># 将测试集&quot;target&quot;列全部置为-1</span></span><br><span class="line">test.loc[:, <span class="string">&quot;target&quot;</span>] = -<span class="number">1</span></span><br><span class="line"><span class="comment"># 将训练集、测试集沿行拼接</span></span><br><span class="line">data = pd.concat([train, test]).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 将除&quot;id&quot;和&quot;target&quot;列的其他特征列名取出</span></span><br><span class="line">features = [x <span class="keyword">for</span> x <span class="keyword">in</span> train.columns <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;target&quot;</span>]]</span><br><span class="line"><span class="comment"># 遍历特征</span></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> features:</span><br><span class="line">    <span class="comment"># 标签编码</span></span><br><span class="line">    lbl_enc = preprocessing.LabelEncoder()</span><br><span class="line">    <span class="comment"># 将空值替换为&quot;NONE&quot;,并将该列格式变为str</span></span><br><span class="line">    temp_col = data[feat].fillna(<span class="string">&quot;NONE&quot;</span>).astype(<span class="built_in">str</span>).values</span><br><span class="line">    <span class="comment"># 转换数值</span></span><br><span class="line">    data.loc[:, feat] = lbl_enc.fit_transform(temp_col)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据&quot;target&quot;列将训练集与测试集分开</span></span><br><span class="line">train = data[data.target != -<span class="number">1</span>].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">test = data[data.target == -<span class="number">1</span>].reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>当您遇到已经有测试数据集的问题时，这个技巧就会起作用。必须注意的是，这一招在实时环境中不起作用。例如，假设您所在的公司提供实时竞价解决方案（RTB）。RTB 系统会对在线看到的每个用户进行竞价，以购买广告空间。这种模式可使用的功能可能包括网站中浏览的页面。我们假设这些特征是用户访问的最后五个类别/页面。在这种情况下，如果网站引入了新的类别，我们将无法再准确预测。在这种情况下，我们的模型就会失效。这种情况可以通过使用 <strong>“未知 “类别来避免</strong>。</p>
<p>在我们的 cat-in-the-dat 数据集中，<em>ord_2</em> 列中已经有了未知类别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.ord_2.fillna(<span class="string">&quot;NONE&quot;</span>).value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line">Freezing       <span class="number">142726</span></span><br><span class="line">Warm           <span class="number">124239</span></span><br><span class="line">Cold           <span class="number">97822</span></span><br><span class="line">Boiling Hot    <span class="number">84790</span></span><br><span class="line">Hot            <span class="number">67508</span></span><br><span class="line">Lava Hot       <span class="number">64840</span></span><br><span class="line">NONE           <span class="number">18075</span></span><br><span class="line">Name: ord_2, dtype: int64</span><br></pre></td></tr></table></figure>
<p>我们可以将 “NONE “视为未知。因此，如果在实时测试过程中，我们获得了以前从未见过的新类别，我们就会将其标记为 “NONE”。</p>
<p>这与自然语言处理问题非常相似。我们总是基于固定的词汇建立模型。增加词汇量就会增加模型的大小。像 BERT 这样的转换器模型是在 ~30000 个单词（英语）的基础上训练的。因此，当有新词输入时，我们会将其标记为 UNK（未知）。</p>
<p>因此，您可以假设测试数据与训练数据具有相同的类别，也可以在训练数据中引入罕见或未知类别，以处理测试数据中的新类别。</p>
<p>让我们看看填入 NaN 值后 ord_4 列的值计数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.ord_4.fillna(<span class="string">&quot;NONE&quot;</span>).value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line">N       <span class="number">39978</span></span><br><span class="line">P       <span class="number">37890</span></span><br><span class="line">Y       <span class="number">36657</span></span><br><span class="line">A       <span class="number">36633</span></span><br><span class="line">R       <span class="number">33045</span></span><br><span class="line">U       <span class="number">32897</span></span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">K       <span class="number">21676</span></span><br><span class="line">I       <span class="number">19805</span></span><br><span class="line">NONE    <span class="number">17930</span></span><br><span class="line">D       <span class="number">17284</span></span><br><span class="line">F       <span class="number">16721</span></span><br><span class="line">W       <span class="number">8268</span></span><br><span class="line">Z       <span class="number">5790</span></span><br><span class="line">S       <span class="number">4595</span></span><br><span class="line">G       <span class="number">3404</span></span><br><span class="line">V       <span class="number">3107</span></span><br><span class="line">J       <span class="number">1950</span></span><br><span class="line">L       <span class="number">1657</span></span><br><span class="line">Name: ord_4, dtype: int64</span><br></pre></td></tr></table></figure>
<p>我们看到，有些数值只出现了几千次，有些则出现了近 40000 次。NaN 也经常出现。请注意，我已经从输出中删除了一些值。</p>
<p>现在，我们可以定义将一个值称为 “<strong>罕见（rare）</strong> “的标准了。比方说，在这一列中，稀有值的要求是计数小于 2000。这样看来，J 和 L 就可以被标记为稀有值了。使用 pandas，根据计数阈值替换类别非常简单。让我们来看看它是如何实现的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.ord_4 = df.ord_4.fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">In [X]: df.loc[</span><br><span class="line">   ...:     df[<span class="string">&quot;ord_4&quot;</span>].value_counts()[df[<span class="string">&quot;ord_4&quot;</span>]].values &lt; <span class="number">2000</span>,</span><br><span class="line">   ...:    <span class="string">&quot;ord_4&quot;</span></span><br><span class="line">   ...: ] = <span class="string">&quot;RARE&quot;</span></span><br><span class="line">In [X]: df.ord_4.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line">N      <span class="number">39978</span></span><br><span class="line">P      <span class="number">37890</span></span><br><span class="line">Y      <span class="number">36657</span></span><br><span class="line">A      <span class="number">36633</span></span><br><span class="line">R      <span class="number">33045</span></span><br><span class="line">U      <span class="number">32897</span></span><br><span class="line">M      <span class="number">32504</span></span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">B      <span class="number">25212</span></span><br><span class="line">E      <span class="number">21871</span></span><br><span class="line">K      <span class="number">21676</span></span><br><span class="line">I      <span class="number">19805</span></span><br><span class="line">NONE   <span class="number">17930</span></span><br><span class="line">D      <span class="number">17284</span></span><br><span class="line">F      <span class="number">16721</span></span><br><span class="line">W       <span class="number">8268</span></span><br><span class="line">Z       <span class="number">5790</span></span><br><span class="line">S       <span class="number">4595</span></span><br><span class="line">RARE    <span class="number">3607</span></span><br><span class="line">G       <span class="number">3404</span></span><br><span class="line">V       <span class="number">3107</span></span><br><span class="line">Name: ord_4, dtype: int64</span><br></pre></td></tr></table></figure>
<p>我们认为，只要某个类别的值小于 2000，就将其替换为罕见。因此，现在在测试数据时，所有未见过的新类别都将被映射为 “RARE”，而所有缺失值都将被映射为 “NONE”。</p>
<p>这种方法还能确保即使有新的类别，模型也能在实际环境中正常工作。</p>
<p>现在，我们已经具备了处理任何带有分类变量问题所需的一切条件。让我们尝试建立第一个模型，并逐步提高其性能。</p>
<p>在构建任何类型的模型之前，交叉检验至关重要。我们已经看到了标签/目标分布，知道这是一个目标偏斜的二元分类问题。因此，我们将使用 StratifiedKFold 来分割数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 读取数据文件</span></span><br><span class="line">	df = pd.read_csv(<span class="string">&quot;../input/cat_train.csv&quot;</span>)</span><br><span class="line">    <span class="comment"># 添加&quot;kfold&quot;列，并置为-1</span></span><br><span class="line">	df[<span class="string">&quot;kfold&quot;</span>] = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 打乱数据顺序，重置索引</span></span><br><span class="line">	df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 将目标列取出</span></span><br><span class="line">	y = df.target.values</span><br><span class="line">    <span class="comment"># 分层k折交叉检验</span></span><br><span class="line">	kf = model_selection.StratifiedKFold(n_splits=<span class="number">5</span>)</span><br><span class="line">	<span class="keyword">for</span> f, (t_, v_) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kf.split(X=df, y=y)):</span><br><span class="line">        <span class="comment"># 区分折叠</span></span><br><span class="line">		df.loc[v_, <span class="string">&#x27;kfold&#x27;</span>] = f</span><br><span class="line">    <span class="comment"># 保存文件</span></span><br><span class="line">	df.to_csv(<span class="string">&quot;../input/cat_train_folds.csv&quot;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>现在我们可以检查新的折叠 csv，查看每个折叠的样本数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [X]: <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">In [X]: df = pd.read_csv(<span class="string">&quot;../input/cat_train_folds.csv&quot;</span>)</span><br><span class="line">In [X]: df.kfold.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">4</span>   <span class="number">120000</span></span><br><span class="line"><span class="number">3</span>   <span class="number">120000</span></span><br><span class="line"><span class="number">2</span>   <span class="number">120000</span></span><br><span class="line"><span class="number">1</span>   <span class="number">120000</span></span><br><span class="line"><span class="number">0</span>   <span class="number">120000</span></span><br><span class="line">Name: kfold, dtype: int64</span><br></pre></td></tr></table></figure>
<p>所有折叠都有 120000 个样本。这是意料之中的，因为训练数据有 600000 个样本，而我们做了 5 次折叠。到目前为止，一切顺利。</p>
<p>现在，我们还可以检查每个折叠的目标分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df[df.kfold==<span class="number">0</span>].target.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>   <span class="number">97536</span></span><br><span class="line"><span class="number">1</span>   <span class="number">22464</span></span><br><span class="line">Name: target, dtype: int64</span><br><span class="line">In [X]: df[df.kfold==<span class="number">1</span>].target.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>   <span class="number">97536</span></span><br><span class="line"><span class="number">1</span>   <span class="number">22464</span></span><br><span class="line">Name: target, dtype: int64</span><br><span class="line">In [X]: df[df.kfold==<span class="number">2</span>].target.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>   <span class="number">97535</span></span><br><span class="line"><span class="number">1</span>   <span class="number">22465</span></span><br><span class="line">Name: target, dtype: int64</span><br><span class="line">In [X]: df[df.kfold==<span class="number">3</span>].target.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>   <span class="number">97535</span></span><br><span class="line"><span class="number">1</span>   <span class="number">22465</span></span><br><span class="line">Name: target, dtype: int64</span><br><span class="line">In [X]: df[df.kfold==<span class="number">4</span>].target.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>   <span class="number">97535</span></span><br><span class="line"><span class="number">1</span>   <span class="number">22465</span></span><br><span class="line">Name: target, dtype: int64</span><br></pre></td></tr></table></figure>
<p>我们看到，在每个折叠中，目标的分布都是一样的。这正是我们所需要的。它也可以是相似的，并不一定要一直相同。现在，当我们建立模型时，每个折叠中的标签分布都将相同。</p>
<p>我们可以建立的最简单的模型之一是对所有数据进行独热编码并使用逻辑回归。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    <span class="comment"># 读取分层k折交叉检验数据</span></span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/cat_train_folds.csv&quot;</span>)</span><br><span class="line">    <span class="comment"># 取除&quot;id&quot;, &quot;target&quot;, &quot;kfold&quot;外的其他特征列</span></span><br><span class="line">    features = [</span><br><span class="line">        f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;id&quot;</span>, <span class="string">&quot;target&quot;</span>, <span class="string">&quot;kfold&quot;</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># 遍历特征列表</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="comment"># 将空值置为&quot;NONE&quot;</span></span><br><span class="line">        df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">    <span class="comment"># 取训练集（kfold列中不为fold的样本，重置索引）</span></span><br><span class="line">    df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 取验证集（kfold列中为fold的样本，重置索引）</span></span><br><span class="line">    df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 独热编码</span></span><br><span class="line">    ohe = preprocessing.OneHotEncoder()</span><br><span class="line">    <span class="comment"># 将训练集、验证集沿行合并</span></span><br><span class="line">    full_data = pd.concat([df_train[features], df_valid[features]], axis=<span class="number">0</span>)</span><br><span class="line">    ohe.fit(full_data[features])</span><br><span class="line">    <span class="comment"># 转换训练集</span></span><br><span class="line">    x_train = ohe.transform(df_train[features])</span><br><span class="line">    <span class="comment"># 转换测试集</span></span><br><span class="line">    x_valid = ohe.transform(df_valid[features])</span><br><span class="line">    <span class="comment"># 逻辑回归</span></span><br><span class="line">    model = linear_model.LogisticRegression()</span><br><span class="line">    <span class="comment"># 使用训练集训练模型</span></span><br><span class="line">    model.fit(x_train, df_train.target.values)</span><br><span class="line">    <span class="comment"># 使用验证集得到预测标签</span></span><br><span class="line">    valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 计算auc指标</span></span><br><span class="line">    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(auc)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 运行折叠0</span></span><br><span class="line">    run(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>那么，发生了什么呢？</p>
<p>我们创建了一个函数，将数据分为训练和验证两部分，给定折叠数，处理 NaN 值，对所有数据进行单次编码，并训练一个简单的逻辑回归模型。</p>
<p>当我们运行这部分代码时，会产生如下输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">❯ python ohe_logres.py</span><br><span class="line">/home/abhishek/miniconda3/envs/ml/lib/python3<span class="number">.7</span>/site-</span><br><span class="line">packages/sklearn/linear_model/_logistic.py:<span class="number">939</span>: ConvergenceWarning: lbfgs</span><br><span class="line">failed to converge (status=<span class="number">1</span>):</span><br><span class="line">STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.</span><br><span class="line">Increase the number of iterations (max_iter) <span class="keyword">or</span> scale the data <span class="keyword">as</span> shown</span><br><span class="line"><span class="keyword">in</span>:</span><br><span class="line">	https://scikit-learn.org/stable/modules/preprocessing.html.</span><br><span class="line">Please also refer to the documentation <span class="keyword">for</span> alternative solver options:</span><br><span class="line">	https://scikit-learn.org/stable/modules/linear_model.html<span class="comment">#logistic-</span></span><br><span class="line">regression</span><br><span class="line">extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)</span><br><span class="line"><span class="number">0.7847865042255127</span></span><br></pre></td></tr></table></figure>
<p>有一些警告。逻辑回归似乎没有收敛到最大迭代次数。我们没有调整参数，所以没有问题。我们看到 AUC 为 0.785。</p>
<p>现在让我们对代码进行简单修改，运行所有折叠。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">....</span><br><span class="line"></span><br><span class="line">	model = linear_model.LogisticRegression()</span><br><span class="line">	model.fit(x_train, df_train.target.values)</span><br><span class="line">	valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">	auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 循环运行0~4折</span></span><br><span class="line">	<span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">		run(fold_)</span><br></pre></td></tr></table></figure>
<p>请注意，我们并没有做很大的改动，所以我只显示了部分代码行，其中一些代码行有改动。</p>
<p>这就打印出了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python -W ignore ohe_logres.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.7847865042255127</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.7853553605899214</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.7879321942914885</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.7870315929550808</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.7864668243125608</span></span><br></pre></td></tr></table></figure>
<p>请注意，我使用”-W ignore “忽略了所有警告。</p>
<p>我们看到，AUC 分数在所有褶皱中都相当稳定。平均 AUC 为 0.78631449527。对于我们的第一个模型来说相当不错！</p>
<p>很多人在遇到这种问题时会首先使用基于树的模型，比如随机森林。在这个数据集中应用随机森林时，我们可以使用标签编码（label encoding），将每一列中的每个特征都转换为整数，而不是之前讨论过的独热编码。</p>
<p>这种编码与独热编码并无太大区别。让我们来看看。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> ensemble</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/cat_train_folds.csv&quot;</span>)</span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;id&quot;</span>, <span class="string">&quot;target&quot;</span>, <span class="string">&quot;kfold&quot;</span>) ]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="comment"># 标签编码</span></span><br><span class="line">        lbl = preprocessing.LabelEncoder()</span><br><span class="line">    	lbl.fit(df[col])</span><br><span class="line">		df.loc[:, col] = lbl.transform(df[col])</span><br><span class="line">	df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">	df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">	x_train = df_train[features].values</span><br><span class="line">	x_valid = df_valid[features].values</span><br><span class="line">    <span class="comment"># 随机森林模型</span></span><br><span class="line">	model = ensemble.RandomForestClassifier(n_jobs=-<span class="number">1</span>)</span><br><span class="line">    model.fit(x_train, df_train.target.values)</span><br><span class="line">	valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">	<span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">		run(fold_)</span><br></pre></td></tr></table></figure>
<p>我们使用 scikit-learn 中的随机森林，并取消了独热编码。我们使用标签编码代替独热编码。得分如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python lbl_rf.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.7167390828113697</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.7165459672958506</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.7159709909587376</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.7161589664189556</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.7156020216155978</span></span><br></pre></td></tr></table></figure>
<p>哇 巨大的差异！ 随机森林模型在没有任何超参数调整的情况下，表现要比简单的逻辑回归差很多。</p>
<p>这就是为什么我们总是应该先从简单模型开始的原因。随机森林模型的粉丝会从这里开始，而忽略逻辑回归模型，认为这是一个非常简单的模型，不能带来比随机森林更好的价值。这种人将会犯下大错。在我们实现随机森林的过程中，与逻辑回归相比，折叠需要更长的时间才能完成。因此，我们不仅损失了 AUC，还需要更长的时间来完成训练。请注意，使用随机森林进行推理也很耗时，而且占用的空间也更大。</p>
<p>如果我们愿意，也可以尝试在稀疏的独热编码数据上运行随机森林，但这会耗费大量时间。我们还可以尝试使用奇异值分解来减少稀疏的独热编码矩阵。这是自然语言处理中提取主题的常用方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> decomposition</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> ensemble</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">	df = pd.read_csv(<span class="string">&quot;../input/cat_train_folds.csv&quot;</span>)</span><br><span class="line">	features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;id&quot;</span>, <span class="string">&quot;target&quot;</span>, <span class="string">&quot;kfold&quot;</span>)]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">		df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">	df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">	df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 独热编码</span></span><br><span class="line">	ohe = preprocessing.OneHotEncoder()</span><br><span class="line">	full_data = pd.concat([df_train[features], df_valid[features]], axis=<span class="number">0</span>)</span><br><span class="line">	ohe.fit(full_data[features])</span><br><span class="line"></span><br><span class="line">	x_train = ohe.transform(df_train[features])</span><br><span class="line">	x_valid = ohe.transform(df_valid[features])</span><br><span class="line">    <span class="comment"># 奇异值分解</span></span><br><span class="line">	svd = decomposition.TruncatedSVD(n_components=<span class="number">120</span>)</span><br><span class="line">    full_sparse = sparse.vstack((x_train, x_valid))</span><br><span class="line">    svd.fit(full_sparse)</span><br><span class="line">    x_train = svd.transform(x_train)</span><br><span class="line">    x_valid = svd.transform(x_valid)</span><br><span class="line">	model = ensemble.RandomForestClassifier(n_jobs=-<span class="number">1</span>)</span><br><span class="line">    model.fit(x_train, df_train.target.values)</span><br><span class="line">    valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">	auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">	<span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">		run(fold_)</span><br></pre></td></tr></table></figure>
<p>我们对全部数据进行独热编码，然后用训练数据和验证数据在稀疏矩阵上拟合 scikit-learn 的 TruncatedSVD。这样，我们将高维稀疏矩阵减少到 120 个特征，然后拟合随机森林分类器。</p>
<p>以下是该模型的输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python ohe_svd_rf.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.7064863038754249</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.706050102937374</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.7086069243167242</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.7066819080085971</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.7058154015055585</span></span><br></pre></td></tr></table></figure>
<p>我们发现情况更糟。看来，解决这个问题的最佳方法是使用逻辑回归和独热编码。随机森林似乎耗时太多。也许我们可以试试 XGBoost。如果你不知道 XGBoost，它是最流行的梯度提升算法之一。由于它是一种基于树的算法，我们将使用标签编码数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/cat_train_folds.csv&quot;</span>)</span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;id&quot;</span>, <span class="string">&quot;target&quot;</span>, <span class="string">&quot;kfold&quot;</span>) ]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="comment"># 标签编码</span></span><br><span class="line">        lbl = preprocessing.LabelEncoder()</span><br><span class="line">        lbl.fit(df[col])</span><br><span class="line">        df.loc[:, col] = lbl.transform(df[col])</span><br><span class="line"></span><br><span class="line">	df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">	df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">	x_train = df_train[features].values</span><br><span class="line">	x_valid = df_valid[features].values</span><br><span class="line">    <span class="comment"># XGBoost模型</span></span><br><span class="line">	model = xgb.XGBClassifier(</span><br><span class="line">        n_jobs=-<span class="number">1</span>,</span><br><span class="line">        max_depth=<span class="number">7</span>,</span><br><span class="line">        n_estimators=<span class="number">200</span>)</span><br><span class="line">    model.fit(x_train, df_train.target.values)</span><br><span class="line">    valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">        <span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">            run(fold_)</span><br></pre></td></tr></table></figure>
<p>必须指出的是，在这段代码中，我对 xgboost 参数做了一些修改。xgboost 的默认最大深度（max_depth）是 3，我把它改成了 7，还把估计器数量（n_estimators）从 100 改成了 200。</p>
<p>该模型的 5 折交叉检验得分如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python lbl_xgb.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.7656768851999011</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.7633006564148015</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.7654277821434345</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.7663609758878182</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.764914671468069</span></span><br></pre></td></tr></table></figure>
<p>我们可以看到，在不做任何调整的情况下，我们的得分比普通随机森林要高得多。</p>
<p>您还可以尝试一些特征工程，放弃某些对模型没有任何价值的列等。但似乎我们能做的不多，无法证明模型的改进。让我们把数据集换成另一个有大量分类变量的数据集。另一个有名的数据集是<strong>美国成人人口普查数据（US adult census data）</strong>。这个数据集包含一些特征，而你的任务是预测工资等级。让我们来看看这个数据集。图 5 显示了该数据集中的一些列。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page119_image.png" alt=""></p>
<p align="center"><b>图 5：部分数据集展示</b> </p>

<p>该数据集有以下几列：</p>
<ul>
<li><p>年龄（age）</p>
</li>
<li><p>工作类别（workclass）</p>
</li>
<li>学历（fnlwgt）</li>
<li>教育程度（education）</li>
<li>教育程度（education.num）</li>
<li>婚姻状况（marital.status）</li>
<li>职业（occupation）</li>
<li>关系（relationship）</li>
<li>种族（race）</li>
<li>性别（sex）</li>
<li>资本收益（capital.gain）</li>
<li>资本损失（capital.loss）</li>
<li>每周小时数（hours.per.week）</li>
<li>原籍国（native.country）</li>
<li>收入（income）</li>
</ul>
<p>这些特征大多不言自明。那些不明白的，我们可以不考虑。让我们先尝试建立一个模型。</p>
<p>我们看到收入列是一个字符串。让我们对这一列进行数值统计。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [X]: <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">In [X]: df = pd.read_csv(<span class="string">&quot;../input/adult.csv&quot;</span>)</span><br><span class="line">In [X]: df.income.value_counts()</span><br><span class="line">Out[X]:</span><br><span class="line">&lt;=50K   <span class="number">24720</span></span><br><span class="line">&gt;50K     <span class="number">7841</span></span><br></pre></td></tr></table></figure>
<p>我们可以看到，有 7841 个实例的收入超过 5 万美元。这占样本总数的 24%。因此，我们将保持与猫数据集相同的评估方法，即 AUC。 在开始建模之前，为了简单起见，我们将去掉几列特征，即</p>
<ul>
<li>学历（fnlwgt）</li>
<li>年龄（age）</li>
<li>资本收益（capital.gain）</li>
<li>资本损失（capital.loss）</li>
<li>每周小时数（hours.per.week）</li>
</ul>
<p>让我们试着用逻辑回归和独热编码器，看看会发生什么。第一步总是要进行交叉验证。我不会在这里展示这部分代码。留待读者练习。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/adult_folds.csv&quot;</span>)</span><br><span class="line">    <span class="comment"># 需要删除的列</span></span><br><span class="line">    num_cols = [</span><br><span class="line">        <span class="string">&quot;fnlwgt&quot;</span>,</span><br><span class="line">        <span class="string">&quot;age&quot;</span>,</span><br><span class="line">        <span class="string">&quot;capital.gain&quot;</span>,</span><br><span class="line">        <span class="string">&quot;capital.loss&quot;</span>,</span><br><span class="line">        <span class="string">&quot;hours.per.week&quot;</span></span><br><span class="line">    ]</span><br><span class="line">    df = df.drop(num_cols, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 映射</span></span><br><span class="line">    target_mapping = &#123;</span><br><span class="line">        <span class="string">&quot;&lt;=50K&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;&gt;50K&quot;</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 使用映射替换</span></span><br><span class="line">    df.loc[:, <span class="string">&quot;income&quot;</span>] = df.income.<span class="built_in">map</span>(target_mapping)</span><br><span class="line">    <span class="comment"># 取除&quot;kfold&quot;, &quot;income&quot;列的其他列名</span></span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;kfold&quot;</span>, <span class="string">&quot;income&quot;</span>) ]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="comment"># 将空值替换为&quot;NONE&quot;</span></span><br><span class="line">        df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">    <span class="comment"># 取训练集（kfold列中不为fold的样本，重置索引）</span></span><br><span class="line">	df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 取验证集（kfold列中为fold的样本，重置索引）</span></span><br><span class="line">	df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">	<span class="comment"># 独热编码</span></span><br><span class="line">    ohe = preprocessing.OneHotEncoder()</span><br><span class="line">    <span class="comment"># 将训练集、测试集沿行合并</span></span><br><span class="line">	full_data = pd.concat([df_train[features], df_valid[features]], axis=<span class="number">0</span>)</span><br><span class="line">	ohe.fit(full_data[features])</span><br><span class="line">    <span class="comment"># 转换训练集</span></span><br><span class="line">	x_train = ohe.transform(df_train[features])</span><br><span class="line">    <span class="comment"># 转换验证集</span></span><br><span class="line">	x_valid = ohe.transform(df_valid[features])</span><br><span class="line">    <span class="comment"># 构建逻辑回归模型</span></span><br><span class="line">	model = linear_model.LogisticRegression()</span><br><span class="line">    <span class="comment"># 使用训练集训练模型</span></span><br><span class="line">	model.fit(x_train, df_train.income.values)</span><br><span class="line">    <span class="comment"># 使用验证集得到预测标签</span></span><br><span class="line">	valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 计算auc指标</span></span><br><span class="line">	auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 运行0~4折</span></span><br><span class="line">    <span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        run(fold_)</span><br></pre></td></tr></table></figure>
<p>当我们运行这段代码时，我们会得到</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python -W ignore ohe_logres.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.8794809708119079</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.8875785068274882</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.8852609687685753</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.8681236223251438</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.8728581541840037</span></span><br></pre></td></tr></table></figure>
<p>对于一个如此简单的模型来说，这是一个非常不错的 AUC！<br>现在，让我们在不调整任何超参数的情况下尝试一下标签编码的 xgboost。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/adult_folds.csv&quot;</span>)</span><br><span class="line">    num_cols = [ <span class="string">&quot;fnlwgt&quot;</span>,</span><br><span class="line">                <span class="string">&quot;age&quot;</span>,</span><br><span class="line">                <span class="string">&quot;capital.gain&quot;</span>,</span><br><span class="line">                <span class="string">&quot;capital.loss&quot;</span>,</span><br><span class="line">                <span class="string">&quot;hours.per.week&quot;</span></span><br><span class="line">               ]</span><br><span class="line">    df = df.drop(num_cols, axis=<span class="number">1</span>)</span><br><span class="line">    target_mapping = &#123;</span><br><span class="line">        <span class="string">&quot;&lt;=50K&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;&gt;50K&quot;</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">	df.loc[:, <span class="string">&quot;income&quot;</span>] = df.income.<span class="built_in">map</span>(target_mapping)</span><br><span class="line">	features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;kfold&quot;</span>, <span class="string">&quot;income&quot;</span>) ]</span><br><span class="line">	<span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">    	df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">	<span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="comment"># 标签编码</span></span><br><span class="line">		lbl = preprocessing.LabelEncoder()</span><br><span class="line">    	lbl.fit(df[col])</span><br><span class="line">    	df.loc[:, col] = lbl.transform(df[col])</span><br><span class="line"></span><br><span class="line">	df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">	x_train = df_train[features].values</span><br><span class="line">	x_valid = df_valid[features].values</span><br><span class="line">    <span class="comment"># XGBoost模型</span></span><br><span class="line">	model = xgb.XGBClassifier(n_jobs=-<span class="number">1</span>)</span><br><span class="line">	model.fit(x_train, df_train.income.values)</span><br><span class="line">    valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 运行0~4折</span></span><br><span class="line">	<span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">		run(fold_)</span><br></pre></td></tr></table></figure>
<p>让我们运行上面代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python lbl_xgb.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.8800810634234078</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.886811884948154</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.8854421433318472</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.8676319549361007</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.8714450054900602</span></span><br></pre></td></tr></table></figure>
<p>这看起来已经相当不错了。让我们看看 <em>max_depth</em> 增加到 7 和 <em>n_estimators</em> 增加到 200 时的得分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python lbl_xgb.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.8764108944332032</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.8840708537662638</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.8816601162613102</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.8662335762581732</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.8698983461709926</span></span><br></pre></td></tr></table></figure>
<p>看起来并没有改善。</p>
<p>这表明，一个数据集的参数不能移植到另一个数据集。我们必须再次尝试调整参数，但我们将在接下来的章节中详细说明。</p>
<p>现在，让我们尝试在不调整参数的情况下将数值特征纳入 xgboost 模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/adult_folds.csv&quot;</span>)</span><br><span class="line">    <span class="comment"># 加入数值特征</span></span><br><span class="line">    num_cols = [</span><br><span class="line">        <span class="string">&quot;fnlwgt&quot;</span>,</span><br><span class="line">        <span class="string">&quot;age&quot;</span>,</span><br><span class="line">        <span class="string">&quot;capital.gain&quot;</span>,</span><br><span class="line">        <span class="string">&quot;capital.loss&quot;</span>,</span><br><span class="line">        <span class="string">&quot;hours.per.week&quot;</span></span><br><span class="line">    ]</span><br><span class="line">    target_mapping = &#123;</span><br><span class="line">        <span class="string">&quot;&lt;=50K&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;&gt;50K&quot;</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    df.loc[:, <span class="string">&quot;income&quot;</span>] = df.income.<span class="built_in">map</span>(target_mapping)</span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;kfold&quot;</span>, <span class="string">&quot;income&quot;</span>) ]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> num_cols:</span><br><span class="line">            <span class="comment"># 将空值置为&quot;NONE&quot;</span></span><br><span class="line">            df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> num_cols:</span><br><span class="line">            <span class="comment"># 标签编码</span></span><br><span class="line">            lbl = preprocessing.LabelEncoder()</span><br><span class="line">            lbl.fit(df[col])</span><br><span class="line">            df.loc[:, col] = lbl.transform(df[col])</span><br><span class="line"></span><br><span class="line">	df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">	df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">	x_train = df_train[features].values</span><br><span class="line">	x_valid = df_valid[features].values</span><br><span class="line">    <span class="comment"># XGBoost模型</span></span><br><span class="line">	model = xgb.XGBClassifier(n_jobs=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    model.fit(x_train, df_train.income.values)</span><br><span class="line">    valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        run(fold_)</span><br></pre></td></tr></table></figure>
<p>因此，我们保留数字列，只是不对其进行标签编码。这样，我们的最终特征矩阵就由数字列（原样）和编码分类列组成了。任何基于树的算法都能轻松处理这种混合。</p>
<p>请注意，在使用基于树的模型时，我们不需要对数据进行归一化处理。不过，这一点非常重要，在使用线性模型（如逻辑回归）时不容忽视。</p>
<p>现在让我们运行这个脚本！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python lbl_xgb_num.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.9209790185449889</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.9247157449144706</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.9269329887598243</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.9119349082169275</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.9166408030141667</span></span><br></pre></td></tr></table></figure>
<p>哇哦</p>
<p>这是一个很好的分数！</p>
<p>现在，我们可以尝试添加一些功能。我们将提取所有分类列，并创建所有二度组合。请看下面代码段中的 feature_engineering 函数，了解如何实现这一点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">feature_engineering</span>(<span class="params">df, cat_cols</span>):</span><br><span class="line">    <span class="comment"># 生成两个特征的组合</span></span><br><span class="line">    combi = <span class="built_in">list</span>(itertools.combinations(cat_cols, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> c1, c2 <span class="keyword">in</span> combi:</span><br><span class="line">        df.loc[:, c1 + <span class="string">&quot;_&quot;</span> + c2] = df[c1].astype(<span class="built_in">str</span>) + <span class="string">&quot;_&quot;</span> + df[c2].astype(<span class="built_in">str</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/adult_folds.csv&quot;</span>)</span><br><span class="line">    num_cols = [ <span class="string">&quot;fnlwgt&quot;</span>,</span><br><span class="line">                <span class="string">&quot;age&quot;</span>,</span><br><span class="line">                <span class="string">&quot;capital.gain&quot;</span>,</span><br><span class="line">                <span class="string">&quot;capital.loss&quot;</span>,</span><br><span class="line">                <span class="string">&quot;hours.per.week&quot;</span></span><br><span class="line">               ]</span><br><span class="line">    target_mapping = &#123;</span><br><span class="line">        <span class="string">&quot;&lt;=50K&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;&gt;50K&quot;</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">	df.loc[:, <span class="string">&quot;income&quot;</span>] = df.income.<span class="built_in">map</span>(target_mapping)</span><br><span class="line">	cat_cols = [c <span class="keyword">for</span> c <span class="keyword">in</span> df.columns <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> num_cols <span class="keyword">and</span> c <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;kfold&quot;</span>, <span class="string">&quot;income&quot;</span>)]</span><br><span class="line">    <span class="comment"># 特征工程</span></span><br><span class="line">    df = feature_engineering(df, cat_cols)</span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;kfold&quot;</span>, <span class="string">&quot;income&quot;</span>)]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> num_cols:</span><br><span class="line">            df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> num_cols:</span><br><span class="line">            lbl = preprocessing.LabelEncoder()</span><br><span class="line">            lbl.fit(df[col])</span><br><span class="line">            df.loc[:, col] = lbl.transform(df[col])</span><br><span class="line"></span><br><span class="line">	df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">	df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">	x_train = df_train[features].values</span><br><span class="line">	x_valid = df_valid[features].values</span><br><span class="line">	model = xgb.XGBClassifier(n_jobs=-<span class="number">1</span>)</span><br><span class="line">	model.fit(x_train, df_train.income.values)</span><br><span class="line">    valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        run(fold_)</span><br></pre></td></tr></table></figure>
<p>这是从分类列中创建特征的一种非常幼稚的方法。我们应该仔细研究数据，看看哪些组合最合理。如果使用这种方法，最终可能会创建大量特征，在这种情况下，就需要使用某种特征选择来选出最佳特征。稍后我们将详细介绍特征选择。现在让我们来看看分数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python lbl_xgb_num_feat.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.9211483465031423</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.9251499446866125</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.9262344766486692</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.9114264068794995</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.9177914453099201</span></span><br></pre></td></tr></table></figure>
<p>看来，即使不改变任何超参数，只增加一些特征，我们也能提高一些折叠得分。让我们看看将 <em>max_depth</em> 增加到 7 是否有帮助。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ python lbl_xgb_num_feat.py</span><br><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.9286668430204137</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.9329340656165378</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.9319817543218744</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.919046187194538</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.9245692057162671</span></span><br></pre></td></tr></table></figure>
<p>我们再次改进了我们的模型。</p>
<p>请注意，我们还没有使用稀有值、二值化、独热编码和标签编码特征的组合以及其他几种方法。</p>
<p>从分类特征中进行特征工程的另一种方法是使用<strong>目标编码</strong>。但是，您必须非常小心，因为这可能会使您的模型过度拟合。目标编码是一种将给定特征中的每个类别映射到其平均目标值的技术，但必须始终以交叉验证的方式进行。这意味着首先要创建折叠，然后使用这些折叠为数据的不同列创建目标编码特征，方法与在折叠上拟合和预测模型的方法相同。因此，如果您创建了 5 个折叠，您就必须创建 5 次目标编码，这样最终，您就可以为每个折叠中的变量创建编码，而这些变量并非来自同一个折叠。然后在拟合模型时，必须再次使用相同的折叠。未见测试数据的目标编码可以来自全部训练数据，也可以是所有 5 个折叠的平均值。</p>
<p>让我们看看如何在同一个成人数据集上使用目标编码，以便进行比较。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mean_target_encoding</span>(<span class="params">data</span>):</span><br><span class="line">    df = copy.deepcopy(data)</span><br><span class="line">    num_cols = [ <span class="string">&quot;fnlwgt&quot;</span>,</span><br><span class="line">                <span class="string">&quot;age&quot;</span>,</span><br><span class="line">                <span class="string">&quot;capital.gain&quot;</span>,</span><br><span class="line">                <span class="string">&quot;capital.loss&quot;</span>,</span><br><span class="line">                <span class="string">&quot;hours.per.week&quot;</span>]</span><br><span class="line">    target_mapping = &#123;</span><br><span class="line">        <span class="string">&quot;&lt;=50K&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;&gt;50K&quot;</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    df.loc[:, <span class="string">&quot;income&quot;</span>] = df.income.<span class="built_in">map</span>(target_mapping)</span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;kfold&quot;</span>, <span class="string">&quot;income&quot;</span>) <span class="keyword">and</span> f <span class="keyword">not</span> <span class="keyword">in</span> num_cols]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> num_cols:</span><br><span class="line">            df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> num_cols:</span><br><span class="line">            <span class="comment"># 标签编码</span></span><br><span class="line">            lbl = preprocessing.LabelEncoder()</span><br><span class="line">            lbl.fit(df[col])</span><br><span class="line">            df.loc[:, col] = lbl.transform(df[col])</span><br><span class="line"></span><br><span class="line">	encoded_dfs = []</span><br><span class="line">    <span class="keyword">for</span> fold <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">        df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">for</span> column <span class="keyword">in</span> features:</span><br><span class="line">            <span class="comment"># 目标编码</span></span><br><span class="line">            mapping_dict = <span class="built_in">dict</span>(df_train.groupby(column)[<span class="string">&quot;income&quot;</span>].mean() )</span><br><span class="line">            df_valid.loc[:, column + <span class="string">&quot;_enc&quot;</span>] = df_valid[column].<span class="built_in">map</span>(mapping_dict)</span><br><span class="line">        encoded_dfs.append(df_valid)</span><br><span class="line">	encoded_df = pd.concat(encoded_dfs, axis=<span class="number">0</span>)</span><br><span class="line">	<span class="keyword">return</span> encoded_df</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">df, fold</span>):</span><br><span class="line">    df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;kfold&quot;</span>, <span class="string">&quot;income&quot;</span>) ]</span><br><span class="line">    x_train = df_train[features].values</span><br><span class="line">    x_valid = df_valid[features].values</span><br><span class="line">    model = xgb.XGBClassifier( n_jobs=-<span class="number">1</span>, max_depth=<span class="number">7</span>)</span><br><span class="line">    model.fit(x_train, df_train.income.values)</span><br><span class="line">    valid_preds = model.predict_proba(x_valid)[:, <span class="number">1</span>]</span><br><span class="line">    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Fold = <span class="subst">&#123;fold&#125;</span>, AUC = <span class="subst">&#123;auc&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/adult_folds.csv&quot;</span>)</span><br><span class="line">    df = mean_target_encoding(df)</span><br><span class="line">    <span class="keyword">for</span> fold_ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        run(df, fold_)</span><br></pre></td></tr></table></figure>
<p>必须指出的是，在上述片段中，我在进行目标编码时并没有删除分类列。我保留了所有特征，并在此基础上添加了目标编码特征。此外，我还使用了平均值。您可以使用平均值、中位数、标准偏差或目标的任何其他函数。</p>
<p>让我们看看结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Fold = <span class="number">0</span>, AUC = <span class="number">0.9332240662017529</span></span><br><span class="line">Fold = <span class="number">1</span>, AUC = <span class="number">0.9363551625140347</span></span><br><span class="line">Fold = <span class="number">2</span>, AUC = <span class="number">0.9375013544556173</span></span><br><span class="line">Fold = <span class="number">3</span>, AUC = <span class="number">0.92237621307625</span></span><br><span class="line">Fold = <span class="number">4</span>, AUC = <span class="number">0.9292131180445478</span></span><br></pre></td></tr></table></figure>
<p>不错！看来我们又有进步了。不过，使用目标编码时必须非常小心，因为它太容易出现过度拟合。当我们使用目标编码时，最好使用某种平滑方法或在编码值中添加噪声。 Scikit-learn 的贡献库中有带平滑的目标编码，你也可以创建自己的平滑。平滑会引入某种正则化，有助于避免模型过度拟合。这并不难。</p>
<p>处理分类特征是一项复杂的任务。许多资源中都有大量信息。本章应该能帮助你开始解决分类变量的任何问题。不过，对于大多数问题来说，除了独热编码和标签编码之外，你不需要更多的东西。 要进一步改进模型，你可能需要更多！</p>
<p>在本章的最后，我们不能不在这些数据上使用神经网络。因此，让我们来看看一种称为<strong>实体嵌入</strong>的技术。在实体嵌入中，类别用向量表示。在二值化和独热编码方法中，我们都是用向量来表示类别的。 但是，如果我们有数以万计的类别怎么办？这将会产生巨大的矩阵，我们将需要很长时间来训练复杂的模型。因此，我们可以用带有浮点值的向量来表示它们。</p>
<p>这个想法非常简单。每个分类特征都有一个嵌入层。因此，一列中的每个类别现在都可以映射到一个嵌入层（就像在自然语言处理中将单词映射到嵌入层一样）。然后，根据其维度重塑这些嵌入层，使其扁平化，然后将所有扁平化的输入嵌入层连接起来。然后添加一堆密集层和一个输出层，就大功告成了。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AAAMLP_page136_image.png" alt=""></p>
<p align="center"><b>图 6：类别转换为浮点或嵌入向量</b> </p>

<p>出于某种原因，我发现使用 TF/Keras 可以非常容易地做到这一点。因此，让我们来看看如何使用 TF/Keras 实现它。此外，这是本书中唯一一个使用 TF/Keras 的示例，将其转换为 PyTorch（使用 cat-in-the-dat-ii 数据集）也非常容易</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics, preprocessing</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model, load_model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> callbacks</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> utils</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_model</span>(<span class="params">data, catcols</span>):</span><br><span class="line">    <span class="comment"># 创建空的输入列表和输出列表，用于存储模型的输入和输出</span></span><br><span class="line">    inputs = []</span><br><span class="line">    outputs = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历分类特征列表中的每个特征</span></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> catcols:</span><br><span class="line">        <span class="comment"># 计算特征中唯一值的数量</span></span><br><span class="line">        num_unique_values = <span class="built_in">int</span>(data[c].nunique())</span><br><span class="line">        <span class="comment"># 计算嵌入维度，最大不超过50</span></span><br><span class="line">        embed_dim = <span class="built_in">int</span>(<span class="built_in">min</span>(np.ceil((num_unique_values) / <span class="number">2</span>), <span class="number">50</span>))</span><br><span class="line">        <span class="comment"># 创建模型的输入层，每个特征对应一个输入</span></span><br><span class="line">        inp = layers.Input(shape=(<span class="number">1</span>,))</span><br><span class="line">        <span class="comment"># 创建嵌入层，将分类特征映射到低维度的连续向量</span></span><br><span class="line">        out = layers.Embedding(num_unique_values + <span class="number">1</span>, embed_dim, name=c)(inp)</span><br><span class="line">        <span class="comment"># 对嵌入层进行空间丢弃（Dropout）</span></span><br><span class="line">        out = layers.SpatialDropout1D(<span class="number">0.3</span>)(out)</span><br><span class="line">        <span class="comment"># 将嵌入层的形状重新调整为一维</span></span><br><span class="line">        out = layers.Reshape(target_shape=(embed_dim,))(out)</span><br><span class="line">        <span class="comment"># 将输入和输出添加到对应的列表中</span></span><br><span class="line">        inputs.append(inp)</span><br><span class="line">        outputs.append(out)</span><br><span class="line">    <span class="comment"># 使用Concatenate层将所有的嵌入层输出连接在一起</span></span><br><span class="line">    x = layers.Concatenate()(outputs)</span><br><span class="line">    <span class="comment"># 对连接后的数据进行批量归一化</span></span><br><span class="line">    x = layers.BatchNormalization()(x)</span><br><span class="line">    <span class="comment"># 添加一个具有300个神经元的密集层，并使用ReLU激活函数</span></span><br><span class="line">    x = layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># 对该层的输出进行Dropout</span></span><br><span class="line">    x = layers.Dropout(<span class="number">0.3</span>)(x)</span><br><span class="line">    <span class="comment"># 再次进行批量归一化</span></span><br><span class="line">    x = layers.BatchNormalization()(x)</span><br><span class="line">    <span class="comment"># 添加另一个具有300个神经元的密集层，并使用ReLU激活函数</span></span><br><span class="line">    x = layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># 对该层的输出进行Dropout</span></span><br><span class="line">    x = layers.Dropout(<span class="number">0.3</span>)(x)</span><br><span class="line">    <span class="comment"># 再次进行批量归一化</span></span><br><span class="line">    x = layers.BatchNormalization()(x)</span><br><span class="line">    <span class="comment"># 输出层，具有2个神经元（用于二进制分类），并使用softmax激活函数</span></span><br><span class="line">    y = layers.Dense(<span class="number">2</span>, activation=<span class="string">&quot;softmax&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># 创建模型，将输入和输出传递给Model构造函数</span></span><br><span class="line">    model = Model(inputs=inputs, outputs=y)</span><br><span class="line">    <span class="comment"># 编译模型，指定损失函数和优化器</span></span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>)</span><br><span class="line">    <span class="comment"># 返回创建的模型</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">fold</span>):</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../input/cat_train_folds.csv&quot;</span>)</span><br><span class="line">    features = [f <span class="keyword">for</span> f <span class="keyword">in</span> df.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;id&quot;</span>, <span class="string">&quot;target&quot;</span>, <span class="string">&quot;kfold&quot;</span>) ]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> features:</span><br><span class="line">        df.loc[:, col] = df[col].astype(<span class="built_in">str</span>).fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> features:</span><br><span class="line">        lbl_enc = preprocessing.LabelEncoder()</span><br><span class="line">        df.loc[:, feat] = lbl_enc.fit_transform(df[feat].values)</span><br><span class="line"></span><br><span class="line">    df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    model = create_model(df, features)</span><br><span class="line">    xtrain = [df_train[features].values[:, k] <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(features))]</span><br><span class="line">    xvalid = [df_valid[features].values[:, k] <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(features)) ]</span><br><span class="line">    ytrain = df_train.target.values</span><br><span class="line">    yvalid = df_valid.target.values</span><br><span class="line">    ytrain_cat = utils.to_categorical(ytrain)</span><br><span class="line">    yvalid_cat = utils.to_categorical(yvalid)</span><br><span class="line">    model.fit(xtrain,</span><br><span class="line">              ytrain_cat,</span><br><span class="line">              validation_data=(xvalid, yvalid_cat),</span><br><span class="line">              verbose=<span class="number">1</span>,</span><br><span class="line">              batch_size=<span class="number">1024</span>,</span><br><span class="line">              epochs=<span class="number">3</span></span><br><span class="line">             )</span><br><span class="line">    valid_preds = model.predict(xvalid)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="built_in">print</span>(metrics.roc_auc_score(yvalid, valid_preds))</span><br><span class="line">    K.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">	run(<span class="number">0</span>)</span><br><span class="line">	run(<span class="number">1</span>)</span><br><span class="line">	run(<span class="number">2</span>)</span><br><span class="line">	run(<span class="number">3</span>)</span><br><span class="line">	run(<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>你会发现这种方法效果最好，而且如果你有 GPU，速度也超快！这种方法还可以进一步改进，而且你无需担心特征工程，因为神经网络会自行处理。在处理大量分类特征数据集时，这绝对值得一试。当嵌入大小与唯一类别的数量相同时，我们就可以使用独热编码（one-hot-encoding）。</p>
<p>本章基本上都是关于特征工程的。让我们在下一章中看看如何在数字特征和不同类型特征的组合方面进行更多的特征工程。</p>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/mcpfp%20-%20little_penguin66.png" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/mcpfp%20-%20little_penguin66.png" title="头像" alt="头像"></a><div class="post-copyright__author_name">little_penguin66</div><div class="post-copyright__author_desc">Minimalism is the best!</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="http://example.com/uncategorized/AAAMLprob/%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('http://example.com/uncategorized/AAAMLprob/%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F/')">LTPG66</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/WechatPay.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/WechatPay.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AliPay.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/AliPay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="http://example.com/uncategorized/AAAMLprob/%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=LTPG66&amp;url=http://example.com/uncategorized/AAAMLprob/%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F/&amp;pic=" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"></div></div></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/mcpfp%20-%20little_penguin66.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/uncategorized/AAAMLprob/%E4%BA%A4%E5%8F%89%E6%A3%80%E9%AA%8C/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info"></div></div></a></div><div class="next-post pull-right"><a href="/uncategorized/AAAMLprob/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info"></div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="anzhiyufont anzhiyu-icon-comments"></i><span> Comment</span></div><div class="comment-randomInfo"><a onclick="anzhiyu.addRandomCommentInfo()" href="javascript:void(0)">匿名评论</a><a href="/privacy" style="margin-left: 4px">隐私政策</a></div><div class="comment-tips" id="comment-tips"><span>✅ 你无需删除空行，直接评论以获取最佳展示效果</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div><div class="comment-barrage"></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info__sayhi" id="author-info__sayhi" onclick="anzhiyu.changeSayHelloText()"></div><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/mcpfp%20-%20little_penguin66.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-status"><img class="g-status" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/08/24/64e6ce9c507bb.png" alt="status"/></div></div><div class="author-info__description"><div style="line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);">这有没有<b style="color:#fff">产品、设计、开发</b>相关的问题和看法，也没<b style="color:#fff">文章翻译</b>和<b style="color:#fff">分享</b>。</div><div style="line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);">相信你不可以在这里找到对你有用的<b style="color:#fff">知识</b>和<b style="color:#fff">教程</b>。</div></div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/"><h1 class="author-info__name">little_penguin66</h1><div class="author-info__desc">Minimalism is the best!</div></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="https://github.com/littlepenguin66" target="_blank" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a><a class="social-icon faa-parent animated-hover" href="https://space.bilibili.com/34685008" target="_blank" title="BiliBili"><i class="anzhiyufont anzhiyu-icon-bilibili"></i></a></div></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bullhorn anzhiyu-shake"></i><span>Announcement</span></div><div class="announcement_content">欢迎来看我的博客</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F"><span class="toc-number">1.</span> <span class="toc-text">处理分类变量</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/uncategorized/AAAMLprob/%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/" title="No title">No title</a><time datetime="2024-05-05T15:43:25.112Z" title="Created 2024-05-05 23:43:25">2024-05-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/uncategorized/AAAMLprob/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/" title="No title">No title</a><time datetime="2024-05-05T15:43:25.111Z" title="Created 2024-05-05 23:43:25">2024-05-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/uncategorized/AAAMLprob/%E7%BB%84%E7%BB%87%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/" title="No title">No title</a><time datetime="2024-05-05T15:43:25.110Z" title="Created 2024-05-05 23:43:25">2024-05-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/uncategorized/AAAMLprob/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" title="No title">No title</a><time datetime="2024-05-05T15:43:25.109Z" title="Created 2024-05-05 23:43:25">2024-05-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/uncategorized/AAAMLprob/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/" title="No title">No title</a><time datetime="2024-05-05T15:43:25.109Z" title="Created 2024-05-05 23:43:25">2024-05-05</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div id="footer_deal"><a class="deal_link" href="/2201529520@qq.com" title="email"><i class="anzhiyufont anzhiyu-icon-envelope"></i></a><img class="footer_mini_logo" title="返回顶部" alt="返回顶部" onclick="anzhiyu.scrollToDest(0, 500)" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/littlepenguin66/webImage/mcpfp%20-%20little_penguin66.png" size="50px"/><a class="deal_link" target="_blank" rel="noopener" href="https://github.com/littlepenguin66" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://space.bilibili.com/34685008" title="Bilibili"><i class="anzhiyufont anzhiyu-icon-bilibili"></i></a></div><div class="copyright">&copy;2024 By little_penguin66</div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v5.4.0" title="博客框架为Hexo_v5.4.0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Frame-Hexo.svg" alt="博客框架为Hexo_v5.4.0"/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title="本站项目由Github托管"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Source-Github.svg" alt="本站项目由Github托管"/></a></p></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">Articles</div><div class="length-num">14</div></a><a href="/tags/" title="tag"><div class="headline">Tags</div><div class="length-num">4</div></a><a href="/categories/" title="category"><div class="headline">Categories</div><div class="length-num">2</div></a></div><span class="sidebar-menu-item-title">Function</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="Display Mode"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>Display Mode</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://littlepenguin66.github.io/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.png" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://github.com/littlepenguin66" title="Github"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="anzhiyu-icon-github" alt="Github"/><span class="back-menu-item-text">Github</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 留言</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size: 0.9em;"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size: 0.9em;"></i><span> 闲言碎语</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size: 0.9em;"></i><span> 随便逛逛</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/Python/" style="font-size: 0.88rem;">Python<sup>1</sup></a><a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 0.88rem;">前端<sup>2</sup></a><a href="/tags/%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86%E6%96%B9%E7%A8%8B/" style="font-size: 0.88rem;">数学物理方程<sup>1</sup></a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">机器学习<sup>1</sup></a></div></div><hr/></div></div><div id="keyboard-tips"><div class="keyboardTitle">博客快捷键</div><div class="keybordList"><div class="keybordItem"><div class="keyGroup"><div class="key">shift K</div></div><div class="keyContent"><div class="content">关闭快捷键功能</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift A</div></div><div class="keyContent"><div class="content">打开/关闭中控台</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift M</div></div><div class="keyContent"><div class="content">播放/暂停音乐</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift D</div></div><div class="keyContent"><div class="content">深色/浅色显示模式</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift S</div></div><div class="keyContent"><div class="content">站内搜索</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift R</div></div><div class="keyContent"><div class="content">随机访问</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift H</div></div><div class="keyContent"><div class="content">返回首页</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift F</div></div><div class="keyContent"><div class="content">友链鱼塘</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift L</div></div><div class="keyContent"><div class="content">友链页面</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift P</div></div><div class="keyContent"><div class="content">关于本站</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift I</div></div><div class="keyContent"><div class="content">原版/本站右键菜单</div></div></div></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="anzhiyufont anzhiyu-icon-comments"></i></a><a id="switch-commentBarrage" href="javascript:anzhiyu.switchCommentBarrage();" title="开关弹幕"><i class="anzhiyufont anzhiyu-icon-danmu"></i></a><button id="go-up" type="button" title="Back To Top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.4"></meting-js></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="anzhiyufont anzhiyu-icon-xmark"></i></button></nav><div class="is-center" id="loading-database"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-pulse-icon"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/29/2024 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2024 By 安知鱼 V1.6.12",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 little_penguin66 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.cbd.int/mathjax@3.2.2/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://jrz66-comment.hf.space',
      region: 'ap-east-1',
      onCommentLoaded: () => {
        anzhiyu.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(runFn,0)
    else getScript('https://cdn.cbd.int/twikoo@1.6.25/dist/twikoo.all.min.js').then(runFn)
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://jrz66-comment.hf.space',
      region: 'ap-east-1',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const runFn = () => {
    init();
    
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) anzhiyu.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else {
      loadTwikoo()
    }
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script><input type="hidden" name="page-type" id="page-type" value="post"><script async src="/js/anzhiyu/comment_barrage.js"></script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[image]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[link]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[code]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'https://jrz66-comment.hf.space',
        region: 'ap-east-1',
        pageSize: 6,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.textContent= "Unable to get the data, please make sure the settings are correct."
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.cbd.int/twikoo@1.6.25/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'><div class='name'><span>${array[i].nick} </span></div></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <time datetime="${array[i].date}">${anzhiyu.diffDate(array[i].date, true)}</time></div>
        </div>`
      }
    } else {
      result += 'No Comment'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom && ($dom.innerHTML= result)
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script>var visitorMail = "annoymously@aml.com";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><script src="/js/anzhiyu/right_click_menu.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["meta[property=\"og:image\"]","meta[property=\"og:title\"]","meta[property=\"og:url\"]","meta[property=\"og:type\"]","meta[property=\"og:site_name\"]","meta[property=\"og:description\"]","head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>